<!DOCTYPE html>
<html class="no-js" lang="zh">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="NoughtQ的笔记本，主要记录一些 CS 相关的笔记" name="description"/>
<meta content="NoughtQ" name="author"/>
<link href="https://notebook.noughtq.top/ai/ml/11.html" rel="canonical"/>
<link href="10.html" rel="prev"/>
<link href="12.html" rel="next"/>
<link href="../../feed_rss_created.xml" rel="alternate" title="RSS 订阅" type="application/rss+xml"/>
<link href="../../feed_rss_updated.xml" rel="alternate" title="已更新内容的 RSS 订阅" type="application/rss+xml"/>
<link href="../../assets/favicon.png" rel="icon"/>
<meta content="mkdocs-1.6.1, mkdocs-material-9.6.18" name="generator"/>
<title>Reinforcement Learning - NoughtQ的笔记本</title>
<link href="../../assets/stylesheets/main.7e37652d.min.css" rel="stylesheet"/>
<link href="../../assets/stylesheets/palette.06af60db.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=JetBrains+Mono,+LXGW+WenKai+Screen+GB+Screen:300,300i,400,400i,700,700i%7CJetBrains+Mono,+Consolas:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"JetBrains Mono, LXGW WenKai Screen GB Screen";--md-code-font:"JetBrains Mono, Consolas"}</style>
<link href="../../css/heti.css" rel="stylesheet"/>
<link href="../../css/toc_extra.css" rel="stylesheet"/>
<link href="../../css/timeline.css" rel="stylesheet"/>
<link href="../../css/card.css" rel="stylesheet"/>
<link href="../../css/custom.css" rel="stylesheet"/>
<link href="../../css/extra_changelog.css" rel="stylesheet"/>
<link href="../../css/header.css" rel="stylesheet"/>
<link href="../../css/sidebar.css" rel="stylesheet"/>
<link href="https://unpkg.com/katex@0/dist/katex.min.css" rel="stylesheet"/>
<link href="https://gcore.jsdelivr.net/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css" rel="stylesheet"/>
<link href="https://gcore.jsdelivr.net/npm/lxgw-wenkai-webfont@1.1.0/style.css" rel="stylesheet"/>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&amp;display=swap" rel="stylesheet"/>
<script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-43NH8CVRCJ"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-43NH8CVRCJ",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-43NH8CVRCJ",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
<script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
</head>
<body data-md-color-accent="indigo" data-md-color-primary="indigo" data-md-color-scheme="slate" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#reinforcement-learning">
          跳转至
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header" data-md-component="header">
<nav aria-label="页眉" class="md-header__inner md-grid">
<a aria-label="NoughtQ的笔记本" class="md-header__button md-logo" data-md-component="logo" href="../.." title="NoughtQ的笔记本">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.05 9H7.06V6h1.99V4.03H7.06v-1c0-1.11.89-1.99 1.99-1.99h5.98V8l2.47-1.5L20 8V1.04h1c1.05 0 2 .96 2 1.99V17c0 1.03-.95 2-2 2H9.05c-1.05 0-1.99-.95-1.99-2v-1h1.99v-2H7.06v-3h1.99zM1 18h2v-3H1v-2h2v-3H1V8h2V5h2v3H3v2h2v3H3v2h2v3H3v2h2v1h16v2H5a2 2 0 0 1-2-2v-1H1z"></path></svg>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            NoughtQ的笔记本
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              Reinforcement Learning
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Dark Mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefer-color-scheme: dark)" data-md-color-primary="indigo" data-md-color-scheme="slate" id="__palette_0" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Dark Mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
</label>
<input aria-label="Light Mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefer-color-scheme: light)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_0" hidden="" title="Light Mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"></path></svg>
</label>
</form>
<script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="搜索" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="搜索" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
<svg viewbox="0 0 256 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L77.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z" fill="currentColor"></path></svg>
</label>
<nav aria-label="查找" class="md-search__options">
<a aria-label="分享" class="md-search__icon md-icon" data-clipboard="" data-clipboard-text="" data-md-component="search-share" href="javascript:void(0)" tabindex="-1" title="分享">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg>
</a>
<button aria-label="清空当前内容" class="md-search__icon md-icon" tabindex="-1" title="清空当前内容" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
</button>
</nav>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="" tabindex="0">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
<ol class="md-search-result__list" role="presentation"></ol>
</div>
</div>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/noughtq/notebook" title="前往仓库">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4" fill="currentColor"></path></svg>
</div>
<div class="md-source__repository">
    NoughtQ/Notebook
  </div>
</a>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<nav aria-label="标签" class="md-tabs" data-md-component="tabs">
<div class="md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../index.html">
          
  
  
    
  
  🏫主页

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../lang/index.html">
          
  
  
    
  
  🔡语言

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../math/index.html">
          
  
  
    
  
  📊数学相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../algorithms/index.html">
          
  
  
    
  
  🧮算法相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../software/index.html">
          
  
  
    
  
  💾软件相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../system/index.html">
          
  
  
    
  
  💻系统相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../web/index.html">
          
  
  
    
  
  🌏Web相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../sec/ctf-101/index.html">
          
  
  
    
  
  🛡️信息安全

        </a>
</li>
<li class="md-tabs__item md-tabs__item--active">
<a class="md-tabs__link" href="../index.html">
          
  
  
    
  
  🤖人工智能

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../misc/index.html">
          
  
  
    
  
  🗃️杂项

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../tools/index.html">
          
  
  
    
  
  🛠️工具

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../papers/index.html">
          
  
  
    
  
  📑论文阅读

        </a>
</li>
</ul>
</div>
</nav>
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="导航栏" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="NoughtQ的笔记本" class="md-nav__button md-logo" data-md-component="logo" href="../.." title="NoughtQ的笔记本">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.05 9H7.06V6h1.99V4.03H7.06v-1c0-1.11.89-1.99 1.99-1.99h5.98V8l2.47-1.5L20 8V1.04h1c1.05 0 2 .96 2 1.99V17c0 1.03-.95 2-2 2H9.05c-1.05 0-1.99-.95-1.99-2v-1h1.99v-2H7.06v-3h1.99zM1 18h2v-3H1v-2h2v-3H1V8h2V5h2v3H3v2h2v3H3v2h2v3H3v2h2v1h16v2H5a2 2 0 0 1-2-2v-1H1z"></path></svg>
</a>
    NoughtQ的笔记本
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/noughtq/notebook" title="前往仓库">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4" fill="currentColor"></path></svg>
</div>
<div class="md-source__repository">
    NoughtQ/Notebook
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../index.html">
<span class="md-ellipsis">
    🏫主页
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../lang/index.html">
<span class="md-ellipsis">
    🔡语言
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../math/index.html">
<span class="md-ellipsis">
    📊数学相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../algorithms/index.html">
<span class="md-ellipsis">
    🧮算法相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../software/index.html">
<span class="md-ellipsis">
    💾软件相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../system/index.html">
<span class="md-ellipsis">
    💻系统相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../web/index.html">
<span class="md-ellipsis">
    🌏Web相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../sec/ctf-101/index.html">
<span class="md-ellipsis">
    🛡️信息安全
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_9" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../index.html">
<span class="md-ellipsis">
    🤖人工智能
    
  </span>
</a>
<label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_9_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_9">
<span class="md-nav__icon md-icon"></span>
            🤖人工智能
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_9_2" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="index.html">
<span class="md-ellipsis">
    机器学习
    
  </span>
</a>
<label class="md-nav__link" for="__nav_9_2" id="__nav_9_2_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_9_2_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_9_2">
<span class="md-nav__icon md-icon"></span>
            机器学习
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="1.html">
<span class="md-ellipsis">
    Machine Learning - P1
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="2.html">
<span class="md-ellipsis">
    Machine Learning - P2
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="3.html">
<span class="md-ellipsis">
    Deep Learning
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="4.html">
<span class="md-ellipsis">
    Self-Attention
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="5.html">
<span class="md-ellipsis">
    Transformer
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="6.html">
<span class="md-ellipsis">
    Generative Adversarial Network
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="7.html">
<span class="md-ellipsis">
    Self-Supervised Learning
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="8.html">
<span class="md-ellipsis">
    Explainable Machine Learning
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="9.html">
<span class="md-ellipsis">
    Adversarial Attack
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="10.html">
<span class="md-ellipsis">
    Adaptation
    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
<span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="11.html">
<span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
</a>
<nav aria-label="目录" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
        目录
      </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#basic-ideas">
<span class="md-ellipsis">
      Basic Ideas
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#policy-gradient">
<span class="md-ellipsis">
      Policy Gradient
    </span>
</a>
<nav aria-label="Policy Gradient" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#how-to-control-the-actor">
<span class="md-ellipsis">
      How to Control the Actor
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#version-0">
<span class="md-ellipsis">
      Version 0
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#version-1">
<span class="md-ellipsis">
      Version 1
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#version-2">
<span class="md-ellipsis">
      Version 2
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#version-3">
<span class="md-ellipsis">
      Version 3
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#algorithm">
<span class="md-ellipsis">
      Algorithm
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#on-policy-vs-off-policy">
<span class="md-ellipsis">
      On-policy v.s. Off-policy
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#exploration">
<span class="md-ellipsis">
      Exploration
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#actor-critic">
<span class="md-ellipsis">
      Actor-Critic
    </span>
</a>
<nav aria-label="Actor-Critic" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#how-to-estimate-value-function">
<span class="md-ellipsis">
      How to Estimate Value Function
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#version-35">
<span class="md-ellipsis">
      Version 3.5
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#version-4">
<span class="md-ellipsis">
      Version 4
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#tip">
<span class="md-ellipsis">
      Tip
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#outlook-deep-q-network">
<span class="md-ellipsis">
      Outlook: Deep Q Network
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#reward-shaping">
<span class="md-ellipsis">
      Reward Shaping
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#no-reward-learning-from-demonstration">
<span class="md-ellipsis">
      No Reward: Learning from Demonstration
    </span>
</a>
<nav aria-label="No Reward: Learning from Demonstration" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#imitation-learning">
<span class="md-ellipsis">
      Imitation Learning
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#inverse-reinforcement-learning">
<span class="md-ellipsis">
      Inverse Reinforcement Learning
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="12.html">
<span class="md-ellipsis">
    Network Compression
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="13.html">
<span class="md-ellipsis">
    Life-long Learning
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="14.html">
<span class="md-ellipsis">
    Meta Learning
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../genai/index.html">
<span class="md-ellipsis">
    生成式人工智能
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../cv/index.html">
<span class="md-ellipsis">
    计算机视觉导论
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../misc/index.html">
<span class="md-ellipsis">
    🗃️杂项
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../tools/index.html">
<span class="md-ellipsis">
    🛠️工具
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../papers/index.html">
<span class="md-ellipsis">
    📑论文阅读
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="目录" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
        目录
      </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#basic-ideas">
<span class="md-ellipsis">
      Basic Ideas
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#policy-gradient">
<span class="md-ellipsis">
      Policy Gradient
    </span>
</a>
<nav aria-label="Policy Gradient" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#how-to-control-the-actor">
<span class="md-ellipsis">
      How to Control the Actor
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#version-0">
<span class="md-ellipsis">
      Version 0
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#version-1">
<span class="md-ellipsis">
      Version 1
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#version-2">
<span class="md-ellipsis">
      Version 2
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#version-3">
<span class="md-ellipsis">
      Version 3
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#algorithm">
<span class="md-ellipsis">
      Algorithm
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#on-policy-vs-off-policy">
<span class="md-ellipsis">
      On-policy v.s. Off-policy
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#exploration">
<span class="md-ellipsis">
      Exploration
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#actor-critic">
<span class="md-ellipsis">
      Actor-Critic
    </span>
</a>
<nav aria-label="Actor-Critic" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#how-to-estimate-value-function">
<span class="md-ellipsis">
      How to Estimate Value Function
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#version-35">
<span class="md-ellipsis">
      Version 3.5
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#version-4">
<span class="md-ellipsis">
      Version 4
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#tip">
<span class="md-ellipsis">
      Tip
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#outlook-deep-q-network">
<span class="md-ellipsis">
      Outlook: Deep Q Network
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#reward-shaping">
<span class="md-ellipsis">
      Reward Shaping
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#no-reward-learning-from-demonstration">
<span class="md-ellipsis">
      No Reward: Learning from Demonstration
    </span>
</a>
<nav aria-label="No Reward: Learning from Demonstration" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#imitation-learning">
<span class="md-ellipsis">
      Imitation Learning
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#inverse-reinforcement-learning">
<span class="md-ellipsis">
      Inverse Reinforcement Learning
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<a class="md-content__button md-icon" href="https://github.com/noughtq/notebook/edit/master/docs/ai/ml/11.md" rel="edit" title="编辑此页">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
</a>
<a class="md-content__button md-icon" href="https://github.com/noughtq/notebook/raw/master/docs/ai/ml/11.md" title="查看本页的源代码">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
</a>
<div><h1 id="reinforcement-learning">Reinforcement Learning<a class="headerlink" href="#reinforcement-learning" title="Permanent link">⚓︎</a></h1>
<div style="margin-top: -30px; font-size: 0.9em; opacity: 0.7;">
<p><span class="twemoji"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10h-2a8 8 0 0 1-8 8 8 8 0 0 1-8-8 8 8 0 0 1 8-8zm6.78 1a.7.7 0 0 0-.48.2l-1.22 1.21 2.5 2.5L20.8 5.7c.26-.26.26-.7 0-.95L19.25 3.2c-.13-.13-.3-.2-.47-.2m-2.41 2.12L9 12.5V15h2.5l7.37-7.38z"></path></svg></span> 约<span class="heti-skip"><span class="heti-spacing"> </span>4852<span class="heti-spacing"> </span></span>个字 <span class="twemoji"><svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M360.8 1.2c-17-4.9-34.7 5-39.6 22l-128 448c-4.9 17 5 34.7 22 39.6s34.7-5 39.6-22l128-448c4.9-17-5-34.7-22-39.6m64.6 136.1c-12.5 12.5-12.5 32.8 0 45.3l73.4 73.4-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l96-96c12.5-12.5 12.5-32.8 0-45.3l-96-96c-12.5-12.5-32.8-12.5-45.3 0zm-274.7 0c-12.5-12.5-32.8-12.5-45.3 0l-96 96c-12.5 12.5-12.5 32.8 0 45.3l96 96c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L77.3 256l73.3-73.4c12.5-12.5 12.5-32.8 0-45.3z" fill="currentColor"></path></svg></span> <span>7<span class="heti-spacing"> </span></span>行代码 <span class="twemoji"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 20c4.42 0 8-3.58 8-8s-3.58-8-8-8-8 3.58-8 8 3.58 8 8 8m0-18c5.5 0 10 4.5 10 10s-4.5 10-10 10C6.47 22 2 17.5 2 12S6.5 2 12 2m.5 11H11V7h1.5v4.26l3.7-2.13.75 1.3z"></path></svg></span> 预计阅读时间<span class="heti-skip"><span class="heti-spacing"> </span>24<span class="heti-spacing"> </span></span>分钟</p>
</div>
<p>之前的章节中，有一大半都是在讲监督学习，这种训练方式要求有一个标注好的数据集。但并不是所有任务都能够被标注，而且机器也没法仅凭一个标签对结果的好坏做出准确的分析，比如下围棋等等。而<strong>强化学习</strong><span>(reinforcement learning)<span class="heti-spacing"> </span></span>就能解决这些无法被监督学习解决的问题，所以本节将详细介绍这一耳熟能详的技术。</p>
<h2 id="basic-ideas">Basic Ideas<a class="headerlink" href="#basic-ideas" title="Permanent link">⚓︎</a></h2>
<p>最开始的时候，我们认识到机器学习的目标近似于找一个函数；身为机器学习的成员之一，强化学习的目标亦是如此。整个强化学习的过程中有两个对象：<strong>行动者</strong><span>(actor)<span class="heti-spacing"> </span></span>和<strong>环境</strong>(environment)。它们之间的关系是：</p>
<ul>
<li>环境向行动者提供了<strong>观察</strong>(observation)（函数输入）</li>
<li>行动者在环境中采取<strong>行动</strong>(action)（函数输出）</li>
</ul>
<p>所以行动者就是强化学习中要找的“函数”：行动<span class="heti-skip"><span class="heti-spacing"> </span>= f(<span class="heti-spacing"> </span></span>观察<span><span class="heti-spacing"> </span>)</span>。函数的目标是找到一个策略，来最大化行动者从环境中得到的总<strong>奖励</strong>(reward)，而这个奖励来自行动者和环境的交互中。</p>
<div style="text-align: center">
<img src="images/lec11/1.png" width="70%/"/>
</div>
<details class="example" open="open">
<summary>例子</summary>
<div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="" id="__tabbed_1_1" name="__tabbed_1" type="radio"/><input id="__tabbed_1_2" name="__tabbed_1" type="radio"/><div class="tabbed-labels"><label for="__tabbed_1_1">电子游戏：太空侵略者</label><label for="__tabbed_1_2">围棋</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p>我们将前面介绍的抽象模型套到这个最经典的电子游戏之一，以便我们更好地理解强化学习的基本概念。</p>
<p></p><div style="text-align: center">
<img src="images/lec11/2.png" width="70%/"/>
</div>
<ul>
<li>环境即为整个游戏程序，而游戏界面对应行动者的观察</li>
<li>行动者是玩家操控的飞船，它可以发起的行动有：左移、右移和开火（<del>当然一动不动也可以算作一种行动</del>）</li>
<li>奖励就是左上角的分数，击落外星人即可得分</li>
<li>游戏的终止条件是：要么击败所有外星人，要么玩家的飞船被外星人摧毁</li>
</ul>
<p>将游戏画面和上面的结构图结合起来，得到以下结构图：</p>
<div class="tabbed-set tabbed-alternate" data-tabs="2:2"><input checked="" id="__tabbed_2_1" name="__tabbed_2" type="radio"/><input id="__tabbed_2_2" name="__tabbed_2" type="radio"/><div class="tabbed-labels"><label for="__tabbed_2_1">右移</label><label for="__tabbed_2_2">开火</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p></p><div style="text-align: center">
<img src="images/lec11/3.png" width="70%/"/>
</div>
</div>
<div class="tabbed-block">
<p></p><div style="text-align: center">
<img src="images/lec11/4.png" width="70%/"/>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<p><span>AlphaGo<span class="heti-spacing"> </span></span>也是基于强化学习训练出来的，下面是简化后的结构图：</p>
<p></p><div style="text-align: center">
<img src="images/lec11/5.png" width="70%/"/>
</div>
</div>
</div>
</div>
</details>
<p>既然强化学习是机器学习的一种，那我们仍然可以在强化学习上套用机器学习的三个步骤（仍以太空侵略者为例<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>：</p>
<ol>
<li>
<p>构造带未知参数的函数</p>
<ul>
<li>行动者就是强化学习中的函数，里面也用到了神经网络结构，称为<strong>策略网络</strong>(policy network)</li>
<li>神经网络的输入是用向量或矩阵形式表示的机器观察，输出是输出层中每个神经元对应的行动，</li>
<li>由于每个输出神经元都会为各自的行动打分（概率<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，因此要根据这个分布采样，找出一个确定的行动</li>
<li>由于是“采样”，所以即便对于同一张画面，通过强化学习训练的模型也可能采取不同的行动</li>
<li>这个神经网络可以是<span><span class="heti-spacing"> </span>CNN</span>，也可以是简单的全连接网络；或者可以用<span class="heti-skip"><span class="heti-spacing"> </span>RNN<span class="heti-spacing"> </span></span>或<span class="heti-skip"><span class="heti-spacing"> </span>Transformer<span class="heti-spacing"> </span></span>来看多个游戏画面，以把握全局</li>
</ul>
<p></p><div style="text-align: center">
<img src="images/lec11/6.png" width="70%/"/>
</div>
</li>
<li>
<p>从训练数据中定义损失</p>
<ul>
<li>行动者会根据当前环境<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(s_i\)</span><span class="heti-spacing"> </span></span>采取行动<span><span class="heti-spacing"> </span><span class="arithmatex">\(a_i\)</span></span>，使环境更新至<span><span class="heti-spacing"> </span><span class="arithmatex">\(s_{i+1}\)</span></span>，对应的奖励为<span><span class="heti-spacing"> </span><span class="arithmatex">\(r_i\)</span></span></li>
<li>这里假设在第<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(T\)</span><span class="heti-spacing"> </span></span>轮交互后游戏结束，这整一个过程称为一个<strong>回合</strong>(episode)</li>
</ul>
<p></p><div style="text-align: center">
<img src="images/lec11/7.png" width="60%/"/>
<img src="images/lec11/8.png" width="30%/"/>
</div>
<ul>
<li>总奖励（在<span class="heti-skip"><span class="heti-spacing"> </span>RL<span class="heti-spacing"> </span></span>相关文献中也叫做返回<span><span class="heti-spacing"> </span>(return)</span>）记作<span><span class="heti-spacing"> </span><span class="arithmatex">\(R = \sum\limits_{t=1}^T r_t\)</span></span>，而这正是我们期望最大化的值（对应一般机器学习中损失的相反数）</li>
</ul>
</li>
<li>
<p>优化</p>
<ul>
<li>
<p>以更抽象的形式表示上述过程，其中序列<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\tau = \{s_1, a_1, s_2, a_2, \dots \}\)</span><span class="heti-spacing"> </span></span>称为<strong>轨迹</strong>(trajectory)</p>
<p></p><div style="text-align: center">
<img src="images/lec11/9.png" width="70%/"/>
</div>
</li>
<li>
<p>然而，要想训练这样的模型，难度可不小</p>
<ul>
<li>首先，行动是采样得到的，所以同一个输入可能有多个输出的可能</li>
<li>其次，除了行动者是可控的网络外，环境和奖励机制都是黑箱，而且都带有一定的随机性（在太空侵略者游戏中，我们不知道每次游戏出现什么画面）</li>
</ul>
</li>
<li>因此，类似<span><span class="heti-spacing"> </span>GAN</span>，<span>RL<span class="heti-spacing"> </span></span>的主要难点便是优化</li>
</ul>
</li>
</ol>
<h2 id="policy-gradient">Policy Gradient<a class="headerlink" href="#policy-gradient" title="Permanent link">⚓︎</a></h2>
<p><strong>策略梯度</strong><span>(policy gradient)<span class="heti-spacing"> </span></span>是<span class="heti-skip"><span class="heti-spacing"> </span>RL<span class="heti-spacing"> </span></span>常用的一种优化方法，下面将会详细介绍。</p>
<h3 id="how-to-control-the-actor">How to Control the Actor<a class="headerlink" href="#how-to-control-the-actor" title="Permanent link">⚓︎</a></h3>
<p>首先来看如何控制行动者。实际上行动者要做的事就是：对于给定的观察<span><span class="heti-spacing"> </span><span class="arithmatex">\(s\)</span></span>，希望采取或不采取某一行动<span><span class="heti-spacing"> </span><span class="arithmatex">\(\hat{a}\)</span></span>（这个<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\hat{a}\)</span><span class="heti-spacing"> </span></span>是一个标签，不过我们暂且不去管它是从哪里来的<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。而损失函数的定义为：<span class="arithmatex">\(L = \begin{cases}e &amp; \text{take action } \hat{a} \\ -e &amp; \text{don't take action } \hat{a} \end{cases}\)</span>（误差<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(e\)</span><span class="heti-spacing"> </span></span>可用交叉熵计算得到<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，因而优化的目标就是找到行动者的参数<span><span class="heti-spacing"> </span><span class="arithmatex">\(\theta^* = \arg \min\limits_\theta L\)</span></span>。</p>
<div style="text-align: center">
<img src="images/lec11/10.png" width="70%/"/>
</div>
<p>下面举个简单的例子：假如对于观察<span><span class="heti-spacing"> </span><span class="arithmatex">\(s\)</span></span>，我们希望行动者采取行动<span><span class="heti-spacing"> </span><span class="arithmatex">\(\hat{a}\)</span></span>；而对于观察<span><span class="heti-spacing"> </span><span class="arithmatex">\(s'\)</span></span>，我们不希望行动者采取行动<span><span class="heti-spacing"> </span><span class="arithmatex">\(\hat{a}'\)</span></span>。那么对应的损失函数就是<span><span class="heti-spacing"> </span><span class="arithmatex">\(L = e_1 - e_2\)</span></span>。</p>
<div style="text-align: center">
<img src="images/lec11/11.png" width="70%/"/>
</div>
<p>接下来扩展到一般情况：假如已经迭代了<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(N\)</span><span class="heti-spacing"> </span></span>轮，得到<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\{s_1, \hat{a}_1\}, \{s_2, \hat{a}_2\}, \dots, \{s_N, \hat{a}_N\}\)</span><span class="heti-spacing"> </span></span>以及对应的我们是否想要采取行动的期望（<del>不是概率论中的“期望”</del>）——采取行动为<span><span class="heti-spacing"> </span>+1</span>，不采取行动为<span><span class="heti-spacing"> </span>-1</span>，作为计算损失函数时误差前面的系数。那么损失函数就是一组误差的加减法，如下图所示：</p>
<div style="text-align: center">
<img src="images/lec11/12.png" width="60%/"/>
</div>
<p>更一般地，我们为每个数据对赋予不同的实数权重<span><span class="heti-spacing"> </span><span class="arithmatex">\(A_n\)</span></span>，而不是只有<span class="heti-skip"><span class="heti-spacing"> </span>-1<span class="heti-spacing"> </span></span>和<span class="heti-skip"><span class="heti-spacing"> </span>1<span class="heti-spacing"> </span></span>两种选择，毕竟我们的“希望”也有强弱之分。此时损失函数就是<span><span class="heti-spacing"> </span><span class="arithmatex">\(L = \sum A_n e_n\)</span></span>。现在的难点在于我们该如何确定<span><span class="heti-spacing"> </span><span class="arithmatex">\(A_n\)</span></span>，下面就来探讨这个问题。</p>
<div style="text-align: center">
<img src="images/lec11/13.png" width="50%/"/>
</div>
<h3 id="version-0">Version 0<a class="headerlink" href="#version-0" title="Permanent link">⚓︎</a></h3>
<p>最简单的做法是将每轮交互中产生的奖励<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(r_n\)</span><span class="heti-spacing"> </span></span>作为系数<span><span class="heti-spacing"> </span><span class="arithmatex">\(A_n\)</span></span>，这样就能激励那些符合我们期望的行为，而打压那些不符预期的行为。</p>
<div style="text-align: center">
<img src="images/lec11/14.png" width="70%/"/>
</div>
<p>然而，这是一种短视的做法！</p>
<ul>
<li>由于前面的行动会影响到接下来的行动，继而影响到后续的奖励，因此当前最优的做法并不一定让最终结果也是最好的（类似贪心算法）</li>
<li><strong>奖励延迟</strong>(reward delay)：行动者应当牺牲当前的奖励，以获取更多的长远奖励</li>
<li>否则的话，以太空侵略者游戏为例，飞船（行动者）就会一直开火（因为只有开火才有正向奖励）而不移动，但这显然是不对的</li>
</ul>
<h3 id="version-1">Version 1<a class="headerlink" href="#version-1" title="Permanent link">⚓︎</a></h3>
<p>既然当前行动会影响到后续行动，那么它对应的系数<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(A_t\)</span><span class="heti-spacing"> </span></span>除了看自己的奖励外，还要看之后行动的奖励。所以令<span><span class="heti-spacing"> </span><span class="arithmatex">\(A_t = G_t = \sum\limits_{n=t}^T r_n\)</span></span>，其中<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(G_t\)</span><span class="heti-spacing"> </span></span>称为<strong>累积奖励</strong>(cumulated reward)。</p>
<div style="text-align: center">
<img src="images/lec11/15.png" width="60%/"/>
</div>
<h3 id="version-2">Version 2<a class="headerlink" href="#version-2" title="Permanent link">⚓︎</a></h3>
<p>但上述方法有一个问题：当前行动的影响范围有限，可能对很后面的行动影响不是很大。改进办法是增加一个<strong>折扣因子</strong>(discount factor) <span class="arithmatex">\(\gamma &lt; 1\)</span>，此时系数<span><span class="heti-spacing"> </span><span class="arithmatex">\(A_t = G_t' = \sum\limits_{n=t}^N \gamma^{n-t} r_n\)</span></span>。这样当<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(n-t\)</span><span class="heti-spacing"> </span></span>很大时，<span><span class="arithmatex">\(\gamma^{n-t}\)</span><span class="heti-spacing"> </span></span>的值就接近于<span><span class="heti-spacing"> </span>0</span>，第<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(n\)</span><span class="heti-spacing"> </span></span>次行动对系数的计算影响就很小了。</p>
<div style="text-align: center">
<img src="images/lec11/16.png" width="60%/"/>
</div>
<h3 id="version-3">Version 3<a class="headerlink" href="#version-3" title="Permanent link">⚓︎</a></h3>
<p>一个事实是：奖励的好坏是相对的。比如今天一张满分<span class="heti-skip"><span class="heti-spacing"> </span>100<span class="heti-spacing"> </span></span>分的考卷，小明考了<span class="heti-skip"><span class="heti-spacing"> </span>70<span class="heti-spacing"> </span></span>分。如果大家都是八九十分的话，那么小明考的不太理想；如果大家都是勉强及格的话，那么小明考的还算不错。为了让系数的大小更有区分度，在算出累积奖励<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(G_t'\)</span><span class="heti-spacing"> </span></span>后还要再减去一个<strong>基线</strong>(baseline) <span class="arithmatex">\(b\)</span>，使得<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(G_t'\)</span><span class="heti-spacing"> </span></span>的值有正有负。这里的难点是如何寻找一个合适的基线。</p>
<div style="text-align: center">
<img src="images/lec11/17.png" width="60%/"/>
</div>
<h3 id="algorithm">Algorithm<a class="headerlink" href="#algorithm" title="Permanent link">⚓︎</a></h3>
<p>下面将规范表述策略梯度算法：</p>
<div class="language-py highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-1">1</a></span>
<span class="normal"><a href="#__codelineno-0-2">2</a></span>
<span class="normal"><a href="#__codelineno-0-3">3</a></span>
<span class="normal"><a href="#__codelineno-0-4">4</a></span>
<span class="normal"><a href="#__codelineno-0-5">5</a></span>
<span class="normal"><a href="#__codelineno-0-6">6</a></span>
<span class="normal"><a href="#__codelineno-0-7">7</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1"></a><span class="n">initialize</span> <span class="n">actor</span> <span class="n">network</span> <span class="n">parameters</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2"></a><span class="k">for</span> <span class="n">training</span> <span class="n">iteration</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span> <span class="n">to</span> <span class="n">T</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3"></a><span class="hll">    <span class="n">using</span> <span class="n">actor</span> <span class="n">theta</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="n">to</span> <span class="n">interact</span>
</span></span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4"></a><span class="hll">    <span class="n">obtain</span> <span class="n">data</span> <span class="p">{</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]},</span> <span class="p">{</span><span class="n">s</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">]},</span> <span class="o">...</span><span class="p">,</span> <span class="p">{</span><span class="n">s</span><span class="p">[</span><span class="n">N</span><span class="p">],</span> <span class="n">a</span><span class="p">[</span><span class="n">N</span><span class="p">]}</span>
</span></span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5"></a><span class="hll">    <span class="n">compute</span> <span class="n">A</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">...</span><span class="p">,</span> <span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">]</span>
</span></span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6"></a>    <span class="n">compute</span> <span class="n">loss</span> <span class="n">L</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7"></a>    <span class="n">theta</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">nabla_L</span>
</span></code></pre></div></td></tr></table></div>
<p>看起来和梯度下降法的区别不大，但高亮部分正是和梯度下降法最大的区别——训练数据的收集发生在训练阶段，而不是在训练前就准备好，这就意味着每一次迭代（参数更新）都要重新收集一波数据；换句话说，一个训练数据集仅能更新一次参数。</p>
<div style="text-align: center">
<img src="images/lec11/18.png" width="60%/"/>
</div>
<p>为什么是这样的呢？我们可以做以下（<del>可能不太恰当的</del>）类比：对于一位棋艺精湛的棋手（参数更新后的模型）和新入门的小白（参数更新前的模型<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，高手能一眼看破在小白眼里可能比较不错的手法，所以这对<span class="heti-skip"><span class="heti-spacing"> </span>ta<span class="heti-spacing"> </span></span>的棋艺进步没什么帮助。</p>
<h3 id="on-policy-vs-off-policy">On-policy v.s. Off-policy<a class="headerlink" href="#on-policy-vs-off-policy" title="Permanent link">⚓︎</a></h3>
<p>上述<span class="heti-skip"><span class="heti-spacing"> </span>RL<span class="heti-spacing"> </span></span>框架中，用于训练的行动者和用于和环境交互的行动者是相同的，我们称这种情况为<strong>在线策略</strong>(on-policy)。对应地，<strong>离线策略</strong><span>(off-policy)<span class="heti-spacing"> </span></span>就是两者不相同的情况。离线策略在这里不会细讲，不过值得一提的是这种方法的好处是训练当前参数<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\theta^i\)</span><span class="heti-spacing"> </span></span>可沿用上个参数<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\theta^{i-1}\)</span><span class="heti-spacing"> </span></span>的数据集，这样就无需每次更新都要重新收集数据。</p>
<div style="text-align: center">
<img src="images/lec11/19.png" width="60%/"/>
</div>
<p>一种非常经典的离线策略技术是<strong>近端策略优化</strong>(proximal policy optimization, PPO)，它的要点是用于训练的行动者要知道自己和用于和环境交互的行动者之间的差异。</p>
<h3 id="exploration">Exploration<a class="headerlink" href="#exploration" title="Permanent link">⚓︎</a></h3>
<p>在收集数据中，一种重要的技术叫做<strong>探索</strong>(exploration)。之所以需要探索，是因为我们希望行动者的行动具有一定的随机性，而随机性往往是在训练数据中体现出来的，这也正是从概率分布中采样某个行动的原因。否则行动者可能会倾向于采取某个特定行动，这显然不是我们想要的。</p>
<p>探索（或者说增大随机性）的方法包括：</p>
<ul>
<li>增大输出的熵</li>
<li>向参数添加噪音</li>
</ul>
<h2 id="actor-critic">Actor-Critic<a class="headerlink" href="#actor-critic" title="Permanent link">⚓︎</a></h2>
<p>现在我们向<span class="heti-skip"><span class="heti-spacing"> </span>RL<span class="heti-spacing"> </span></span>引入一个新的模块，叫做<strong>批评者</strong>(critic)。训练批评者对训练行动者有一定的帮助，不过在了解起到什么帮助之前先来看一下批评者的工作原理。</p>
<p>批评者要做的事是：对于给定的行动者<span><span class="heti-spacing"> </span><span class="arithmatex">\(\theta\)</span></span>，根据当前的观察<span><span class="heti-spacing"> </span><span class="arithmatex">\(s\)</span></span>（有时还会考虑采取的行动<span><span class="heti-spacing"> </span><span class="arithmatex">\(a\)</span></span>）来评判行动者的表现好坏。这个批评者的一般形式是一个<strong>值函数</strong>(value function) <span class="arithmatex">\(V^\theta(s)\)</span>，它的含义是：采用行动者<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\theta\)</span><span class="heti-spacing"> </span></span>后，根据看到的<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(s\)</span><span class="heti-spacing"> </span></span>时预期得到的<a href="#version-2">折扣累积奖励</a>。注意到函数上标<span><span class="heti-spacing"> </span><span class="arithmatex">\(\theta\)</span></span>，它意味着批评者的输出值取决于对行动者的评估。</p>
<div style="text-align: center">
<img src="images/lec11/20.png" width="70%/"/>
</div>
<h3 id="how-to-estimate-value-function">How to Estimate Value Function<a class="headerlink" href="#how-to-estimate-value-function" title="Permanent link">⚓︎</a></h3>
<p>那么接下来的问题是如何估计<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(V^\theta (s)\)</span><span class="heti-spacing"> </span></span>的值呢？下面给出一些常见的方法：</p>
<ul>
<li>
<p><strong>基于蒙特卡罗的方法</strong>(Monte-Carlo(MC) based approach)：批评者观察和环境交互的行动者<span><span class="heti-spacing"> </span><span class="arithmatex">\(\theta\)</span></span>。例如对于环境<span><span class="heti-spacing"> </span><span class="arithmatex">\(s_a\)</span></span>，批评者直到完成整个回合后才能参考对应的累积奖励<span><span class="heti-spacing"> </span><span class="arithmatex">\(G_a'\)</span></span></p>
<p></p><div style="text-align: center">
<img src="images/lec11/21.png" width="70%/"/>
</div>
</li>
<li>
<p><strong>时差法</strong>(temporal-difference(TD) approach)：该方法不会看后续整个回合的数据，而是只看当前环境下的数据对以及下一个环境，即<span><span class="heti-spacing"> </span><span class="arithmatex">\((s_t, a_t, r_t, s_{t+1})\)</span></span></p>
<ul>
<li>对于那些回合数很长甚至无限长的情况，前一种方法就要延迟很长的时间了，而这种方法就比较管用</li>
<li>
<p>观察<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(s_t\)</span><span class="heti-spacing"> </span></span>和<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(s_{t+1}\)</span><span class="heti-spacing"> </span></span>下值函数的值：</p>
<div class="arithmatex">\[
\begin{align*}
V^\theta (s_t) &amp; = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots \\
V^\theta (s_{t+1}) &amp; = r_{t+1} + \gamma r_{t+2} + \dots
\end{align*}
\]</div>
<p>两式相减，得到<span><span class="heti-spacing"> </span><span class="arithmatex">\(V^\theta (s_t) = \gamma V^\theta (s_{t+1}) + r_t\)</span></span>，或者<span><span class="heti-spacing"> </span><span class="arithmatex">\(r_t= \gamma V^\theta (s_{t+1}) - V^\theta (s_t)\)</span></span></p>
</li>
<li>
<p>现在的训练目标就是让<span><span class="heti-spacing"> </span><span class="arithmatex">\(\gamma V^\theta (s_{t+1}) - V^\theta (s_t)\)</span></span>（批评者的评估结果）和<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(r_t\)</span><span class="heti-spacing"> </span></span>越接近越好</p>
<p></p><div style="text-align: center">
<img src="images/lec11/22.png" width="70%/"/>
</div>
</li>
</ul>
</li>
</ul>
<details class="example" open="open">
<summary>例子：比较<span class="heti-skip"><span class="heti-spacing"> </span>MC<span class="heti-spacing"> </span></span>和<span><span class="heti-spacing"> </span>TD</span></summary>
<p>下面展示了批评者目前观察到的<span class="heti-skip"><span class="heti-spacing"> </span>8<span class="heti-spacing"> </span></span>个回合（为方便讨论，假设<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\gamma = 1\)</span><span class="heti-spacing"> </span></span>且忽略具体采取的行动<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>：</p>
<p><img align="left" alt="" src="images/lec11/23.png" width="30%"/></p>
<ul>
<li>不难得到<span><span class="heti-spacing"> </span><span class="arithmatex">\(V^\theta (s_b) = \dfrac{6}{8} = \dfrac{3}{4}\)</span></span></li>
<li>但<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(V^\theta (s_a)\)</span><span class="heti-spacing"> </span></span>的计算可能会有歧义<ul>
<li>如果用蒙特卡罗法做，<span class="arithmatex">\(V^\theta (s_a) = \dfrac{0}{1} = 0\)</span></li>
<li>如果用时差法做，<span class="arithmatex">\(V^\theta (s_a) = V^\theta (s_b) + r = \dfrac{3}{4} + 0 = \dfrac{3}{4}\)</span></li>
<li>两者的区别是：前者会认为<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(s_a\)</span><span class="heti-spacing"> </span></span>和<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(s_b\)</span><span class="heti-spacing"> </span></span>会相互影响，而后者并不这么认为</li>
</ul>
</li>
</ul>
</details>
<h3 id="version-35">Version 3.5<a class="headerlink" href="#version-35" title="Permanent link">⚓︎</a></h3>
<p>前面说过，训练行动者的<span class="heti-skip"><span class="heti-spacing"> </span>V3<span class="heti-spacing"> </span></span>中的难点是基线的选择。现在我们可以用批评者的评估结果（值函数）来替代基线，得到：</p>
<div style="text-align: center">
<img src="images/lec11/24.png" width="60%/"/>
</div>
<p>对于某个数据对<span><span class="heti-spacing"> </span><span class="arithmatex">\(\{s_t, a_t\}\)</span></span>，系数<span><span class="heti-spacing"> </span><span class="arithmatex">\(A_t = G_t' - V^\theta (s_t)\)</span></span>。</p>
<ul>
<li>
<p><span><span class="arithmatex">\(V^\theta (s_t)\)</span><span class="heti-spacing"> </span></span>的计算：由于行动者会从一个概率分布中采样选择一个行动，所以对于当前观察<span><span class="heti-spacing"> </span><span class="arithmatex">\(s_t\)</span></span>，它不一定会选择行动<span><span class="heti-spacing"> </span><span class="arithmatex">\(a_t\)</span></span>，而是有多种行动可能，因此计算时要考虑到各种可能，求的是一种平均值</p>
<p></p><div style="text-align: center">
<img src="images/lec11/25.png" width="60%/"/>
</div>
</li>
<li>
<p><span><span class="arithmatex">\(G_t'\)</span><span class="heti-spacing"> </span></span>的计算：需要考虑在<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(s_t\)</span><span class="heti-spacing"> </span></span>时采取<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(a_t\)</span><span class="heti-spacing"> </span></span>后的累积奖励</p>
<p></p><div style="text-align: center">
<img src="images/lec11/26.png" width="50%/"/>
</div>
</li>
</ul>
<p>这个方法的缺陷在于<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(G_t'\)</span><span class="heti-spacing"> </span></span>的计算仅考虑了一种采样可能，实际上对于<span><span class="heti-spacing"> </span><span class="arithmatex">\(s_t\)</span></span>，在沿<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(a_t\)</span><span class="heti-spacing"> </span></span>后到达<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(s_{t+1}\)</span><span class="heti-spacing"> </span></span>后，还有多条路可以选择。</p>
<h3 id="version-4">Version 4<a class="headerlink" href="#version-4" title="Permanent link">⚓︎</a></h3>
<p>基于上述分析，计算<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(G_t'\)</span><span class="heti-spacing"> </span></span>时可参考<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(V^\theta (s_t)\)</span><span class="heti-spacing"> </span></span>的计算，考虑多种行动可能（实际上也确实用到了值函数的值<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>：</p>
<div style="text-align: center">
<img src="images/lec11/27.png" width="60%/"/>
</div>
<p>所以<span><span class="heti-spacing"> </span><span class="arithmatex">\(A_t = r_t + V^\theta (s_{t+1}) - V^\theta (s_t)\)</span></span>。这种方法叫做<strong>优势行动者<span class="heti-skip"><span class="heti-spacing"> </span>-<span class="heti-spacing"> </span></span>批评者</strong>(advantage actor-critic)。</p>
<h3 id="tip">Tip<a class="headerlink" href="#tip" title="Permanent link">⚓︎</a></h3>
<p>采用行动者<span class="heti-skip"><span class="heti-spacing"> </span>-<span class="heti-spacing"> </span></span>批评者框架训练时，一个小技巧是让行动者和批评者共享一些网络参数。这样做之所以合理，是因为两者的输入是相同的，那么在早期处理中，两者的部分内容应该是相似的。</p>
<div style="text-align: center">
<img src="images/lec11/28.png" width="60%/"/>
</div>
<h3 id="outlook-deep-q-network">Outlook: Deep Q Network<a class="headerlink" href="#outlook-deep-q-network" title="Permanent link">⚓︎</a></h3>
<p>还有一种神奇的做法是只用批评者就知道要做什么行动，其中最知名的做法是深度<span class="heti-skip"><span class="heti-spacing"> </span>Q<span class="heti-spacing"> </span></span>网络<span><span class="heti-spacing"> </span>(deep Q network,</span> <strong>DQN</strong>)。<a href="https://arxiv.org/abs/1710.02298">下图</a>展示了<span class="heti-skip"><span class="heti-spacing"> </span>DQN<span class="heti-spacing"> </span></span>及其各种变体的表现。</p>
<div style="text-align: center">
<img src="images/lec11/29.png" width="50%/"/>
</div>
<h2 id="reward-shaping">Reward Shaping<a class="headerlink" href="#reward-shaping" title="Permanent link">⚓︎</a></h2>
<p>有时在一个回合内，大部分迭代中的奖励都是<span><span class="heti-spacing"> </span>0</span>（比如下围棋，机械臂拧螺丝等任务<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，那么系数<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(A_t\)</span><span class="heti-spacing"> </span></span>就无法很好体现某个行动的表现好坏。这时开发者需要定义一种额外的奖励来引导智能体<span class="heti-skip"><span class="heti-spacing"> </span>(agents)<span class="heti-spacing"> </span></span>要采取什么样的行动，这种技术就叫做<strong>奖励塑造</strong>(reward shaping)。</p>
<details class="example" open="open">
<summary>例子</summary>
<div class="tabbed-set tabbed-alternate" data-tabs="3:3"><input checked="" id="__tabbed_3_1" name="__tabbed_3" type="radio"/><input id="__tabbed_3_2" name="__tabbed_3" type="radio"/><input id="__tabbed_3_3" name="__tabbed_3" type="radio"/><div class="tabbed-labels"><label for="__tabbed_3_1">例<span><span class="heti-spacing"> </span>1</span>：<span>VisDoom<span class="heti-spacing"> </span></span></label><label for="__tabbed_3_2">例<span><span class="heti-spacing"> </span>2</span>：机械臂</label><label for="__tabbed_3_3">例<span><span class="heti-spacing"> </span>3</span>：基于好奇心的方法</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p><a href="https://vizdoom.cs.put.edu.pl/"><span>VisDoom<span class="heti-spacing"> </span></span></a>是一个<span class="heti-skip"><span class="heti-spacing"> </span>AI<span class="heti-spacing"> </span></span>机器人玩枪战游戏<span class="heti-skip"><span class="heti-spacing"> </span>DOOM<span class="heti-spacing"> </span></span>的项目。</p>
<p><span>PPT<span class="heti-spacing"> </span></span>上有一个关于<a href="https://openreview.net/forum?id=Hk3mPK5gg&amp;noteId=Hk3mPK5gg"><span class="heti-skip"><span class="heti-spacing"> </span>Visual Doom AI Competition @ CIG 2016<span class="heti-spacing"> </span></span></a>的例子，但是视频过长，读者可在有空的时候观看：</p>
<p></p><div class="video-container"><video alt="type:video" controls="" style="position:relative;width:100%;height:22.172vw"><source src="images/lec11/39.mp4" type="video/mp4"/></video></div>
</div>
<div class="tabbed-block">
<p>某个任务要求机械臂将蓝色的板子套在棍子上。如果奖励仅仅考虑让板子和棍子的距离越近越好，那么就会出现下面右边两张图的错误情况，这时无论机械臂怎么弄都没法成功。所以需要通过奖励塑造的技术设定合理的奖励，促使机械臂朝正确目标前进。</p>
<p></p><div style="text-align: center">
<img src="images/lec11/30.gif" width="70%/"/>
</div>
<p><a href="https://bair.berkeley.edu/blog/2017/12/20/reverse-curriculum/">相关链接</a></p>
</div>
<div class="tabbed-block">
<p>该方法的思路是：智能体看到新的且有意义的东西时会获得额外奖励。只是新的，但是没有意义的东西（比如不断变化的噪音）不应该作为奖励的来源。下面是一段<span class="heti-skip"><span class="heti-spacing"> </span>demo<span class="heti-spacing"> </span></span>视频：</p>
<p></p><div class="video-container"><video alt="type:video" controls="" style="position:relative;width:100%;height:22.172vw"><source src="images/lec11/40.mp4" type="video/mp4"/></video></div>
<p>相关链接：<a href="https://pathak22.github.io/noreward-rl/">官网</a>，<a href="https://arxiv.org/abs/1705.05363">论文</a></p>
</div>
</div>
</div>
</details>
<h2 id="no-reward-learning-from-demonstration">No Reward: Learning from Demonstration<a class="headerlink" href="#no-reward-learning-from-demonstration" title="Permanent link">⚓︎</a></h2>
<p>对于某些任务，奖励的定义相当困难。可即便是精心定义的奖励也可能为智能体会带来不受控制的行为，比如机器人可根据<a href="https://zh.wikipedia.org/zh-hans/%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%89%E5%AE%9A%E5%BE%8B">阿西莫夫三大定律</a>得到这样的结论：将人类关起来就能保障人类的生存，但这显然是不正确的。所以下面来看一些不用奖励进行<span class="heti-skip"><span class="heti-spacing"> </span>RL<span class="heti-spacing"> </span></span>的方法。</p>
<h3 id="imitation-learning">Imitation Learning<a class="headerlink" href="#imitation-learning" title="Permanent link">⚓︎</a></h3>
<div style="text-align: center">
<img src="images/lec11/31.png" width="60%/"/>
</div>
<p>在<strong>模仿学习</strong><span>(imitation learning)<span class="heti-spacing"> </span></span>中，行动者仍然与环境互动，但是不去使用奖励函数，而是参考<strong>专家的演示</strong>(demonstration)——机器会根据一系列关于专家的轨迹<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\{\hat{\tau}_1, \hat{\tau}_2, \dots, \hat{\tau}_K\}\)</span><span class="heti-spacing"> </span></span>来训练自己，从而达到“模仿”专家行为的效果。这种训练方法可用在无人驾驶、机器人等领域中。</p>
<p><img align="left" alt="" src="images/lec11/32.png" width="30%"/></p>
<p>聪明的读者也许会发现，这种方法和监督学习十分相似：训练的目标同样是缩小模型输出和基准事实（监督学习中的标签<span class="heti-skip"><span class="heti-spacing"> </span>/<span class="heti-spacing"> </span></span>模仿学习中的专家数据）的差距（如左图所示<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。所以模仿学习又称为<strong>行为克隆</strong>(behavior cloning)。</p>
<p><img align="right" alt="" src="images/lec11/33.png" width="30%"/></p>
<p>但这种训练方法有一个明显的问题：专家数据是不完整的，只是从所有情况中采样部分数据。比如在无人驾驶任务中，专家数据（一般）不会包含撞墙的数据（不要拿自己生命开玩笑<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，所以机器看到的都是正常驾驶的数据。那么在遇到意外情况时，机器就不知道如何应对了。</p>
<p>上述问题衍生出来的另一个问题是：机器会完全模仿专家的行为，即便某些行为是没有意义的（可能是专家的一些习惯等等<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。</p>
<details class="example">
<summary>例子</summary>
<p></p><div class="video-container"><video alt="type:video" controls="" style="position:relative;width:100%;height:22.172vw"><source src="images/lec11/41.mp4" type="video/mp4"/></video></div>
</details>
<h3 id="inverse-reinforcement-learning">Inverse Reinforcement Learning<a class="headerlink" href="#inverse-reinforcement-learning" title="Permanent link">⚓︎</a></h3>
<p>这是一般<span class="heti-skip"><span class="heti-spacing"> </span>RL<span class="heti-spacing"> </span></span>的流程图，可以看到它是基于环境和奖励来训练一个行动者的：</p>
<div style="text-align: center">
<img src="images/lec11/34.png" width="60%/"/>
</div>
<p>但是现在讨论的情况是没有奖励的，所以我们可以采用相反的顺序来训练模型：根据环境和专家演示来反推奖励应该是什么样子的，接着根据奖励函数来寻找最优的行动者。这便是<strong>逆强化学习</strong><span>(inverse reinforcement learning, IRL)<span class="heti-spacing"> </span></span>的大致流程。</p>
<div style="text-align: center">
<img src="images/lec11/35.png" width="60%/"/>
</div>
<p>也许有人想学出来的奖励函数形式上会不会过于简单，从而影响训练效果。大可不必有这种担忧，因为即便是简单的奖励函数也能产生千变万化的、复杂的行动者。</p>
<p><span>IRL<span class="heti-spacing"> </span></span>的一个原则是：<strong>老师永远是最棒的</strong>(the teacher is always the best)。这并不意味着机器要模仿专家的行为，而是假定专家的行为能够得到最高价值的奖励。而<span class="heti-skip"><span class="heti-spacing"> </span>IRL<span class="heti-spacing"> </span></span>的基本思路是：</p>
<ul>
<li>初始化<span style="color: cornflowerblue">行动者</span></li>
<li>在每轮迭代中<ul>
<li><span style="color: cornflowerblue">行动者</span>和<span style="color: yellow">环境</span>交互，得到一些轨迹</li>
<li>定义一个<span style="color: green">奖励函数</span>，使得老师的轨迹优于<span style="color: cornflowerblue">行动者</span></li>
<li><span style="color: cornflowerblue">行动者</span>基于新的<span style="color: green">奖励函数</span>最大化<span style="color: green">奖励</span></li>
</ul>
</li>
<li>输出奖励函数和从<span style="color: green">奖励函数</span>中学习的<span style="color: cornflowerblue">行动者</span></li>
</ul>
<p>对应的<span class="heti-skip"><span class="heti-spacing"> </span>IRL<span class="heti-spacing"> </span></span>框架图如下：</p>
<div style="text-align: center">
<img src="images/lec11/36.png" width="70%/"/>
</div>
<p>是不是感觉<span class="heti-skip"><span class="heti-spacing"> </span>IRL<span class="heti-spacing"> </span></span>和<span class="heti-skip"><span class="heti-spacing"> </span>GAN<span class="heti-spacing"> </span></span>很像？确实如此，我们可将行动者看作<span class="heti-skip"><span class="heti-spacing"> </span>GAN<span class="heti-spacing"> </span></span>的生成器，而把奖励函数看作<span class="heti-skip"><span class="heti-spacing"> </span>GAN<span class="heti-spacing"> </span></span>的判别器。可从下图直观感受<span class="heti-skip"><span class="heti-spacing"> </span>GAN<span class="heti-spacing"> </span></span>和<span class="heti-skip"><span class="heti-spacing"> </span>IRL<span class="heti-spacing"> </span></span>的共同之处：</p>
<div style="text-align: center">
<img src="images/lec11/37.png" width="70%/"/>
</div>
<details class="example">
<summary>例子（有关机器人的）</summary>
<div class="tabbed-set tabbed-alternate" data-tabs="4:2"><input checked="" id="__tabbed_4_1" name="__tabbed_4" type="radio"/><input id="__tabbed_4_2" name="__tabbed_4" type="radio"/><div class="tabbed-labels"><label for="__tabbed_4_1">例<span class="heti-skip"><span class="heti-spacing"> </span>1<span class="heti-spacing"> </span></span></label><label for="__tabbed_4_2">例<span><span class="heti-spacing"> </span>2</span></label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p></p><div class="video-container"><video alt="type:video" controls="" style="position:relative;width:100%;height:22.172vw"><source src="images/lec11/42.mp4" type="video/mp4"/></video></div>
</div>
<div class="tabbed-block">
<p>Chelsea Finn, Sergey Levine, Pieter Abbeel,  Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization, ICML, 2016</p>
<ul>
<li><a href="http://rll.berkeley.edu/gcl/">官网</a></li>
<li><a href="https://www.youtube.com/watch?v=hXxaepw0zAw">视频链接</a></li>
</ul>
</div>
</div>
</div>
</details>
<details class="abstract" open="open">
<summary>更多方法</summary>
<ul>
<li><a href="https://arxiv.org/abs/1807.04742">Visual Reinforcement Learning with Imagined Goals</a>, NIPS 2018</li>
<li><a href="https://arxiv.org/abs/1903.03698">Skew-Fit: State-Covering Self-Supervised Reinforcement Learning</a>, ICML 2020</li>
<li>
<p>Reinforcement learning with Imagined Goals (RIG)</p>
<p></p><div style="text-align: center">
<img src="images/lec11/38.png" width="80%/"/>
</div>
</li>
</ul>
</details></div>
<aside class="md-source-file">
<span class="md-source-file__fact">
<span class="md-icon" title="最后更新">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"></path></svg>
</span>
<span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime" title="2025年8月23日 18:28:24">2025年8月23日 18:28:24</span>
</span>
<span class="md-source-file__fact">
<span class="md-icon" title="创建日期">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"></path></svg>
</span>
<span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime" title="2024年10月7日 21:20:16">2024年10月7日 21:20:16</span>
</span>
</aside>
<p style="font-size: 30px; font-weight: 600">评论区</p>
<div>
    如果大家有什么问题或想法，欢迎在下方留言~
  </div>
<!-- Insert generated snippet here -->
<script async="" crossorigin="anonymous" data-category="Announcements" data-category-id="DIC_kwDOMAb9Zs4CfmpP" data-emit-metadata="0" data-input-position="bottom" data-lang="zh-CN" data-mapping="pathname" data-reactions-enabled="1" data-repo="noughtq/notebook" data-repo-id="R_kgDOMAb9Zg" data-strict="0" data-theme="preferred_color_scheme" src="https://giscus.app/client.js">
</script>
<!-- Synchronize Giscus theme with palette -->
<script>
    var giscus = document.querySelector("script[src*=giscus]")

    // Set palette on initial load
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
      var theme = palette.color.scheme === "slate"
        ? "transparent_dark"
        : "light"

      // Instruct Giscus to set theme
      giscus.setAttribute("data-theme", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener("DOMContentLoaded", function() {
      var ref = document.querySelector("[data-md-component=palette]")
      ref.addEventListener("change", function() {
        var palette = __md_get("__palette")
        if (palette && typeof palette.color === "object") {
          var theme = palette.color.scheme === "slate"
            ? "dark"
            : "light"

          // Instruct Giscus to change theme
          var frame = document.querySelector(".giscus-frame")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            "https://giscus.app"
          )
        }
      })
    })
  </script>
<!-- 标题计数器 -->
<link href="/css/counter.css" rel="stylesheet"/>
<!-- 主页个性化 -->
</article>
</div>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div>
<button class="md-top md-icon" data-md-component="top" hidden="" type="button">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  回到页面顶部
</button>
</main>
<footer class="md-footer">
<nav aria-label="页脚" class="md-footer__inner md-grid">
<a aria-label="上一页: Adaptation" class="md-footer__link md-footer__link--prev" href="10.html">
<div class="md-footer__button md-icon">
<svg viewbox="0 0 256 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L77.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z" fill="currentColor"></path></svg>
</div>
<div class="md-footer__title">
<span class="md-footer__direction">
                上一页
              </span>
<div class="md-ellipsis">
                Adaptation
              </div>
</div>
</a>
<a aria-label="下一页: Network Compression" class="md-footer__link md-footer__link--next" href="12.html">
<div class="md-footer__title">
<span class="md-footer__direction">
                下一页
              </span>
<div class="md-ellipsis">
                Network Compression
              </div>
</div>
<div class="md-footer__button md-icon">
<svg viewbox="0 0 256 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z" fill="currentColor"></path></svg>
</div>
</a>
</nav>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright" style="margin-left: 33.5%">
<div class="md-copyright__highlight" style="text-align: center">
        Copyright © 2024-2025 <a href="https://github.com/NoughtQ">NoughtQ</a>
</div>
    
    
      Powered by
      <a href="https://www.mkdocs.org/" rel="noopener" target="_blank">
        MkDocs
      </a>
      with theme
      <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
        Material
      </a>
      modified by
      <a href="https://github.com/NoughtQ" rel="noopener" target="_blank">
        NoughtQ
      </a>
<!-- <br> -->
<div style="text-align: center;">
<a href="https://icp.gov.moe/?keyword=20252357" target="_blank">萌ICP备20252357号</a>
</div>
</div>
<div class="md-social">
<a class="md-social__link" href="https://noughtq.top" rel="noopener" target="_blank" title="noughtq.top">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M277.8 8.6c-12.3-11.4-31.3-11.4-43.5 0l-224 208c-9.6 9-12.8 22.9-8 35.1S18.8 272 32 272h16v176c0 35.3 28.7 64 64 64h288c35.3 0 64-28.7 64-64V272h16c13.2 0 25-8.1 29.8-20.3s1.6-26.2-8-35.1zM240 320h32c26.5 0 48 21.5 48 48v96H192v-96c0-26.5 21.5-48 48-48" fill="currentColor"></path></svg>
</a>
<a class="md-social__link" href="https://blog.noughtq.top" rel="noopener" target="_blank" title="blog.noughtq.top">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M224 24c0-13.3 10.7-24 24-24 145.8 0 264 118.2 264 264 0 13.3-10.7 24-24 24s-24-10.7-24-24c0-119.3-96.7-216-216-216-13.3 0-24-10.7-24-24M80 96c26.5 0 48 21.5 48 48v224c0 26.5 21.5 48 48 48s48-21.5 48-48-21.5-48-48-48c-8.8 0-16-7.2-16-16v-64c0-8.8 7.2-16 16-16 79.5 0 144 64.5 144 144s-64.5 144-144 144S32 447.5 32 368V144c0-26.5 21.5-48 48-48m168 0c92.8 0 168 75.2 168 168 0 13.3-10.7 24-24 24s-24-10.7-24-24c0-66.3-53.7-120-120-120-13.3 0-24-10.7-24-24s10.7-24 24-24" fill="currentColor"></path></svg>
</a>
<a class="md-social__link" href="https://github.com/noughtq" rel="noopener" target="_blank" title="github.com">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg>
</a>
<a class="md-social__link" href="mailto:noughtq666@gmail.com" rel="noopener" target="_blank" title="">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m20 8-8 5-8-5V6l8 5 8-5m0-2H4c-1.11 0-2 .89-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../..", "features": ["content.action.edit", "content.action.view", "content.code.copy", "content.code.annotate", "content.footnote.tooltips", "navigation.tabs", "navigation.top", "navigation.footer", "navigation.indexes", "navigation.tracking", "navigation.prune", "search.share"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script>
<script src="../../assets/javascripts/bundle.92b07e13.min.js"></script>
<script src="../../js/anchor.js"></script>
<script src="../../js/katex.js"></script>
<script src="../../js/toc.js"></script>
<script src="../../js/typed.js"></script>
<script src="../../js/custom.js"></script>
<script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
<script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
</body>
</html>