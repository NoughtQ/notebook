<!DOCTYPE html>
<html class="no-js" lang="zh">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="NoughtQ的笔记本，主要记录一些 CS 相关的笔记" name="description"/>
<meta content="NoughtQ" name="author"/>
<link href="https://notebook.noughtq.top/ai/ml/5.html" rel="canonical"/>
<link href="4.html" rel="prev"/>
<link href="6.html" rel="next"/>
<link href="../../feed_rss_created.xml" rel="alternate" title="RSS 订阅" type="application/rss+xml"/>
<link href="../../feed_rss_updated.xml" rel="alternate" title="已更新内容的 RSS 订阅" type="application/rss+xml"/>
<link href="../../assets/favicon.png" rel="icon"/>
<meta content="mkdocs-1.6.1, mkdocs-material-9.6.15" name="generator"/>
<title>Transformer - NoughtQ的笔记本</title>
<link href="../../assets/stylesheets/main.342714a4.min.css" rel="stylesheet"/>
<link href="../../assets/stylesheets/palette.06af60db.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=JetBrains+Mono,+LXGW+WenKai+Screen+GB+Screen:300,300i,400,400i,700,700i%7CJetBrains+Mono,+Consolas:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"JetBrains Mono, LXGW WenKai Screen GB Screen";--md-code-font:"JetBrains Mono, Consolas"}</style>
<link href="../../css/heti.css" rel="stylesheet"/>
<link href="../../css/toc_extra.css" rel="stylesheet"/>
<link href="../../css/timeline.css" rel="stylesheet"/>
<link href="../../css/card.css" rel="stylesheet"/>
<link href="../../css/custom.css" rel="stylesheet"/>
<link href="../../css/extra_changelog.css" rel="stylesheet"/>
<link href="../../css/header.css" rel="stylesheet"/>
<link href="../../css/sidebar.css" rel="stylesheet"/>
<link href="https://unpkg.com/katex@0/dist/katex.min.css" rel="stylesheet"/>
<link href="https://gcore.jsdelivr.net/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css" rel="stylesheet"/>
<link href="https://gcore.jsdelivr.net/npm/lxgw-wenkai-webfont@1.1.0/style.css" rel="stylesheet"/>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&amp;display=swap" rel="stylesheet"/>
<script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-43NH8CVRCJ"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-43NH8CVRCJ",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-43NH8CVRCJ",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
<script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
</head>
<body data-md-color-accent="indigo" data-md-color-primary="indigo" data-md-color-scheme="slate" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#transformer">
          跳转至
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header" data-md-component="header">
<nav aria-label="页眉" class="md-header__inner md-grid">
<a aria-label="NoughtQ的笔记本" class="md-header__button md-logo" data-md-component="logo" href="../.." title="NoughtQ的笔记本">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.05 9H7.06V6h1.99V4.03H7.06v-1c0-1.11.89-1.99 1.99-1.99h5.98V8l2.47-1.5L20 8V1.04h1c1.05 0 2 .96 2 1.99V17c0 1.03-.95 2-2 2H9.05c-1.05 0-1.99-.95-1.99-2v-1h1.99v-2H7.06v-3h1.99zM1 18h2v-3H1v-2h2v-3H1V8h2V5h2v3H3v2h2v3H3v2h2v3H3v2h2v1h16v2H5a2 2 0 0 1-2-2v-1H1z"></path></svg>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            NoughtQ的笔记本
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              Transformer
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Dark Mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefer-color-scheme: dark)" data-md-color-primary="indigo" data-md-color-scheme="slate" id="__palette_0" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Dark Mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
</label>
<input aria-label="Light Mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefer-color-scheme: light)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_0" hidden="" title="Light Mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"></path></svg>
</label>
</form>
<script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="搜索" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="搜索" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
<svg viewbox="0 0 320 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path></svg>
</label>
<nav aria-label="查找" class="md-search__options">
<a aria-label="分享" class="md-search__icon md-icon" data-clipboard="" data-clipboard-text="" data-md-component="search-share" href="javascript:void(0)" tabindex="-1" title="分享">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg>
</a>
<button aria-label="清空当前内容" class="md-search__icon md-icon" tabindex="-1" title="清空当前内容" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
</button>
</nav>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="" tabindex="0">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
<ol class="md-search-result__list" role="presentation"></ol>
</div>
</div>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/noughtq/notebook" title="前往仓库">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"></path></svg>
</div>
<div class="md-source__repository">
    NoughtQ/Notebook
  </div>
</a>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<nav aria-label="标签" class="md-tabs" data-md-component="tabs">
<div class="md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../index.html">
          
  
  
    
  
  🏫主页

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../lang/index.html">
          
  
  
    
  
  🔡语言

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../math/index.html">
          
  
  
    
  
  📊数学相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../algorithms/index.html">
          
  
  
    
  
  🧮算法相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../software/index.html">
          
  
  
    
  
  💾软件相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../system/index.html">
          
  
  
    
  
  💻系统相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../web/index.html">
          
  
  
    
  
  🌏Web相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../sec/ctf-101/index.html">
          
  
  
    
  
  🛡️信息安全

        </a>
</li>
<li class="md-tabs__item md-tabs__item--active">
<a class="md-tabs__link" href="../index.html">
          
  
  
    
  
  🤖人工智能

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../misc/index.html">
          
  
  
    
  
  🗃️杂项

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../tools/index.html">
          
  
  
    
  
  🛠️工具

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../papers/index.html">
          
  
  
    
  
  📑论文阅读

        </a>
</li>
</ul>
</div>
</nav>
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="导航栏" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="NoughtQ的笔记本" class="md-nav__button md-logo" data-md-component="logo" href="../.." title="NoughtQ的笔记本">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.05 9H7.06V6h1.99V4.03H7.06v-1c0-1.11.89-1.99 1.99-1.99h5.98V8l2.47-1.5L20 8V1.04h1c1.05 0 2 .96 2 1.99V17c0 1.03-.95 2-2 2H9.05c-1.05 0-1.99-.95-1.99-2v-1h1.99v-2H7.06v-3h1.99zM1 18h2v-3H1v-2h2v-3H1V8h2V5h2v3H3v2h2v3H3v2h2v3H3v2h2v1h16v2H5a2 2 0 0 1-2-2v-1H1z"></path></svg>
</a>
    NoughtQ的笔记本
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/noughtq/notebook" title="前往仓库">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"></path></svg>
</div>
<div class="md-source__repository">
    NoughtQ/Notebook
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../index.html">
<span class="md-ellipsis">
    🏫主页
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../lang/index.html">
<span class="md-ellipsis">
    🔡语言
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../math/index.html">
<span class="md-ellipsis">
    📊数学相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../algorithms/index.html">
<span class="md-ellipsis">
    🧮算法相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../software/index.html">
<span class="md-ellipsis">
    💾软件相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../system/index.html">
<span class="md-ellipsis">
    💻系统相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../web/index.html">
<span class="md-ellipsis">
    🌏Web相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../sec/ctf-101/index.html">
<span class="md-ellipsis">
    🛡️信息安全
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_9" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../index.html">
<span class="md-ellipsis">
    🤖人工智能
    
  </span>
</a>
<label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_9_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_9">
<span class="md-nav__icon md-icon"></span>
            🤖人工智能
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_9_2" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="index.html">
<span class="md-ellipsis">
    机器学习
    
  </span>
</a>
<label class="md-nav__link" for="__nav_9_2" id="__nav_9_2_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_9_2_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_9_2">
<span class="md-nav__icon md-icon"></span>
            机器学习
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="1.html">
<span class="md-ellipsis">
    Machine Learning - P1
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="2.html">
<span class="md-ellipsis">
    Machine Learning - P2
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="3.html">
<span class="md-ellipsis">
    Deep Learning
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="4.html">
<span class="md-ellipsis">
    Self-Attention
    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
<span class="md-ellipsis">
    Transformer
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="5.html">
<span class="md-ellipsis">
    Transformer
    
  </span>
</a>
<nav aria-label="目录" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
        目录
      </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#sequence-to-sequence-seq2seq">
<span class="md-ellipsis">
      Sequence-to-sequence (Seq2seq)
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#transformer_1">
<span class="md-ellipsis">
      Transformer
    </span>
</a>
<nav aria-label="Transformer" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#encoder">
<span class="md-ellipsis">
      Encoder
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#decoder">
<span class="md-ellipsis">
      Decoder
    </span>
</a>
<nav aria-label="Decoder" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#autoregressive-at">
<span class="md-ellipsis">
      Autoregressive (AT)
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#non-autoregressive-nat">
<span class="md-ellipsis">
      Non-Autoregressive (NAT)
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#encoder-decoder">
<span class="md-ellipsis">
      Encoder-Decoder
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#training">
<span class="md-ellipsis">
      Training
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#tips">
<span class="md-ellipsis">
      Tips
    </span>
</a>
<nav aria-label="Tips" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#copy-mechanism">
<span class="md-ellipsis">
      Copy Mechanism
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#guided-attention">
<span class="md-ellipsis">
      Guided Attention
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#beam-search">
<span class="md-ellipsis">
      Beam Search
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#optimizing-evaluation-metrics">
<span class="md-ellipsis">
      Optimizing Evaluation Metrics
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#scheduled-sampling">
<span class="md-ellipsis">
      Scheduled Sampling
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="6.html">
<span class="md-ellipsis">
    Generative Adversarial Network
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="7.html">
<span class="md-ellipsis">
    Self-Supervised Learning
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="8.html">
<span class="md-ellipsis">
    Explainable Machine Learning
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../cv/index.html">
<span class="md-ellipsis">
    计算机视觉导论
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../misc/index.html">
<span class="md-ellipsis">
    🗃️杂项
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../tools/index.html">
<span class="md-ellipsis">
    🛠️工具
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../papers/index.html">
<span class="md-ellipsis">
    📑论文阅读
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="目录" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
        目录
      </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#sequence-to-sequence-seq2seq">
<span class="md-ellipsis">
      Sequence-to-sequence (Seq2seq)
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#transformer_1">
<span class="md-ellipsis">
      Transformer
    </span>
</a>
<nav aria-label="Transformer" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#encoder">
<span class="md-ellipsis">
      Encoder
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#decoder">
<span class="md-ellipsis">
      Decoder
    </span>
</a>
<nav aria-label="Decoder" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#autoregressive-at">
<span class="md-ellipsis">
      Autoregressive (AT)
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#non-autoregressive-nat">
<span class="md-ellipsis">
      Non-Autoregressive (NAT)
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#encoder-decoder">
<span class="md-ellipsis">
      Encoder-Decoder
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#training">
<span class="md-ellipsis">
      Training
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#tips">
<span class="md-ellipsis">
      Tips
    </span>
</a>
<nav aria-label="Tips" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#copy-mechanism">
<span class="md-ellipsis">
      Copy Mechanism
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#guided-attention">
<span class="md-ellipsis">
      Guided Attention
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#beam-search">
<span class="md-ellipsis">
      Beam Search
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#optimizing-evaluation-metrics">
<span class="md-ellipsis">
      Optimizing Evaluation Metrics
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#scheduled-sampling">
<span class="md-ellipsis">
      Scheduled Sampling
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<a class="md-content__button md-icon" href="https://github.com/noughtq/notebook/edit/master/docs/ai/ml/5.md" rel="edit" title="编辑此页">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
</a>
<a class="md-content__button md-icon" href="https://github.com/noughtq/notebook/raw/master/docs/ai/ml/5.md" title="查看本页的源代码">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
</a>
<div><h1 id="transformer">Transformer<a class="headerlink" href="#transformer" title="Permanent link">⚓︎</a></h1>
<div style="margin-top: -30px; font-size: 0.9em; opacity: 0.7;">
<p><span class="twemoji"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10h-2a8 8 0 0 1-8 8 8 8 0 0 1-8-8 8 8 0 0 1 8-8zm6.78 1a.7.7 0 0 0-.48.2l-1.22 1.21 2.5 2.5L20.8 5.7c.26-.26.26-.7 0-.95L19.25 3.2c-.13-.13-.3-.2-.47-.2m-2.41 2.12L9 12.5V15h2.5l7.37-7.38z"></path></svg></span> 约<span class="heti-skip"><span class="heti-spacing"> </span>3669<span class="heti-spacing"> </span></span>个字 <span class="twemoji"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 20c4.42 0 8-3.58 8-8s-3.58-8-8-8-8 3.58-8 8 3.58 8 8 8m0-18c5.5 0 10 4.5 10 10s-4.5 10-10 10C6.47 22 2 17.5 2 12S6.5 2 12 2m.5 11H11V7h1.5v4.26l3.7-2.13.75 1.3z"></path></svg></span> 预计阅读时间<span class="heti-skip"><span class="heti-spacing"> </span>18<span class="heti-spacing"> </span></span>分钟</p>
</div>
<h2 id="sequence-to-sequence-seq2seq">Sequence-to-sequence (Seq2seq)<a class="headerlink" href="#sequence-to-sequence-seq2seq" title="Permanent link">⚓︎</a></h2>
<p>现在我们考虑一种<strong>输入和输出都是序列</strong><span>(sequence)<span class="heti-spacing"> </span></span>的模型。这类模型的输出长度由模型自己决定。像语音识别，机器翻译，语音翻译等场景下的模型都属于这类模型。</p>
<div style="text-align: center">
<img src="images/lec5/1.png" width="70%/"/>
</div>
<p>下面介绍一些<span class="heti-skip"><span class="heti-spacing"> </span>Seq2seq<span class="heti-spacing"> </span></span>的具体应用：</p>
<details class="example" open="open">
<summary>例子</summary>
<div class="tabbed-set tabbed-alternate" data-tabs="1:7"><input checked="" id="__tabbed_1_1" name="__tabbed_1" type="radio"/><input id="__tabbed_1_2" name="__tabbed_1" type="radio"/><input id="__tabbed_1_3" name="__tabbed_1" type="radio"/><input id="__tabbed_1_4" name="__tabbed_1" type="radio"/><input id="__tabbed_1_5" name="__tabbed_1" type="radio"/><input id="__tabbed_1_6" name="__tabbed_1" type="radio"/><input id="__tabbed_1_7" name="__tabbed_1" type="radio"/><div class="tabbed-labels"><label for="__tabbed_1_1">例<span><span class="heti-spacing"> </span>1</span>：闽南语<span class="heti-skip"><span class="heti-spacing"> </span>/<span class="heti-spacing"> </span></span>台语</label><label for="__tabbed_1_2">例<span><span class="heti-spacing"> </span>2</span>：文本<span class="heti-skip"><span class="heti-spacing"> </span>-<span class="heti-spacing"> </span></span>语音<span class="heti-skip"><span class="heti-spacing"> </span>(TTS)<span class="heti-spacing"> </span></span>合成</label><label for="__tabbed_1_3">例<span><span class="heti-spacing"> </span>3</span>：聊天机器人</label><label for="__tabbed_1_4">例<span><span class="heti-spacing"> </span>4</span>：<span>NLP<span class="heti-spacing"> </span></span></label><label for="__tabbed_1_5">例<span><span class="heti-spacing"> </span>5</span>：句法分析<span class="heti-skip"><span class="heti-spacing"> </span>(syntactic parsing)<span class="heti-spacing"> </span></span></label><label for="__tabbed_1_6">例<span><span class="heti-spacing"> </span>6</span>：多标签分类<span class="heti-skip"><span class="heti-spacing"> </span>(multi-label classification)<span class="heti-spacing"> </span></span></label><label for="__tabbed_1_7">例<span><span class="heti-spacing"> </span>7</span>：目标检测<span><span class="heti-spacing"> </span>(object detection)</span></label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p></p><div style="text-align: center">
<img src="images/lec5/2.png" width="70%/"/>
</div>
<p>在训练的时候，我们不去考虑：</p>
<ul>
<li>（肥皂剧的）背景音乐和噪音</li>
<li>（和声音对不上的）字幕</li>
<li>闽南语的发音</li>
</ul>
<p>像这样不去考虑各种因素，直接拿数据训练的行为，李宏毅老师称之为“<strong>硬<span class="heti-skip"><span class="heti-spacing"> </span>train<span class="heti-spacing"> </span></span>一发</strong>”~</p>
</div>
<div class="tabbed-block">
<p></p><div style="text-align: center">
<img src="images/lec5/3.png" width="70%/"/>
</div>
</div>
<div class="tabbed-block">
<p></p><div style="text-align: center">
<img src="images/lec5/4.png" width="70%/"/>
</div>
</div>
<div class="tabbed-block">
<p>其实很多自然语言处理<span class="heti-skip"><span class="heti-spacing"> </span>(NLP)<span class="heti-spacing"> </span></span>的任务都可以看作是问题回答<span class="heti-skip"><span class="heti-spacing"> </span>(QA)<span class="heti-spacing"> </span></span>的任务，即使是一些看上去和<span class="heti-skip"><span class="heti-spacing"> </span>QA<span class="heti-spacing"> </span></span>没啥关系的任务也都可以想象成是<span><span class="heti-spacing"> </span>QA</span>，比如机器翻译，为文章做摘要，情感分析<span><span class="heti-spacing"> </span>(sentiment analysis)</span>（下图就是其中一个例子）等任务。</p>
<p></p><div style="text-align: center">
<img src="images/lec5/5.png" width="70%/"/>
</div>
<p>而<span class="heti-skip"><span class="heti-spacing"> </span>QA<span class="heti-spacing"> </span></span>这类问题就可以用<span class="heti-skip"><span class="heti-spacing"> </span>seq2seq<span class="heti-spacing"> </span></span>的模型来解决，相应的示意图如下所示：</p>
<p></p><div style="text-align: center">
<img src="images/lec5/6.png" width="70%/"/>
</div>
<p>但需要注意的是，多数<span class="heti-skip"><span class="heti-spacing"> </span>NLP<span class="heti-spacing"> </span></span>任务，特别是和语音相关的任务，往往需要为这些任务定制模型，这样才能得到更好的结果；换句话说，<span>seq2seq<span class="heti-spacing"> </span></span>模型就像一把瑞士军刀，什么都能做，但给出的结果不一定是最好的。</p>
</div>
<div class="tabbed-block">
<p></p><div style="text-align: center">
<img src="images/lec5/7.png" width="70%/"/>
</div>
</div>
<div class="tabbed-block">
<div class="admonition warning">
<p class="admonition-title">注意</p>
<p>不要和前面介绍过的多类分类<span class="heti-skip"><span class="heti-spacing"> </span>(multi-class classfication)<span class="heti-spacing"> </span></span>弄混淆（前面介绍的分类都属于这种<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。</p>
</div>
<p><img align="right" alt="" src="images/lec5/8.png" width="50%"/></p>
<p>多标签分类任务允许某个对象属于多个类中（而原先介绍的分类问题每个对象仅属于一个类）</p>
<p></p><div style="text-align: center">
<img src="images/lec5/9.png" width="70%/"/>
</div>
</div>
<div class="tabbed-block">
<p></p><div style="text-align: center">
<img src="images/lec5/10.png" width="80%/"/>
</div>
</div>
</div>
</div>
</details>
<blockquote>
<p>上面的众多例子是想让读者感受到<span class="heti-skip"><span class="heti-spacing"> </span>seq2seq<span class="heti-spacing"> </span></span>模型的强大！</p>
</blockquote>
<p>那么<span class="heti-skip"><span class="heti-spacing"> </span>seq2seq<span class="heti-spacing"> </span></span>模型具体是如何实现的呢？一般的<span class="heti-skip"><span class="heti-spacing"> </span>seq2seq<span class="heti-spacing"> </span></span>模型的组成如下所示：</p>
<div style="text-align: center">
<img src="images/lec5/11.png" width="40%/"/>
</div>
<ul>
<li><strong>编码器</strong>(encoder)：处理输入序列，并将处理好的结果传给解码器</li>
<li><strong>解码器</strong>(decoder)：决定输出序列</li>
</ul>
<p>下面这篇<span class="heti-skip"><span class="heti-spacing"> </span>paper<span class="heti-spacing"> </span></span>是<span class="heti-skip"><span class="heti-spacing"> </span>seq2seq<span class="heti-spacing"> </span></span>模型的最早起源：</p>
<div style="text-align: center">
<img src="images/lec5/12.png" width="70%/"/>
</div>
<p>当然，最知名的模型莫过于 <a href="https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)"><strong>Transformer</strong></a>（它也是目前众多主流<span class="heti-skip"><span class="heti-spacing"> </span>LLM<span class="heti-spacing"> </span></span>的基石，可见其重要性<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，所以下面将会详细介绍！</p>
<div style="text-align: center">
<img src="images/lec5/13.jpeg" width="50%/["/>
</div>
<h2 id="transformer_1">Transformer<a class="headerlink" href="#transformer_1" title="Permanent link">⚓︎</a></h2>
<h3 id="encoder">Encoder<a class="headerlink" href="#encoder" title="Permanent link">⚓︎</a></h3>
<p>简单来说，编码器要做的事就是对于给定的一组（输入）向量，输出一组向量。也许读者会想：之前介绍的<a href="4.html#self-attention-vs-rnn"><span class="heti-skip"><span class="heti-spacing"> </span>RNN<span class="heti-spacing"> </span></span></a>或<a href="3.html#cnn"><span class="heti-skip"><span class="heti-spacing"> </span>CNN<span class="heti-spacing"> </span></span></a>也能完成这样的任务，那么这个编码器的高明之处在哪里呢？那我们就得关注编码器的具体实现了（右图）~</p>
<div style="text-align: center">
<img src="images/lec5/14.png" width="70%/"/>
</div>
<p>当然右图的结构有些复杂，所以下面给出简化后的样子：</p>
<div style="text-align: center">
<img src="images/lec5/15.png" width="40%/"/>
</div>
<p>可以看到，编码器内部被划分成多个块<span><span class="heti-spacing"> </span>(block)</span>，每个块要做的就是接收一组向量，然后输出新的一组向量，传递给下一个块。而每个块内部的实现如下——实际上用的就是前面刚介绍的<a href="4.html">自注意机制</a><span class="heti-skip"><span class="heti-spacing"> </span>+<span class="heti-spacing"> </span></span><a href="3.html">全连接神经网络</a></p>
<div style="text-align: center">
<img src="images/lec5/16.png" width="40%/"/>
</div>
<p>注意：前面的实现是简化过的，在<span class="heti-skip"><span class="heti-spacing"> </span>Transformer<span class="heti-spacing"> </span></span>中真正的实现是长这样子的（就拿其中一个输入向量分析<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>：</p>
<div style="text-align: center">
<img src="images/lec5/17.png" width="70%/"/>
</div>
<ul>
<li>记经过自注意处理得到的向量为<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{a}\)</span></span>，原输入向量为<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{b}\)</span></span></li>
<li>先做一次<strong>残差连接</strong>(residual connection)：<span class="arithmatex">\(\bm{a} + \bm{b}\)</span></li>
<li>
<p>然后将残差连接后的结果做<strong>层归一化</strong><span>(layer normalization)<span class="heti-spacing"> </span></span>处理</p>
<ul>
<li>
<p>归一化要做的就是：
    $$
    \begin{bmatrix}x_1 \ x_2 \ \dots \ x_K\end{bmatrix} \Rightarrow \begin{bmatrix}x_1' \ x_2' \ \dots \ x_K'\end{bmatrix}
    $$</p>
<p>其中<span><span class="heti-spacing"> </span><span class="arithmatex">\(x_i' = \dfrac{x_i - m}{\sigma}\)</span></span>，<span><span class="arithmatex">\(m, \sigma\)</span><span class="heti-spacing"> </span></span>分别为原向量的均值和标准差</p>
</li>
</ul>
</li>
<li>
<p>接着将层归一化后的向量传给<span><span class="heti-spacing"> </span>FC</span>，做和自注意机制类似的处理（重复上面的步骤<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>：</p>
<p></p><div style="text-align: center">
<img src="images/lec5/18.png" width="20%/"/>
</div>
<p>图中红色边框的向量才是一个块的输出向量</p>
</li>
</ul>
<hr/>
<div style="text-align: center">
<img src="images/lec5/14.png" width="70%/"/>
</div>
<p>上面的分析其实还漏了一些细节：</p>
<ul>
<li>在进入自注意前，要对输入向量进行<a href="4.html#positional-encoding">位置编码</a></li>
<li>这里的自注意机制是<a href="4.html#multi-head-self-attention">多头自注意</a></li>
<li>右图中的<span class="heti-skip"><span class="heti-spacing"> </span>"Add &amp; Norm"<span class="heti-spacing"> </span></span>就是残差连接<span class="heti-skip"><span class="heti-spacing"> </span>+<span class="heti-spacing"> </span></span>层归一化处理</li>
</ul>
<p>知名的<a href="https://en.wikipedia.org/wiki/BERT_(language_model)"><span class="heti-skip"><span class="heti-spacing"> </span>BERT<span class="heti-spacing"> </span></span>模型</a>采用和<span class="heti-skip"><span class="heti-spacing"> </span>Transformer<span class="heti-spacing"> </span></span>编码器相同的网络架构。</p>
<p>原始的<span class="heti-skip"><span class="heti-spacing"> </span>Transformer<span class="heti-spacing"> </span></span>编码器中各模块的排列顺序不一定是最好的——有一些研究就是从这里出发做出一些改进：</p>
<div style="text-align: center">
<img src="images/lec5/19.png" width="70%/"/>
</div>
<h3 id="decoder">Decoder<a class="headerlink" href="#decoder" title="Permanent link">⚓︎</a></h3>
<p>解码器有<span class="heti-skip"><span class="heti-spacing"> </span>2<span class="heti-spacing"> </span></span>种，比较常见的是<strong>自回归</strong><span>(autoregressive)<span class="heti-spacing"> </span></span>的解码器，所以先来介绍这个。</p>
<h4 id="autoregressive-at">Autoregressive (AT)<a class="headerlink" href="#autoregressive-at" title="Permanent link">⚓︎</a></h4>
<p>下面以语音识别这一应用为例介绍。</p>
<div style="text-align: center">
<img src="images/lec5/20.png" width="70%/"/>
</div>
<ul>
<li>解码器读取编码器的输出（至于怎么读<a href="#encoder-decoder">之后</a>再说）</li>
<li>解码器还接收一个表示开始<span class="heti-skip"><span class="heti-spacing"> </span>(START)<span class="heti-spacing"> </span></span>的特殊记号，这个记号可以用一个独热向量来表示</li>
<li>解码器输出的向量长度和词汇表（这里包含了所有常见的汉字）的长度一致（因此很长）</li>
<li>在解码器生成向量前要经过<a href="2.html#softmax"><span class="heti-skip"><span class="heti-spacing"> </span>softmax<span class="heti-spacing"> </span></span></a>处理（类似做分类任务<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，使得向量所有元素之和<span><span class="heti-spacing"> </span>= 1</span>，此时输出向量就是一个<strong>概率分布</strong>(distribution)</li>
<li>向量元素值最大的对应的字就是最终的输出</li>
</ul>
<p>接下来就要将第一个输出（上图的“机”）作为解码器的第二个输入（也用独热向量表示<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，对应也会得到一个输出（比如下图的“器”<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。之后的过程以此类推，所以就在赘述，可通过下面的示意图直观感受这一过程：</p>
<div style="text-align: center">
<img src="images/lec5/21.png" width="70%/"/>
</div>
<hr/>
<p>解码器的内部结构如下（比编码器更复杂<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>：</p>
<div style="text-align: center">
<img src="images/lec5/22.png" width="70%/"/>
</div>
<p>我们将编码器和解码器的结构图放在一起看，发现如果将解码器中间的模块遮起来后，二者的结构几乎一模一样，除了：</p>
<div style="text-align: center">
<img src="images/lec5/23.png" width="70%/"/>
</div>
<ul>
<li>因为解码器输出的是一个概率而非向量，因此最后要做一步<span class="heti-skip"><span class="heti-spacing"> </span>softmax<span class="heti-spacing"> </span></span>处理</li>
<li>解码器的自注意是一种<strong>掩码<span class="heti-skip"><span class="heti-spacing"> </span>(masked)<span class="heti-spacing"> </span></span>多头自注意</strong>，下面就来分析其细节</li>
</ul>
<p>先来回顾一下一般的自注意机制的样子：</p>
<div style="text-align: center">
<img src="images/lec5/24.png" width="70%/"/>
</div>
<p>而这是掩码后的自注意机制：</p>
<div style="text-align: center">
<img src="images/lec5/25.png" width="70%/"/>
</div>
<p>不难发现，在掩码版本的自注意机制中，输出向量<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{b}^i\)</span><span class="heti-spacing"> </span></span>只能读取其左边的输入向量（从<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{a}^1\)</span><span class="heti-spacing"> </span></span>到<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{a}^{i}\)</span></span><heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，不能看所有的输入向量。以<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{b}^2\)</span><span class="heti-spacing"> </span></span>为例，对应的示意图如下：</p>
<div style="text-align: center">
<img src="images/lec5/26.png" width="70%/"/>
</div>
<p>但为什么要这么做呢？答案就在解码器的工作机制中——前面提到过，解码器第<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(i\)</span><span class="heti-spacing"> </span></span>个位置上的输出向量是根据之前的输出向量（左侧）得到的，所以它只知道前面的向量是什么，无法得知所有的输入向量。这就是为什么解码器的自注意比较特殊的原因。</p>
<hr/>
<p>由于我们无法预先得知正确的输出长度，所以如果不加处理的话，模型可能会一直输出下去，停不下来了，这显然不符我们的预期。</p>
<div style="text-align: center">
<img src="images/lec5/27.png" width="70%/"/>
</div>
<p>和解码器最初接收的<span class="heti-skip"><span class="heti-spacing"> </span>START<span class="heti-spacing"> </span></span>特殊记号一样，我们引入一种停止记号，记作<span><span class="heti-spacing"> </span>END</span>。对于前面“机器学习”这四个字的语音识别，我们要求解码器在看到<span class="heti-skip"><span class="heti-spacing"> </span>BEGIN<span class="heti-spacing"> </span></span>记号和这四个字对应的输入向量后，就知道语音识别要结束了，应该输出<span class="heti-skip"><span class="heti-spacing"> </span>END<span class="heti-spacing"> </span></span>记号，从而结束输出。</p>
<div style="text-align: center">
<img src="images/lec5/28.png" width="70%/"/>
</div>
<h4 id="non-autoregressive-nat">Non-Autoregressive (NAT)<a class="headerlink" href="#non-autoregressive-nat" title="Permanent link">⚓︎</a></h4>
<blockquote>
<p>下面仅简单介绍<span><span class="heti-spacing"> </span>NAT</span>。</p>
</blockquote>
<p>对比自回归<span class="heti-skip"><span class="heti-spacing"> </span>(AT)<span class="heti-spacing"> </span></span>和非自回归<span class="heti-skip"><span class="heti-spacing"> </span>(non-autoregressive, NAT)<span class="heti-spacing"> </span></span>这两类解码器：</p>
<div style="text-align: center">
<img src="images/lec5/29.png" width="60%/"/>
</div>
<p>不同于<span class="heti-skip"><span class="heti-spacing"> </span>AT<span class="heti-spacing"> </span></span>解码器一次只吐一个输出，<span>NAT<span class="heti-spacing"> </span></span>解码器可以一次性吐出所有输出；并且<span class="heti-skip"><span class="heti-spacing"> </span>NAT<span class="heti-spacing"> </span></span>接收的是输入都是<span class="heti-skip"><span class="heti-spacing"> </span>BEGIN<span class="heti-spacing"> </span></span>记号。</p>
<ul>
<li>如何决定<span class="heti-skip"><span class="heti-spacing"> </span>NAT<span class="heti-spacing"> </span></span>解码器的输出长度<ul>
<li>用一个预测器来决定</li>
<li>允许输出一个非常长的序列，但是忽略<span class="heti-skip"><span class="heti-spacing"> </span>END<span class="heti-spacing"> </span></span>记号后面的输出</li>
</ul>
</li>
<li>优点：并行程度高，更易控制输出长度</li>
<li>但<span class="heti-skip"><span class="heti-spacing"> </span>NAT<span class="heti-spacing"> </span></span>通常不敌<span><span class="heti-spacing"> </span>AT</span>（原因：<strong>多模态</strong>(multi-modality)，这里不具体展开）</li>
</ul>
<h3 id="encoder-decoder">Encoder-Decoder<a class="headerlink" href="#encoder-decoder" title="Permanent link">⚓︎</a></h3>
<p>接下来考虑前面被我们忽略掉的一个问题：编码器和解码器之间是如何传递数据的？这个结构就藏在前面被遮挡的解码器模块——<strong>交叉注意</strong><span>(cross attention)<span class="heti-spacing"> </span></span>中：</p>
<div style="text-align: center">
<img src="images/lec5/30.png" width="60%/"/>
</div>
<p>注意到该模块有<span class="heti-skip"><span class="heti-spacing"> </span>3<span class="heti-spacing"> </span></span>个输入，其中有<span class="heti-skip"><span class="heti-spacing"> </span>2<span class="heti-spacing"> </span></span>个输入来自编码器的输出（蓝圈表示<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，另<span class="heti-skip"><span class="heti-spacing"> </span>1<span class="heti-spacing"> </span></span>个来自解码器内部。具体的结构如下：</p>
<div style="text-align: center">
<img src="images/lec5/31.png" width="80%/"/>
</div>
<p>其实和一般的自注意很像，不同之处在于<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{k}, \bm{v}\)</span><span class="heti-spacing"> </span></span>和<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{q}\)</span><span class="heti-spacing"> </span></span>是来自两个不同的地方（这应该就是<span class="heti-skip"><span class="heti-spacing"> </span>cross<span class="heti-spacing"> </span></span>的含义<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。</p>
<p>交叉注意的实际案例如下（注意：下图不来自于<span><span class="heti-spacing"> </span>Transformer</span><heti-adjacent class="heti-adjacent-half">）</heti-adjacent>：</p>
<div style="text-align: center">
<img src="images/lec5/32.png" width="80%/"/>
</div>
<blockquote>
<p>上图是编码器的输入（一列表示一个向量<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，下图是解码器的输出</p>
</blockquote>
<p>还有人研究了不同的交叉注意的效果：</p>
<div style="text-align: center">
<img src="images/lec5/33.png" width="80%/"/>
</div>
<h3 id="training">Training<a class="headerlink" href="#training" title="Permanent link">⚓︎</a></h3>
<p>前面只介绍了<span class="heti-skip"><span class="heti-spacing"> </span>Transformer<span class="heti-spacing"> </span></span>模型的运作机制（对应模型的<strong>测试</strong><span>(testing)<span class="heti-spacing"> </span></span>或<strong>推理</strong>(inference)<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，还没有讲过该如何<strong>训练</strong>模型。所以下面就来补充这块知识。</p>
<p>还是以“机器学习”这四个字的语音识别为例——</p>
<div style="text-align: center">
<img src="images/lec5/34.png" width="70%/"/>
</div>
<p>模型训练的目标就是让输出向量（概率分布）和<strong>基准事实</strong>(ground truth)（一种独热向量，就是标准答案）之间的差距越小越好。而这个差距就用<a href="2.html#loss"><strong>交叉熵</strong></a><span>(cross entropy)<span class="heti-spacing"> </span></span>来量化（和分类差不多<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，所以目标就是最小化这个交叉熵。</p>
<p>常用的一种训练方法叫做<strong>教师强迫</strong>(teacher forcing)（<del>好奇怪的术语</del><heti-adjacent class="heti-adjacent-half">）</heti-adjacent>：将基准事实作为解码器的输入（让解码器“偷看答案”<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，希望解码器的输出结果和标准答案越接近越好（即最小化交叉熵<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。</p>
<div style="text-align: center">
<img src="images/lec5/35.png" width="70%/"/>
</div>
<p>读者不难想到，这样的训练方法存在问题：测试的时候我们是让解码器读取前面的输出作为输入的，但训练的时候却读取基准事实，这中间就不太匹配。这个问题在后面会得到解决，先暂时放一下。</p>
<h3 id="tips">Tips<a class="headerlink" href="#tips" title="Permanent link">⚓︎</a></h3>
<div class="admonition warning">
<p class="admonition-title">注意</p>
<p>这里的<span class="heti-skip"><span class="heti-spacing"> </span>tips<span class="heti-spacing"> </span></span>不局限于<span><span class="heti-spacing"> </span>Transformer</span>，其他的<span class="heti-skip"><span class="heti-spacing"> </span>seq2seq<span class="heti-spacing"> </span></span>模型同样适用。</p>
</div>
<h4 id="copy-mechanism">Copy Mechanism<a class="headerlink" href="#copy-mechanism" title="Permanent link">⚓︎</a></h4>
<p>一般我们要求解码器自己产生输出，但有时解码器没有必要自己创造输出，而是从输入中复制一部分内容作为输出，这便是<strong>复制机制</strong>(copy mechanism)。下面列出一些可以用到这种技术的场景：</p>
<details class="example" open="open">
<summary>例子</summary>
<div class="tabbed-set tabbed-alternate" data-tabs="2:2"><input checked="" id="__tabbed_2_1" name="__tabbed_2" type="radio"/><input id="__tabbed_2_2" name="__tabbed_2" type="radio"/><div class="tabbed-labels"><label for="__tabbed_2_1">例<span><span class="heti-spacing"> </span>1</span>：聊天机器人</label><label for="__tabbed_2_2">例<span><span class="heti-spacing"> </span>2</span>：总结文章</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p></p><div style="text-align: center">
<img src="images/lec5/36.png" width="60%/"/>
</div>
</div>
<div class="tabbed-block">
<p></p><div style="text-align: center">
<img src="images/lec5/37.png" width="80%/"/>
</div>
</div>
</div>
</div>
</details>
<p>最早具备这种复制机制的模型是<strong>指针网络</strong>(pointer network)，之后还有一种称为复制网络<span class="heti-skip"><span class="heti-spacing"> </span>(copy network)<span class="heti-spacing"> </span></span>的变体，这里就不展开介绍了。</p>
<h4 id="guided-attention">Guided Attention<a class="headerlink" href="#guided-attention" title="Permanent link">⚓︎</a></h4>
<details class="bug" open="open">
<summary>问题</summary>
<p>在语音合成时，可能会遇到以下古怪的问题：</p>
<p></p><div style="text-align: center">
<img src="images/lec5/38.png" width="50%/"/>
</div>
<p>当机器连说多个“发财”的时候是正常的，但只说一个“发财”的时候，机器就只说了“财”没说“发”。这意味着模型可能漏掉了一个输入。</p>
</details>
<p>像语音合成、语音识别这样的任务，输入和输出都是单调对齐的<span><span class="heti-spacing"> </span>(monotonically aligned)</span>，就很有可能出现类似上面的问题。解决方案是一种被称为<strong>导向注意</strong><span>(guided attention)<span class="heti-spacing"> </span></span>的技术，强迫模型以固定的方式读取每一个输入。</p>
<div style="text-align: center">
<img src="images/lec5/39.png" width="80%/"/>
</div>
<p>以语音合成为例，我们希望模型的输出顺序是从左向右的，所以在运行的时候会为模型提供一个<strong>注意权重</strong>(attention weight)（或者注意分数<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，上图的曲线越高表示权重越大。这个权重会从左向右移动，从而确保输出是严格按照顺序来的。</p>
<p>如果不加这种约束，就很有可能出现这种乱序的情况，这显然是有问题的：</p>
<div style="text-align: center">
<img src="images/lec5/40.png" width="80%/"/>
</div>
<p>具体的导向注意应用有：</p>
<ul>
<li>单调注意<span><span class="heti-spacing"> </span>(monotonic attention)</span></li>
<li>位置感知注意<span><span class="heti-spacing"> </span>(location-aware attention)</span></li>
</ul>
<h4 id="beam-search">Beam Search<a class="headerlink" href="#beam-search" title="Permanent link">⚓︎</a></h4>
<div class="admonition info">
<p class="admonition-title">注</p>
<p>假设解码器只有两个可能的输出<span class="heti-skip"><span class="heti-spacing"> </span>A<span class="heti-spacing"> </span></span>和<span><span class="heti-spacing"> </span>B</span>。</p>
</div>
<p>下面的树状图列出了模型所有可能的输出情况</p>
<div style="text-align: center">
<img src="images/lec5/41.png" width="70%/"/>
</div>
<ul>
<li>红色路径是<strong>贪心解码</strong>(greedy decoding)</li>
<li>绿色路径是最好的路径</li>
</ul>
<p>要想找到最好的路径，最简单的做法是暴力搜索，把所有可能的路径遍历一遍。但路径树是可能输出数的指数，所以可能的输出数很多的时候，路径数就会变得特别大，显然是无法全部遍历一遍的。这时就要采用一种称为<strong>束搜索</strong><span>(beam search)<span class="heti-spacing"> </span></span>的近似算法，寻找近似解。一件有趣的事是，该算法有时得到不错的结果，有时结果会很烂。</p>
<div style="text-align: center">
<img src="images/lec5/42.png" width="80%/"/>
</div>
<p>比如这篇<span><span class="heti-spacing"> </span>paper</span>，两张图分别展示了用了束搜索和没用的结果。可以看到前者会陷入一个死循环（蓝色部分<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，而后者看着相对比较正常。这带给我们的启示是：有时没找到最佳路径不一定是件坏事，在某些任务（尤其是需要解码器发挥创造力的时候，比如句子补全，语音合成等）中为解码器引入一定的随机性可能有助于产生更好的结果。</p>
<h4 id="optimizing-evaluation-metrics">Optimizing Evaluation Metrics<a class="headerlink" href="#optimizing-evaluation-metrics" title="Permanent link">⚓︎</a></h4>
<p>课程作业采用的评估标准是<span><span class="heti-spacing"> </span>BLEU score</span>，它的做法是将解码器输出的完整句子和正确答案比较，也就是以句为单位做比较。但我们训练模型的时候是一个词一个词看的，分开来看每个词的交叉熵的。两者的关联不大，因此让交叉熵最小化不一定能提高<span class="heti-skip"><span class="heti-spacing"> </span>BLEU<span class="heti-spacing"> </span></span>分数。</p>
<div style="text-align: center">
<img src="images/lec5/43.png" width="70%/"/>
</div>
<p>那么怎样做优化呢？一个口诀是：如果不知道如何优化的话，那就用<strong>强化学习</strong><span>(RL)<span class="heti-spacing"> </span></span>解决！</p>
<h4 id="scheduled-sampling">Scheduled Sampling<a class="headerlink" href="#scheduled-sampling" title="Permanent link">⚓︎</a></h4>
<p>前面还放跑了一个问题：测试（左图）和训练（右图）结果的不匹配，这种现象称为<strong>暴露偏移</strong>(exposure bias)。由于测试的时候解码器读取的是之前的输出，假如之前的输出是错的，那么解码器就会根据这个错误的输入来接着输出，这样很有可能导致“一步错，步步错”的情况。</p>
<div style="text-align: center">
<img src="images/lec5/44.png" width="70%/"/>
</div>
<p>一种解决思路是：在训练的时候不要完全用基准事实，可以偶尔给一些错误的输入，这样反而模型会学的更好。这种技术称为<strong>调度采样</strong>(scheduled sampling)。</p>
<p>下面是一些有关该技术的参考资料，感兴趣的读者可自行到网上搜寻。</p>
<div style="text-align: center">
<img src="images/lec5/45.png" width="70%/"/>
</div></div>
<aside class="md-source-file">
<span class="md-source-file__fact">
<span class="md-icon" title="最后更新">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"></path></svg>
</span>
<span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime" title="2025年3月21日 15:57:34">2025年3月21日 15:57:34</span>
</span>
<span class="md-source-file__fact">
<span class="md-icon" title="创建日期">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"></path></svg>
</span>
<span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime" title="2025年3月2日 21:04:42">2025年3月2日 21:04:42</span>
</span>
</aside>
<p style="font-size: 30px; font-weight: 600">评论区</p>
<div>
    如果大家有什么问题或想法，欢迎在下方留言~
  </div>
<!-- Insert generated snippet here -->
<script async="" crossorigin="anonymous" data-category="Announcements" data-category-id="DIC_kwDOMAb9Zs4CfmpP" data-emit-metadata="0" data-input-position="bottom" data-lang="zh-CN" data-mapping="pathname" data-reactions-enabled="1" data-repo="noughtq/notebook" data-repo-id="R_kgDOMAb9Zg" data-strict="0" data-theme="preferred_color_scheme" src="https://giscus.app/client.js">
</script>
<!-- Synchronize Giscus theme with palette -->
<script>
    var giscus = document.querySelector("script[src*=giscus]")

    // Set palette on initial load
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
      var theme = palette.color.scheme === "slate"
        ? "transparent_dark"
        : "light"

      // Instruct Giscus to set theme
      giscus.setAttribute("data-theme", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener("DOMContentLoaded", function() {
      var ref = document.querySelector("[data-md-component=palette]")
      ref.addEventListener("change", function() {
        var palette = __md_get("__palette")
        if (palette && typeof palette.color === "object") {
          var theme = palette.color.scheme === "slate"
            ? "dark"
            : "light"

          // Instruct Giscus to change theme
          var frame = document.querySelector(".giscus-frame")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            "https://giscus.app"
          )
        }
      })
    })
  </script>
<!-- 标题计数器 -->
<link href="/css/counter.css" rel="stylesheet"/>
<!-- 主页个性化 -->
</article>
</div>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div>
<button class="md-top md-icon" data-md-component="top" hidden="" type="button">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  回到页面顶部
</button>
</main>
<footer class="md-footer">
<nav aria-label="页脚" class="md-footer__inner md-grid">
<a aria-label="上一页: Self-Attention" class="md-footer__link md-footer__link--prev" href="4.html">
<div class="md-footer__button md-icon">
<svg viewbox="0 0 320 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path></svg>
</div>
<div class="md-footer__title">
<span class="md-footer__direction">
                上一页
              </span>
<div class="md-ellipsis">
                Self-Attention
              </div>
</div>
</a>
<a aria-label="下一页: Generative Adversarial Network" class="md-footer__link md-footer__link--next" href="6.html">
<div class="md-footer__title">
<span class="md-footer__direction">
                下一页
              </span>
<div class="md-ellipsis">
                Generative Adversarial Network
              </div>
</div>
<div class="md-footer__button md-icon">
<svg viewbox="0 0 320 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg>
</div>
</a>
</nav>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright" style="margin-left: 33.5%">
<div class="md-copyright__highlight" style="text-align: center">
        Copyright © 2024-2025 <a href="https://github.com/NoughtQ">NoughtQ</a>
</div>
    
    
      Powered by
      <a href="https://www.mkdocs.org/" rel="noopener" target="_blank">
        MkDocs
      </a>
      with theme
      <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
        Material
      </a>
      modified by
      <a href="https://github.com/NoughtQ" rel="noopener" target="_blank">
        NoughtQ
      </a>
<!-- <br> -->
<div style="text-align: center;">
<a href="https://icp.gov.moe/?keyword=20252357" target="_blank">萌ICP备20252357号</a>
</div>
</div>
<div class="md-social">
<a class="md-social__link" href="https://github.com/noughtq" rel="noopener" target="_blank" title="github.com">
<svg viewbox="0 0 496 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
</a>
<a class="md-social__link" href="https://blog.noughtq.top" rel="noopener" target="_blank" title="blog.noughtq.top">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M192 32c0 17.7 14.3 32 32 32 123.7 0 224 100.3 224 224 0 17.7 14.3 32 32 32s32-14.3 32-32C512 128.9 383.1 0 224 0c-17.7 0-32 14.3-32 32m0 96c0 17.7 14.3 32 32 32 70.7 0 128 57.3 128 128 0 17.7 14.3 32 32 32s32-14.3 32-32c0-106-86-192-192-192-17.7 0-32 14.3-32 32m-96 16c0-26.5-21.5-48-48-48S0 117.5 0 144v224c0 79.5 64.5 144 144 144s144-64.5 144-144-64.5-144-144-144h-16v96h16c26.5 0 48 21.5 48 48s-21.5 48-48 48-48-21.5-48-48z"></path></svg>
</a>
<a class="md-social__link" href="mailto:noughtq666@gmail.com" rel="noopener" target="_blank" title="">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m20 8-8 5-8-5V6l8 5 8-5m0-2H4c-1.11 0-2 .89-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../..", "features": ["content.action.edit", "content.action.view", "content.code.copy", "content.code.annotate", "content.footnote.tooltips", "navigation.tabs", "navigation.top", "navigation.footer", "navigation.indexes", "navigation.tracking", "navigation.prune", "search.share"], "search": "../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script>
<script src="../../assets/javascripts/bundle.56ea9cef.min.js"></script>
<script src="../../js/anchor.js"></script>
<script src="../../js/katex.js"></script>
<script src="../../js/toc.js"></script>
<script src="../../js/typed.js"></script>
<script src="../../js/custom.js"></script>
<script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
<script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
</body>
</html>