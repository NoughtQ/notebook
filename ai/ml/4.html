<!DOCTYPE html>
<html class="no-js" lang="zh">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="NoughtQ的笔记本，主要记录一些 CS 相关的笔记" name="description"/>
<meta content="NoughtQ" name="author"/>
<link href="https://notebook.noughtq.top/ai/ml/4.html" rel="canonical"/>
<link href="3.html" rel="prev"/>
<link href="5.html" rel="next"/>
<link href="../../feed_rss_created.xml" rel="alternate" title="RSS 订阅" type="application/rss+xml"/>
<link href="../../feed_rss_updated.xml" rel="alternate" title="已更新内容的 RSS 订阅" type="application/rss+xml"/>
<link href="../../assets/favicon.png" rel="icon"/>
<meta content="mkdocs-1.6.1, mkdocs-material-9.6.14" name="generator"/>
<title>Self-Attention - NoughtQ的笔记本</title>
<link href="../../assets/stylesheets/main.342714a4.min.css" rel="stylesheet"/>
<link href="../../assets/stylesheets/palette.06af60db.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=JetBrains+Mono,+LXGW+WenKai+Screen+GB+Screen:300,300i,400,400i,700,700i%7CJetBrains+Mono,+Consolas:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"JetBrains Mono, LXGW WenKai Screen GB Screen";--md-code-font:"JetBrains Mono, Consolas"}</style>
<link href="../../css/heti.css" rel="stylesheet"/>
<link href="../../css/toc_extra.css" rel="stylesheet"/>
<link href="../../css/timeline.css" rel="stylesheet"/>
<link href="../../css/card.css" rel="stylesheet"/>
<link href="../../css/custom.css" rel="stylesheet"/>
<link href="../../css/extra_changelog.css" rel="stylesheet"/>
<link href="../../css/header.css" rel="stylesheet"/>
<link href="../../css/sidebar.css" rel="stylesheet"/>
<link href="https://unpkg.com/katex@0/dist/katex.min.css" rel="stylesheet"/>
<link href="https://gcore.jsdelivr.net/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css" rel="stylesheet"/>
<link href="https://gcore.jsdelivr.net/npm/lxgw-wenkai-webfont@1.1.0/style.css" rel="stylesheet"/>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&amp;display=swap" rel="stylesheet"/>
<script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-43NH8CVRCJ"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-43NH8CVRCJ",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-43NH8CVRCJ",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
<script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
</head>
<body data-md-color-accent="indigo" data-md-color-primary="indigo" data-md-color-scheme="slate" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#self-attention">
          跳转至
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header" data-md-component="header">
<nav aria-label="页眉" class="md-header__inner md-grid">
<a aria-label="NoughtQ的笔记本" class="md-header__button md-logo" data-md-component="logo" href="../.." title="NoughtQ的笔记本">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.05 9H7.06V6h1.99V4.03H7.06v-1c0-1.11.89-1.99 1.99-1.99h5.98V8l2.47-1.5L20 8V1.04h1c1.05 0 2 .96 2 1.99V17c0 1.03-.95 2-2 2H9.05c-1.05 0-1.99-.95-1.99-2v-1h1.99v-2H7.06v-3h1.99zM1 18h2v-3H1v-2h2v-3H1V8h2V5h2v3H3v2h2v3H3v2h2v3H3v2h2v1h16v2H5a2 2 0 0 1-2-2v-1H1z"></path></svg>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            NoughtQ的笔记本
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              Self-Attention
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Dark Mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefer-color-scheme: dark)" data-md-color-primary="indigo" data-md-color-scheme="slate" id="__palette_0" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Dark Mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
</label>
<input aria-label="Light Mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefer-color-scheme: light)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_0" hidden="" title="Light Mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"></path></svg>
</label>
</form>
<script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="搜索" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="搜索" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
<svg viewbox="0 0 320 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path></svg>
</label>
<nav aria-label="查找" class="md-search__options">
<a aria-label="分享" class="md-search__icon md-icon" data-clipboard="" data-clipboard-text="" data-md-component="search-share" href="javascript:void(0)" tabindex="-1" title="分享">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg>
</a>
<button aria-label="清空当前内容" class="md-search__icon md-icon" tabindex="-1" title="清空当前内容" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
</button>
</nav>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="" tabindex="0">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
<ol class="md-search-result__list" role="presentation"></ol>
</div>
</div>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/noughtq/notebook" title="前往仓库">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"></path></svg>
</div>
<div class="md-source__repository">
    NoughtQ/Notebook
  </div>
</a>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<nav aria-label="标签" class="md-tabs" data-md-component="tabs">
<div class="md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../index.html">
          
  
  
    
  
  🏫主页

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../lang/index.html">
          
  
  
    
  
  🔡语言

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../math/index.html">
          
  
  
    
  
  📊数学相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../algorithms/index.html">
          
  
  
    
  
  🧮算法相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../software/index.html">
          
  
  
    
  
  💾软件相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../system/index.html">
          
  
  
    
  
  💻系统相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../web/index.html">
          
  
  
    
  
  🌏Web相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../sec/ctf-101/index.html">
          
  
  
    
  
  🛡️信息安全

        </a>
</li>
<li class="md-tabs__item md-tabs__item--active">
<a class="md-tabs__link" href="../index.html">
          
  
  
    
  
  🤖人工智能

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../misc/index.html">
          
  
  
    
  
  🗃️杂项

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../tools/index.html">
          
  
  
    
  
  🛠️工具

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../papers/index.html">
          
  
  
    
  
  📑论文阅读

        </a>
</li>
</ul>
</div>
</nav>
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="导航栏" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="NoughtQ的笔记本" class="md-nav__button md-logo" data-md-component="logo" href="../.." title="NoughtQ的笔记本">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.05 9H7.06V6h1.99V4.03H7.06v-1c0-1.11.89-1.99 1.99-1.99h5.98V8l2.47-1.5L20 8V1.04h1c1.05 0 2 .96 2 1.99V17c0 1.03-.95 2-2 2H9.05c-1.05 0-1.99-.95-1.99-2v-1h1.99v-2H7.06v-3h1.99zM1 18h2v-3H1v-2h2v-3H1V8h2V5h2v3H3v2h2v3H3v2h2v3H3v2h2v1h16v2H5a2 2 0 0 1-2-2v-1H1z"></path></svg>
</a>
    NoughtQ的笔记本
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/noughtq/notebook" title="前往仓库">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"></path></svg>
</div>
<div class="md-source__repository">
    NoughtQ/Notebook
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../index.html">
<span class="md-ellipsis">
    🏫主页
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../lang/index.html">
<span class="md-ellipsis">
    🔡语言
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../math/index.html">
<span class="md-ellipsis">
    📊数学相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../algorithms/index.html">
<span class="md-ellipsis">
    🧮算法相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../software/index.html">
<span class="md-ellipsis">
    💾软件相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../system/index.html">
<span class="md-ellipsis">
    💻系统相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../web/index.html">
<span class="md-ellipsis">
    🌏Web相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../sec/ctf-101/index.html">
<span class="md-ellipsis">
    🛡️信息安全
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_9" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../index.html">
<span class="md-ellipsis">
    🤖人工智能
    
  </span>
</a>
<label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_9_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_9">
<span class="md-nav__icon md-icon"></span>
            🤖人工智能
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_9_2" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="index.html">
<span class="md-ellipsis">
    机器学习
    
  </span>
</a>
<label class="md-nav__link" for="__nav_9_2" id="__nav_9_2_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_9_2_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_9_2">
<span class="md-nav__icon md-icon"></span>
            机器学习
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="1.html">
<span class="md-ellipsis">
    Machine Learning - P1
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="2.html">
<span class="md-ellipsis">
    Machine Learning - P2
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="3.html">
<span class="md-ellipsis">
    Deep Learning
    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
<span class="md-ellipsis">
    Self-Attention
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="4.html">
<span class="md-ellipsis">
    Self-Attention
    
  </span>
</a>
<nav aria-label="目录" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
        目录
      </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#inputs-and-outputs">
<span class="md-ellipsis">
      Inputs and Outputs
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#self-attention_1">
<span class="md-ellipsis">
      Self-Attention
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#multi-head-self-attention">
<span class="md-ellipsis">
      Multi-head Self-Attention
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#positional-encoding">
<span class="md-ellipsis">
      Positional Encoding
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#applications">
<span class="md-ellipsis">
      Applications
    </span>
</a>
<nav aria-label="Applications" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#speech">
<span class="md-ellipsis">
      Speech
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#images">
<span class="md-ellipsis">
      Images
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#graphs">
<span class="md-ellipsis">
      Graphs
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#comparison">
<span class="md-ellipsis">
      Comparison
    </span>
</a>
<nav aria-label="Comparison" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#self-attention-vs-cnn">
<span class="md-ellipsis">
      Self-Attention v.s. CNN
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#self-attention-vs-rnn">
<span class="md-ellipsis">
      Self-Attention v.s. RNN
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#variants">
<span class="md-ellipsis">
      Variants
    </span>
</a>
<nav aria-label="Variants" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#skipping-some-calculations-with-human-knowledge">
<span class="md-ellipsis">
      Skipping Some Calculations with Human Knowledge
    </span>
</a>
<nav aria-label="Skipping Some Calculations with Human Knowledge" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#local-attentiontruncated-attention">
<span class="md-ellipsis">
      Local Attention/Truncated Attention
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#stride-attention">
<span class="md-ellipsis">
      Stride Attention
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#global-attention">
<span class="md-ellipsis">
      Global Attention
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#focusing-on-critical-parts">
<span class="md-ellipsis">
      Focusing on Critical Parts
    </span>
</a>
<nav aria-label="Focusing on Critical Parts" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#clustering">
<span class="md-ellipsis">
      Clustering
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#learnable-patterns-sinkhorn-sorting-network">
<span class="md-ellipsis">
      Learnable Patterns: Sinkhorn Sorting Network
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#reducing-number-of-keys">
<span class="md-ellipsis">
      Reducing Number of Keys
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#changing-the-order-of-matrix-multiplication">
<span class="md-ellipsis">
      Changing the Order of Matrix Multiplication
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#synthesizer">
<span class="md-ellipsis">
      Synthesizer
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#attention-free-techniques">
<span class="md-ellipsis">
      Attention-free Techniques
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="5.html">
<span class="md-ellipsis">
    Transformer
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../cv/index.html">
<span class="md-ellipsis">
    计算机视觉导论
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../misc/index.html">
<span class="md-ellipsis">
    🗃️杂项
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../tools/index.html">
<span class="md-ellipsis">
    🛠️工具
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../papers/index.html">
<span class="md-ellipsis">
    📑论文阅读
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="目录" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
        目录
      </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#inputs-and-outputs">
<span class="md-ellipsis">
      Inputs and Outputs
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#self-attention_1">
<span class="md-ellipsis">
      Self-Attention
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#multi-head-self-attention">
<span class="md-ellipsis">
      Multi-head Self-Attention
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#positional-encoding">
<span class="md-ellipsis">
      Positional Encoding
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#applications">
<span class="md-ellipsis">
      Applications
    </span>
</a>
<nav aria-label="Applications" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#speech">
<span class="md-ellipsis">
      Speech
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#images">
<span class="md-ellipsis">
      Images
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#graphs">
<span class="md-ellipsis">
      Graphs
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#comparison">
<span class="md-ellipsis">
      Comparison
    </span>
</a>
<nav aria-label="Comparison" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#self-attention-vs-cnn">
<span class="md-ellipsis">
      Self-Attention v.s. CNN
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#self-attention-vs-rnn">
<span class="md-ellipsis">
      Self-Attention v.s. RNN
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#variants">
<span class="md-ellipsis">
      Variants
    </span>
</a>
<nav aria-label="Variants" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#skipping-some-calculations-with-human-knowledge">
<span class="md-ellipsis">
      Skipping Some Calculations with Human Knowledge
    </span>
</a>
<nav aria-label="Skipping Some Calculations with Human Knowledge" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#local-attentiontruncated-attention">
<span class="md-ellipsis">
      Local Attention/Truncated Attention
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#stride-attention">
<span class="md-ellipsis">
      Stride Attention
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#global-attention">
<span class="md-ellipsis">
      Global Attention
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#focusing-on-critical-parts">
<span class="md-ellipsis">
      Focusing on Critical Parts
    </span>
</a>
<nav aria-label="Focusing on Critical Parts" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#clustering">
<span class="md-ellipsis">
      Clustering
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#learnable-patterns-sinkhorn-sorting-network">
<span class="md-ellipsis">
      Learnable Patterns: Sinkhorn Sorting Network
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#reducing-number-of-keys">
<span class="md-ellipsis">
      Reducing Number of Keys
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#changing-the-order-of-matrix-multiplication">
<span class="md-ellipsis">
      Changing the Order of Matrix Multiplication
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#synthesizer">
<span class="md-ellipsis">
      Synthesizer
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#attention-free-techniques">
<span class="md-ellipsis">
      Attention-free Techniques
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<a class="md-content__button md-icon" href="https://github.com/noughtq/notebook/edit/master/docs/ai/ml/4.md" title="编辑此页">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
</a>
<a class="md-content__button md-icon" href="https://github.com/noughtq/notebook/raw/master/docs/ai/ml/4.md" title="查看本页的源代码">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
</a>
<div><h1 id="self-attention">Self-Attention<a class="headerlink" href="#self-attention" title="Permanent link">⚓︎</a></h1>
<div style="margin-top: -30px; font-size: 0.9em; opacity: 0.7;">
<p><span class="twemoji"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10h-2a8 8 0 0 1-8 8 8 8 0 0 1-8-8 8 8 0 0 1 8-8zm6.78 1a.7.7 0 0 0-.48.2l-1.22 1.21 2.5 2.5L20.8 5.7c.26-.26.26-.7 0-.95L19.25 3.2c-.13-.13-.3-.2-.47-.2m-2.41 2.12L9 12.5V15h2.5l7.37-7.38z"></path></svg></span> 约<span class="heti-skip"><span class="heti-spacing"> </span>6118<span class="heti-spacing"> </span></span>个字 <span class="twemoji"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 20c4.42 0 8-3.58 8-8s-3.58-8-8-8-8 3.58-8 8 3.58 8 8 8m0-18c5.5 0 10 4.5 10 10s-4.5 10-10 10C6.47 22 2 17.5 2 12S6.5 2 12 2m.5 11H11V7h1.5v4.26l3.7-2.13.75 1.3z"></path></svg></span> 预计阅读时间<span class="heti-skip"><span class="heti-spacing"> </span>31<span class="heti-spacing"> </span></span>分钟</p>
</div>
<p>之前我们介绍的模型，都只是将一个向量作为输入（比如<span class="heti-skip"><span class="heti-spacing"> </span>COVID-19<span class="heti-spacing"> </span></span>感染人数预测、图像处理等问题<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，输出的是一个标量（回归问题）或类（分类问题<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。而现在我们将要讨论这样的一个模型：将<strong>一组</strong>向量（向量序列）作为输入，输出的是多个标量或类；而且输入向量的个数是可变的。</p>
<div style="text-align: center">
<img src="images/lec4/39.png" width="80%/"/>
</div>
<h2 id="inputs-and-outputs">Inputs and Outputs<a class="headerlink" href="#inputs-and-outputs" title="Permanent link">⚓︎</a></h2>
<p>首先需要考虑的一个问题是：如何将实际问题的<strong>输入</strong>转化为一组向量的形式呢？来看下面几个例子：</p>
<details class="example" open="open">
<summary>例子</summary>
<div class="tabbed-set tabbed-alternate" data-tabs="1:3"><input checked="" id="__tabbed_1_1" name="__tabbed_1" type="radio"/><input id="__tabbed_1_2" name="__tabbed_1" type="radio"/><input id="__tabbed_1_3" name="__tabbed_1" type="radio"/><div class="tabbed-labels"><label for="__tabbed_1_1">句子</label><label for="__tabbed_1_2">语音</label><label for="__tabbed_1_3">图</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p>一个句子就是一个向量序列，而句子中的每个单词对应一个个的向量。将单词转化为向量的方式有：</p>
<ul>
<li>
<p><strong>独热编码</strong>(one-hot encoding)：假如所有单词的个数为<span><span class="heti-spacing"> </span>n</span>，那么我们就用一个大小为<span class="heti-skip"><span class="heti-spacing"> </span>n<span class="heti-spacing"> </span></span>的向量来表示每个单词，每个位置上的元素表示唯一的单词</p>
<ul>
<li>
<p>这种方式简单粗暴，但是不仅占用内存空间大，而且无法体现出两个单词之间的联系，即没有考虑到语义信息</p>
<p></p><div style="text-align: center">
<img src="images/lec4/40.png" width="50%/"/>
</div>
</li>
</ul>
</li>
<li>
<p><strong>词嵌入</strong>(word embedding)：语义关系越接近的两个单词，它们对应的向量值会更加接近，如下图所示：</p>
<p></p><div style="text-align: center">
<img src="images/lec4/41.png" width="50%/"/>
</div>
</li>
</ul>
</div>
<div class="tabbed-block">
<p>在机器学习课程<span class="heti-skip"><span class="heti-spacing"> </span>HW2<span class="heti-spacing"> </span></span>中，我们将语音看作一段段的帧（<span>25ms<span class="heti-spacing"> </span></span>的语音<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>；并且用一个滑动窗口来划分帧，窗口每次移动的步幅为<span><span class="heti-spacing"> </span>10ms</span>，所以<span class="heti-skip"><span class="heti-spacing"> </span>1s<span class="heti-spacing"> </span></span>的语音里就有<span class="heti-skip"><span class="heti-spacing"> </span>100<span class="heti-spacing"> </span></span>个帧。因此，每个帧就是一个向量，而一段语音就是变长的向量序列。</p>
<p></p><div style="text-align: center">
<img src="images/lec4/42.png" width="70%/"/>
</div>
</div>
<div class="tabbed-block">
<p>我们可以将图上的每个节点看作一个向量，所以整张图就是一组向量。在实际应用中：</p>
<ul>
<li>
<p>社交网络：</p>
<p></p><div style="text-align: center">
<img src="images/lec4/43.png" width="70%/"/>
</div>
</li>
<li>
<p>分子结构</p>
<p></p><div style="text-align: center">
<img src="images/lec4/44.png" width="70%/"/>
</div>
</li>
</ul>
</div>
</div>
</div>
</details>
<p>而输出的内容则可以分为以下几种情况：</p>
<ul>
<li>
<p>每个输入向量对应一个输出标签<span><span class="heti-spacing"> </span>(label)</span>（即预测值）</p>
<p></p><div style="text-align: center">
<img src="images/lec4/45.png" width="70%/"/>
</div>
<details class="example">
<summary>例子</summary>
<p></p><div style="text-align: center">
<img src="images/lec4/46.png" width="70%/"/>
</div>
</details>
</li>
<li>
<p>对于整组向量，输出一个标签</p>
<p></p><div style="text-align: center">
<img src="images/lec4/47.png" width="60%/"/>
</div>
<details class="example">
<summary>例子</summary>
<p></p><div style="text-align: center">
<img src="images/lec4/48.png" width="70%/"/>
</div>
</details>
</li>
<li>
<p>由模型决定输出标签的数量（seq2seq，序列到序列）</p>
<p></p><div style="text-align: center">
<img src="images/lec4/49.png" width="70%/"/>
</div>
</li>
</ul>
<p>不过之后我们只专注第一种输出情况，即对于<span class="heti-skip"><span class="heti-spacing"> </span>N<span class="heti-spacing"> </span></span>个向量的输入，输出相应的<span class="heti-skip"><span class="heti-spacing"> </span>N<span class="heti-spacing"> </span></span>个标量或类。</p>
<h2 id="self-attention_1">Self-Attention<a class="headerlink" href="#self-attention_1" title="Permanent link">⚓︎</a></h2>
<p>接下来考虑如何构建适用于向量序列输入的模型。</p>
<details class="bug">
<summary>Naive Idea</summary>
<p>既然<span class="heti-skip"><span class="heti-spacing"> </span>N<span class="heti-spacing"> </span></span>个向量输入对应<span class="heti-skip"><span class="heti-spacing"> </span>N<span class="heti-spacing"> </span></span>个输出，那么不妨让每个向量单独进入一个全连接层得到一个输出，如下图所示：</p>
<p></p><div style="text-align: center">
<img src="images/lec4/50.png" width="70%/"/>
</div>
<p>显然，这样有一个很大的缺陷：我们没有考虑到向量序列各个向量之间的联系，各个向量都是“各自为政”的。所以，如果有两个向量的值是相等的，那就会被视为意义相同的两个东西，从而产生两个相同的输出。</p>
</details>
<p>每个向量输入都有一个对应的<strong>全连接层</strong>，这个全连接层不仅应该接收对应的向量输入，同时也要顾及整个<strong>上下文</strong><span>(context)<span class="heti-spacing"> </span></span>中的向量。</p>
<ul>
<li>
<p>这个上下文的范围可以是一个局部的窗口，这样就仅考虑与对应输入向量相邻的部分向量</p>
<p></p><div style="text-align: center">
<img src="images/lec4/51.png" width="70%/"/>
</div>
</li>
<li>
<p>也可以将整个向量序列都纳入考虑范围内，这样考虑得更全面些，比如用一个固定大小的窗口覆盖整个向量序列</p>
<ul>
<li>但是向量序列的长度可长可短，所以不能简单地使用窗口来实现这一点</li>
<li>再说，就算有一个足够大小的窗口可以容纳所有向量，但是这样也意味着全连接层需要非常多的参数（<span><span class="arithmatex">\(n\)</span><span class="heti-spacing"> </span></span>个向量就会产生<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(n^2\)</span><span class="heti-spacing"> </span></span>个参数<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，而过多的参数意味着很容易出现过拟合的问题</li>
</ul>
</li>
</ul>
<p>因此，这里引入一种更好的做法——使用一种名为“<strong>自注意</strong>”<span>(self-attention)<span class="heti-spacing"> </span></span>的机制：</p>
<ul>
<li>将输入向量送入全连接层之前，先让这些向量经过一种“自注意”的运算。对于每个输入向量，经过“自注意”运算后都会得到一个对应的输出，我们可以把这个输出看作是包含整个向量序列上下文信息的新向量，但同时也保留了原来输入向量的特征。</li>
<li>然后将这个新向量传给全连接层进行训练</li>
</ul>
<p>这一过程如下图所示：</p>
<div style="text-align: center">
<img src="images/lec4/52.png" width="60%/"/>
</div>
<p>当然，在多层神经网络中，可以在每两个隐藏层之间塞一个自注意计算：</p>
<div style="text-align: center">
<img src="images/lec4/53.png" width="60%/"/>
</div>
<p>在自注意计算这个“盒子”的内部，输入和输出的关系如下所示：</p>
<div style="text-align: center">
<img src="images/lec4/54.png" width="60%/"/>
</div>
<p>其中输入既可以是最外面的向量序列，也可以是经过几层训练后得到的输出向量序列。现在我们就考虑某一个输出向量，比如<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{b^1}\)</span></span>，来认识一下自注意的计算过程。</p>
<p>首先，我们要计算<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{b^1}\)</span><span class="heti-spacing"> </span></span>对应的输入向量<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{a^1}\)</span><span class="heti-spacing"> </span></span>与其他输入向量的<strong>相关程度</strong>（用<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\alpha\)</span><span class="heti-spacing"> </span></span>表示<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。有以下两种常用的方式：</p>
<div style="text-align: center">
<img src="images/lec4/55.png" width="60%/"/>
</div>
<div class="admonition info">
<p class="admonition-title">注</p>
<p>下面我们会将主要考虑的输入向量称为<strong>查询</strong>(query)，而把另外的向量看作<strong>键</strong>(key)，所以它们分别对应的矩阵为<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(W^q\)</span><span class="heti-spacing"> </span></span>和<span><span class="heti-spacing"> </span><span class="arithmatex">\(W^k\)</span></span>，与矩阵相乘的结果分别为<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{q}\)</span><span class="heti-spacing"> </span></span>和<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{k}\)</span></span>。</p>
</div>
<ul>
<li><strong>点积</strong>(dot-product)：相关程度<span><span class="heti-spacing"> </span><span class="arithmatex">\(\alpha = \bm{q} \cdot \bm{k}\)</span></span>（向量的点积，得到一个标量）</li>
<li>加法<span><span class="heti-spacing"> </span>(additive)</span>：将<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{q} + \bm{k}\)</span><span class="heti-spacing"> </span></span>的结果丢给<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\tanh\)</span><span class="heti-spacing"> </span></span>函数计算，然后再经过一次转换（用矩阵<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(W\)</span><span class="heti-spacing"> </span></span>表示）得到<span><span class="heti-spacing"> </span><span class="arithmatex">\(\alpha\)</span></span></li>
</ul>
<p>下面我们仅考虑<strong>点积</strong>这一方法。</p>
<hr/>
<p>回到前面有<span class="heti-skip"><span class="heti-spacing"> </span>4<span class="heti-spacing"> </span></span>个输入的例子，先将<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{a^1}\)</span><span class="heti-spacing"> </span></span>作为查询，其他几个输入向量（也可以包括<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{a^1}\)</span></span>）作为键，计算相关程度<span><span class="heti-spacing"> </span><span class="arithmatex">\(\alpha_{1, j}\ (j = 1, \dots, 4)\)</span></span>：</p>
<div style="text-align: center">
<img src="images/lec4/58.png" width="80%/"/>
</div>
<p>实际上，相关程度的值应该在<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\([0, 1]\)</span><span class="heti-spacing"> </span></span>这一范围内，所以让这些相关程度值再经过一次<span class="heti-skip"><span class="heti-spacing"> </span>softmax<span class="heti-spacing"> </span></span>函数的运算，将它们的值映射到<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\([0, 1]\)</span><span class="heti-spacing"> </span></span>上（用其他激活函数也没有问题<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。</p>
<div style="text-align: center">
<img src="images/lec4/59.png" width="80%/"/>
</div>
<p>另外，对于每个输入向量，我们还要为它们计算第三个向量<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{v} = W^v \bm{a}\)</span></span>，然后将这个向量与刚刚经过<span class="heti-skip"><span class="heti-spacing"> </span>softmax<span class="heti-spacing"> </span></span>得到的相关程度值相乘，再将这些乘积相加（加权和<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，最终得到<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{b}\)</span></span>。对于本例中的<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{b^1}\)</span><span class="heti-spacing"> </span></span>而言，计算公式为：</p>
<div class="arithmatex">\[
\bm{b^1} = \sum\limits_{i} \alpha'_{1, i} \bm{v^i}
\]</div>
<div style="text-align: center">
<img src="images/lec4/60.png" width="80%/"/>
</div>
<p>剩下的<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{b^2}, \bm{b^3}, \dots\)</span><span class="heti-spacing"> </span></span>等输出向量可以一起计算计算（<strong>并行</strong>计算<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，由于计算过程类似，故不再赘述。</p>
<hr/>
<p>现在我们从<strong>矩阵</strong>的角度研究一般情况下的计算过程：</p>
<ul>
<li>
<p>对于每个输入向量<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{a^i}\)</span></span>，需要得到三个向量<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{q^i}, \bm{k^i}, \bm{v^i}\)</span></span>，而这些向量分别通过三个矩阵<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(W^q, W^k, W^v\)</span><span class="heti-spacing"> </span></span>和<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{a^i}\)</span><span class="heti-spacing"> </span></span>相乘得到。与其让矩阵分别和单个的输入向量相乘，不如将这些向量拼在一起，形成一个矩阵，这样就将多次的矩阵<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\times\)</span><span class="heti-spacing"> </span></span>向量的运算转化为一次的矩阵<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\times\)</span><span class="heti-spacing"> </span></span>矩阵的运算，如下所示：</p>
<p></p><div style="text-align: center">
<img src="images/lec4/61.png" width="60%/"/>
</div>
</li>
<li>
<p>我们知道，计算相关程度<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\alpha\)</span><span class="heti-spacing"> </span></span>的过程是一个向量乘法，但是我们也可以将其转化为一个矩阵乘法，一次性算出所有的相关程度值</p>
<ul>
<li>
<p>先将所有的<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{k}\)</span><span class="heti-spacing"> </span></span>拼在一起，一次性计算某个查询下的所有相关程度：</p>
<p></p><div style="text-align: center">
<img src="images/lec4/62.png" width="80%/"/>
</div>
</li>
<li>
<p>然后将所有的<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{q}\)</span><span class="heti-spacing"> </span></span>拼在一起，这样就可以将所有的相关程度一次性算出来了！</p>
<p></p><div style="text-align: center">
<img src="images/lec4/63.png" width="90%/"/>
</div>
</li>
</ul>
</li>
<li>
<p>同理，将所有的向量<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{v}\)</span><span class="heti-spacing"> </span></span>拼在一起，与相关程度构成的矩阵<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(A'\)</span><span class="heti-spacing"> </span></span>相乘，得到所有的输出向量：</p>
<p></p><div style="text-align: center">
<img src="images/lec4/64.png" width="70%/"/>
</div>
</li>
</ul>
<p>综上，整个自注意的计算过程可以抽象为以下一系列的矩阵运算：</p>
<div style="text-align: center">
<img src="images/lec4/65.png" width="70%/"/>
</div>
<p>其中三个矩阵<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(W^q, W^k, W^v\)</span><span class="heti-spacing"> </span></span>是我们需要通过训练学习的参数。而由相关程度构成的（且经过<span class="heti-skip"><span class="heti-spacing"> </span>softmax<span class="heti-spacing"> </span></span>归一化处理后的）矩阵称作<strong>注意矩阵</strong>(attention matrix)。</p>
<h2 id="multi-head-self-attention">Multi-head Self-Attention<a class="headerlink" href="#multi-head-self-attention" title="Permanent link">⚓︎</a></h2>
<p>对于那些需要学习的参数<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{q}, \bm{k}, \bm{v}\)</span></span>，我们可以将同一个参数拆成多份（即“<strong>多个头</strong>”(multi-head)，通过原参数<span class="heti-skip"><span class="heti-spacing"> </span>x<span class="heti-spacing"> </span></span>不同的矩阵得到<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，每个“头”考虑不同的相关性类型，从而能够捕获更复杂的关系，提高模型对复杂模式的理解。下图就是头数<span class="heti-skip"><span class="heti-spacing"> </span>= 2<span class="heti-spacing"> </span></span>的例子：</p>
<div style="text-align: center">
<img src="images/lec4/66.png" width="70%/"/>
</div>
<p>下面以第<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(i\)</span><span class="heti-spacing"> </span></span>个输入<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{a^i}\)</span><span class="heti-spacing"> </span></span>为例介绍输出<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{b^i}\)</span><span class="heti-spacing"> </span></span>的计算过程。先算出第一个头对应的输出<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{b^{i, 1}}\)</span></span>：</p>
<div style="text-align: center">
<img src="images/lec4/67.png" width="70%/"/>
</div>
<p>然后算出第二个头对应的输出<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{b^{i, 2}}\)</span></span>：</p>
<div style="text-align: center">
<img src="images/lec4/68.png" width="70%/"/>
</div>
<p>最后将这<span class="heti-skip"><span class="heti-spacing"> </span>2<span class="heti-spacing"> </span></span>个向量拼起来，乘上某个矩阵<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(W^O\)</span><span class="heti-spacing"> </span></span>后，就能得到完整的输出<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{b^i}\)</span><span class="heti-spacing"> </span></span>了：</p>
<div style="text-align: center">
<img src="images/lec4/69.png" width="40%/"/>
</div>
<h2 id="positional-encoding">Positional Encoding<a class="headerlink" href="#positional-encoding" title="Permanent link">⚓︎</a></h2>
<p>自注意机制的一个缺陷是：没有考虑位置信息，比如输入序列中第<span class="heti-skip"><span class="heti-spacing"> </span>i<span class="heti-spacing"> </span></span>个和第<span class="heti-skip"><span class="heti-spacing"> </span>j<span class="heti-spacing"> </span></span>个向量无论它们之间距离多远，都不会影响到它们在自注意机制中的计算结果。但有时候我们希望将这些位置信息考虑在内，比如在做词性标记的时候，我们知道动词一般不会出现在句首，这就是个值得考虑的位置信息。所以这里引入了一种叫做<strong>位置编码</strong><span>(positional encoding)<span class="heti-spacing"> </span></span>的技术——它会为每个输入向量设置一个唯一的<strong>位置向量</strong> <span class="arithmatex">\(\bm{e^i}\)</span>，在进入自注意计算前，将其和输入向量<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{a^i}\)</span><span class="heti-spacing"> </span></span>相加，这样自注意计算时就将位置信息考虑进去了。</p>
<p>下图展示了一种可行的位置向量序列（这也正是经典论文 <em>Attention Is All Your Need</em> 中最早用到的位置向量<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，其中<span class="heti-skip"><span class="heti-spacing"> </span>1<span class="heti-spacing"> </span></span>列表示<span class="heti-skip"><span class="heti-spacing"> </span>1<span class="heti-spacing"> </span></span>个位置向量：</p>
<div style="text-align: center">
<img src="images/lec4/70.png" width="40%/"/>
</div>
<p>上面的位置向量是<strong>人为设定的</strong>(hand-crafted)——其实还可以通过数据训练出位置向量，下面就是一些典型的例子：</p>
<div style="text-align: center">
<img src="images/lec4/71.png" width="80%/"/>
</div>
<p>注意，这里的图需要横着看，也就是说一行表示一个位置向量。</p>
<h2 id="applications">Applications<a class="headerlink" href="#applications" title="Permanent link">⚓︎</a></h2>
<p>自注意机制被广泛应用在<span class="heti-skip"><span class="heti-spacing"> </span>NLP<span class="heti-spacing"> </span></span>中，而且我们熟知的<span class="heti-skip"><span class="heti-spacing"> </span>Transformer<span class="heti-spacing"> </span></span>和<span class="heti-skip"><span class="heti-spacing"> </span>BERT<span class="heti-spacing"> </span></span>中也用到了自注意。下面来认识一下常见的应用！</p>
<h3 id="speech">Speech<a class="headerlink" href="#speech" title="Permanent link">⚓︎</a></h3>
<p>在语音识别中也可以用自注意机制，但是存在一个问题：正如前面提到的，我们会把一段<span class="heti-skip"><span class="heti-spacing"> </span>10ms<span class="heti-spacing"> </span></span>的语音当做一个向量，那么随便将一两句话，这个输入序列的长度<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(L\)</span><span class="heti-spacing"> </span></span>就大到吓人；而且注意矩阵<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(A'\)</span><span class="heti-spacing"> </span></span>的规模是<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(L \times L\)</span><span class="heti-spacing"> </span></span>的，甚至大到无法被内存容纳。</p>
<p>所以我们采取一种改进手段，叫做<strong>截断自注意</strong>(truncated self-attention)。简单来说就是：对于某个输入向量，我们仅考虑那些和当前输入向量比较接近的向量作为键，不去考虑更远的向量。在语音识别中，这种做法是合理的，因为要识别一句话中的一个字，我们往往无需得知整个段落的内容，也许仅靠这一句话甚至半句话就能判断出来了。</p>
<p>下面就是一个简化的示意图：</p>
<div style="text-align: center">
<img src="images/lec4/72.png" width="40%/"/>
</div>
<h3 id="images">Images<a class="headerlink" href="#images" title="Permanent link">⚓︎</a></h3>
<p>图像除了能用<span class="heti-skip"><span class="heti-spacing"> </span>CNN<span class="heti-spacing"> </span></span>来训练外，也可以用自注意来训练。我们可以将一张图像看作是一个向量集，而每个像素点就是一个向量，每个通道就是向量的元素。</p>
<div style="text-align: center">
<img src="images/lec4/73.png" width="70%/"/>
</div>
<p>下面就是一些具体的应用：</p>
<div style="text-align: center">
<img src="images/lec4/74.png" width="80%/"/>
</div>
<h3 id="graphs">Graphs<a class="headerlink" href="#graphs" title="Permanent link">⚓︎</a></h3>
<p>自注意<span class="heti-skip"><span class="heti-spacing"> </span>+<span class="heti-spacing"> </span></span>图<span><span class="heti-spacing"> </span>=</span> <strong>图神经网络</strong>(graph neural network)</p>
<div style="text-align: center">
<img src="images/lec4/83.png" width="80%/"/>
</div>
<p>这里利用了图的信息，尤其是边的信息。我们仅考虑有边相连的两个节点对应的注意矩阵上的元素（蓝色方框<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，其他元素均为<span><span class="heti-spacing"> </span>0</span>（白色方框<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，这样我们就不会训练哪些没有意义的参数了。</p>
<h2 id="comparison">Comparison<a class="headerlink" href="#comparison" title="Permanent link">⚓︎</a></h2>
<h3 id="self-attention-vs-cnn">Self-Attention v.s. CNN<a class="headerlink" href="#self-attention-vs-cnn" title="Permanent link">⚓︎</a></h3>
<div style="text-align: center">
<img src="images/lec4/75.png" width="40%/"/>
</div>
<p>上图中，黄色方框对应的是<span class="heti-skip"><span class="heti-spacing"> </span>CNN<span class="heti-spacing"> </span></span>的感受野，而红色方框对应的是自注意的查询和键。我们不难发现：</p>
<ul>
<li><strong>CNN</strong> 是一种只考虑感受野范围内的自注意机制，即<span class="heti-skip"><span class="heti-spacing"> </span>CNN<span class="heti-spacing"> </span></span>是一种简化的自注意机制。</li>
<li><strong>自注意机制</strong>是一种感受野范围更大（按<span class="heti-skip"><span class="heti-spacing"> </span>PPT<span class="heti-spacing"> </span></span>说法是可学习的<span><span class="heti-spacing"> </span>(leanable)</span>）的<span><span class="heti-spacing"> </span>CNN</span>，即自注意机制是一种复杂版本的<span><span class="heti-spacing"> </span>CNN</span>。</li>
</ul>
<p>用维恩图可以形象表示两者的关系：</p>
<div style="text-align: center">
<img src="images/lec4/76.png" width="30%/"/>
</div>
<p>由于自注意机制会考虑更多的输入，因而更加灵活，所以在数据不多的情况下更容易出现过拟合现象，效果就不如<span class="heti-skip"><span class="heti-spacing"> </span>CNN<span class="heti-spacing"> </span></span>了；而一旦数据量大到一定程度后，自注意机制的优势就显现出来了。</p>
<div style="text-align: center">
<img src="images/lec4/78.png" width="70%/"/>
</div>
<p>下面这篇论文详细介绍了两者的关系，感兴趣的同学可自行上网搜索：</p>
<div style="text-align: center">
<img src="images/lec4/77.png" width="80%/"/>
</div>
<h3 id="self-attention-vs-rnn">Self-Attention v.s. RNN<a class="headerlink" href="#self-attention-vs-rnn" title="Permanent link">⚓︎</a></h3>
<p>目前<strong>循环神经网络</strong><span>(recurrent neutral network, RNN)<span class="heti-spacing"> </span></span>基本上能被自注意机制给替代了，所以这里就简单介绍一下<span class="heti-skip"><span class="heti-spacing"> </span>RNN<span class="heti-spacing"> </span></span>的机制：</p>
<div style="text-align: center">
<img src="images/lec4/79.png" width="80%/"/>
</div>
<ul>
<li>底下<span class="heti-skip"><span class="heti-spacing"> </span>4<span class="heti-spacing"> </span></span>个小矩形就是输入，最开始会有一个初始内存</li>
<li>将这块内存和第一个输入向量一同放到<span class="heti-skip"><span class="heti-spacing"> </span>RNN<span class="heti-spacing"> </span></span>中，得到一个输出（中间用黑框包裹的矩形<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，该输出可进入<span><span class="heti-spacing"> </span>FC</span>（全连接层）的训练中。</li>
<li>这个输出将和第二个输入向量一同放到<span class="heti-skip"><span class="heti-spacing"> </span>RNN<span class="heti-spacing"> </span></span>中，得到下一个输出，该输出也会进入<span><span class="heti-spacing"> </span>FC</span>（全连接层）的训练中。之后就以此类推下去，故不再赘述。</li>
</ul>
<p>对应的自注意机制示意图为：</p>
<div style="text-align: center">
<img src="images/lec4/80.png" width="60%/"/>
</div>
<div class="admonition bug">
<p class="admonition-title"><span>RNN<span class="heti-spacing"> </span></span>的劣势</p>
<ul>
<li>
<p>难以考虑太远的输入向量：虽然我们可以将<span class="heti-skip"><span class="heti-spacing"> </span>RNN<span class="heti-spacing"> </span></span>改为双向的（这样的话输出向量都能包含任意输入向量，而不是像图示那样只能考虑左侧的输入向量<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，但相比自注意机制能一次看透所有输入的能力，读取较远位置的输入向量还是有些差劲的。</p>
<p></p><div style="text-align: center">
<img src="images/lec4/81.png" width="70%/"/>
</div>
</li>
<li>
<p>无法并行：其实解释和前面的差不多，就是<span class="heti-skip"><span class="heti-spacing"> </span>RNN<span class="heti-spacing"> </span></span>需要一个个看输入，不能并行加速；而自注意能通过并行一次看完所有输入，所以效率之间有着很大差别。</p>
<p></p><div style="text-align: center">
<img src="images/lec4/82.png" width="70%/"/>
</div>
</li>
</ul>
</div>
<p>正因为这些劣势，那些采用<span class="heti-skip"><span class="heti-spacing"> </span>RNN<span class="heti-spacing"> </span></span>的应用开始改用自注意机制了。</p>
<h2 id="variants">Variants<a class="headerlink" href="#variants" title="Permanent link">⚓︎</a></h2>
<p>前面介绍的都是一般的自注意机制技术，实际上后人们在此基础上设计出各种“魔改版本”，想要在确保不影响表现的同时提升效率。下面我们就来认识这些各式各样的自注意机制变体吧！</p>
<h3 id="skipping-some-calculations-with-human-knowledge">Skipping Some Calculations with Human Knowledge<a class="headerlink" href="#skipping-some-calculations-with-human-knowledge" title="Permanent link">⚓︎</a></h3>
<p>先来回顾一般的自注意机制：假如序列长度为<span><span class="heti-spacing"> </span><span class="arithmatex">\(N\)</span></span>，那么中间产生的查询和键的数量也是<span><span class="heti-spacing"> </span><span class="arithmatex">\(N\)</span></span>，而查询序列和键序列相乘得到的注意矩阵的规模就是<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(N \times N\)</span><span class="heti-spacing"> </span></span>的了。</p>
<div style="text-align: center">
<img src="images/lec4/1.png" width="50%/"/>
</div>
<p>另外，我们还有以下发现：</p>
<ul>
<li>自注意机制只是整个网络中的一个模块</li>
<li>当<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(N\)</span><span class="heti-spacing"> </span></span>很大时，自注意机制决定了整个的计算量</li>
<li>上一点通常体现在图像处理等任务中<ul>
<li>比如有一张<span class="heti-skip"><span class="heti-spacing"> </span>256x256<span class="heti-spacing"> </span></span>的图片，那么<span><span class="heti-spacing"> </span><span class="arithmatex">\(N = 256 \times 256\)</span></span>，注意矩阵就有<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(256^4\)</span><span class="heti-spacing"> </span></span>个元素，足以体现计算量之大</li>
</ul>
</li>
</ul>
<p>第一种提升计算效率（或减小计算量）的思路是利用我们人类已有的知识来避免不必要的计算。下面给出一些具体的方法。</p>
<h4 id="local-attentiontruncated-attention">Local Attention/Truncated Attention<a class="headerlink" href="#local-attentiontruncated-attention" title="Permanent link">⚓︎</a></h4>
<div style="text-align: center">
<img src="images/lec4/2.png" width="70%/"/>
</div>
<ul>
<li>注意矩阵中灰色方块对应的元素直接置<span><span class="heti-spacing"> </span>0</span>，不去计算；蓝色方块对应的元素需要计算注意权重</li>
<li>该方法的思路是：输入向量只看相邻的<span class="heti-skip"><span class="heti-spacing"> </span>2<span class="heti-spacing"> </span></span>个向量，其他向量就不去管它</li>
<li>思路上和<a href="3.html#cnn"><span class="heti-skip"><span class="heti-spacing"> </span>CNN<span class="heti-spacing"> </span></span></a>类似</li>
<li>该方法能够加快计算速度，但不一定能给出非常好的结果</li>
</ul>
<h4 id="stride-attention">Stride Attention<a class="headerlink" href="#stride-attention" title="Permanent link">⚓︎</a></h4>
<div style="text-align: center">
<img src="images/lec4/3.png" width="70%/"/>
</div>
<ul>
<li>类似前一种方法，但是看的是跳过几个向量后的向量</li>
<li>跳过的步幅可以根据任务需求自行调整</li>
</ul>
<h4 id="global-attention">Global Attention<a class="headerlink" href="#global-attention" title="Permanent link">⚓︎</a></h4>
<p><strong>全局注意</strong><span>(global attention)<span class="heti-spacing"> </span></span>的思路是：</p>
<ul>
<li>
<p>在序列中设置一类带有特殊记号的向量</p>
<ul>
<li>
<p>可以给原向量做特殊标记（上图<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，也可以插入带有这种特殊记号的新向量（下图）</p>
<p></p><div style="text-align: center">
<img src="images/lec4/4.png" width="40%/"/>
</div>
</li>
</ul>
</li>
<li>
<p>这些特殊向量会考虑序列中所有的向量，因而掌握了全局信息</p>
</li>
<li>其他向量也会考虑所有的特殊向量，从而得知全局信息</li>
<li>一般向量就不会考虑其他一般向量</li>
</ul>
<details class="example" open="open">
<summary>例子</summary>
<p></p><div style="text-align: center">
<img src="images/lec4/5.png" width="40%/"/>
</div>
<p>不难看出，序列中的前两个向量带有特殊记号。</p>
</details>
<hr/>
<p>上述这些方法并不互相排斥，实际上可以一起使用（<del>小孩子才做选择，我全都要！</del><heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，比如：</p>
<div style="text-align: center">
<img src="images/lec4/6.png" width="70%/"/>
</div>
<h3 id="focusing-on-critical-parts">Focusing on Critical Parts<a class="headerlink" href="#focusing-on-critical-parts" title="Permanent link">⚓︎</a></h3>
<p>上面通过人类知识简化计算的思路不一定给出最好的结果，所以我们来看另一种思路：对于一个已有的注意矩阵，每个元素的值有大有小。我们决定：</p>
<ul>
<li>对于较小的值，直接设为<span><span class="heti-spacing"> </span>0</span>，因为它们的影响不大，可以忽略</li>
<li>所以计算时只关心较大的值</li>
</ul>
<div style="text-align: center">
<img src="images/lec4/7.png" width="60%/"/>
</div>
<p>那么，接下来的问题就是：如何快速评估较小注意权重的占比呢？下面介绍一些相应的技术。</p>
<h4 id="clustering">Clustering<a class="headerlink" href="#clustering" title="Permanent link">⚓︎</a></h4>
<details class="info" open="open">
<summary>注</summary>
<p><a href="https://openreview.net/forum?id=rkgNKkHtvB"><span>Reformer<span class="heti-spacing"> </span></span></a>和<a href="https://arxiv.org/abs/2003.05997"><span class="heti-skip"><span class="heti-spacing"> </span>Routing Transformer<span class="heti-spacing"> </span></span></a>用到了这类技术。</p>
</details>
<p><strong>聚集</strong><span>(clustering)<span class="heti-spacing"> </span></span>技术的步骤如下：</p>
<ol>
<li>
<p>基于查询和键的相似性分成多个集群<span><span class="heti-spacing"> </span>(cluster)</span></p>
<ul>
<li>该步一般采用近似方法来做，计算量不会很大</li>
<li>下面的例子就是将查询和键划分成<span class="heti-skip"><span class="heti-spacing"> </span>4<span class="heti-spacing"> </span></span>个集群</li>
</ul>
<p></p><div style="text-align: center">
<img src="images/lec4/8.png" width="70%/"/>
</div>
</li>
<li>
<p>在注意矩阵中，我们只计算那些位于同一集群中的查询和键的乘积，不同集群的就不去计算，直接置<span class="heti-skip"><span class="heti-spacing"> </span>0<span class="heti-spacing"> </span></span>就好了</p>
<p></p><div style="text-align: center">
<img src="images/lec4/9.png" width="70%/"/>
</div>
</li>
</ol>
<h3 id="learnable-patterns-sinkhorn-sorting-network">Learnable Patterns: Sinkhorn Sorting Network<a class="headerlink" href="#learnable-patterns-sinkhorn-sorting-network" title="Permanent link">⚓︎</a></h3>
<p>到目前为止，所有的技术都是基于人类知识来实现的，但实际上还可以通过机器学习，让模型自己决定哪些注意权重需要计算，哪些可以忽略。有一种叫做 <strong><span>Sinkhorn<span class="heti-spacing"> </span></span>排序网络</strong><span>(Sinkhorn sorting network)<span class="heti-spacing"> </span></span>的技术便能做到这一点。</p>
<div style="text-align: center">
<img src="images/lec4/10.png" width="70%/"/>
</div>
<p>大致思路如下：</p>
<ul>
<li>输入序列的每个向量会经过一个神经网络，得到一组规模和注意矩阵一致的向量，这组向量会转化成要求的注意矩阵</li>
<li>然而注意矩阵的注意权重只有<span class="heti-skip"><span class="heti-spacing"> </span>0<span class="heti-spacing"> </span></span>和<span><span class="heti-spacing"> </span>1</span>（离散的<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，而刚刚得到的那组向量是连续的，因此需要做进一步的处理</li>
<li>这个处理过程正是原论文探讨的重点，这里不会细讲，感兴趣的读者可以网上搜索阅读</li>
<li>由于通过神经网络得到的向量组是可微分的，因此可以通过训练提高精度</li>
<li>实际上在原论文中，多个输入向量会共用一个经过神经网络得到的向量，从而减小计算量<ul>
<li>但这样得到的向量组规模就比注意矩阵小了，因此在转化时还要“放大”这个向量组，使得规模匹配的上</li>
</ul>
</li>
</ul>
<h3 id="reducing-number-of-keys">Reducing Number of Keys<a class="headerlink" href="#reducing-number-of-keys" title="Permanent link">⚓︎</a></h3>
<p>现在来思考一下：我们是否真的需要一个完整的注意矩阵吗？<a href="https://arxiv.org/abs/2006.04768"><span>Linformer<span class="heti-spacing"> </span></span></a>这篇论文探讨了这个问题，发现注意矩阵的<strong>秩</strong><span>(rank)<span class="heti-spacing"> </span></span>并不大，也就是说不少列是相互依赖的，是冗余的，那么我们可以尝试将这些多余的列删掉，从而减小计算量。</p>
<div style="text-align: center">
<img src="images/lec4/11.png" width="60%/"/>
</div>
<p>注意矩阵的列对应的是<strong>键</strong>，所以我们从原来<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(N\)</span><span class="heti-spacing"> </span></span>个键中挑出一些具有代表性的键出来，共计<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(K\)</span><span class="heti-spacing"> </span></span>个。相应地，<strong>值</strong>的数量也要从<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(N\)</span><span class="heti-spacing"> </span></span>降至<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(K\)</span><span class="heti-spacing"> </span></span>个。</p>
<div style="text-align: center">
<img src="images/lec4/12.png" width="70%/"/>
</div>
<details class="question" open="open">
<summary>思考</summary>
<div class="tabbed-set tabbed-alternate" data-tabs="2:2"><input checked="" id="__tabbed_2_1" name="__tabbed_2" type="radio"/><input id="__tabbed_2_2" name="__tabbed_2" type="radio"/><div class="tabbed-labels"><label for="__tabbed_2_1">问题</label><label for="__tabbed_2_2">解答</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p>能否减小查询的数量？</p>
</div>
<div class="tabbed-block">
<p>视情况而定。首先要注意的一点是：查询的数量决定了输出的数量。</p>
<ul>
<li>有些任务（比如分类）可能只需要一个输出，这时可以尝试减小查询数量</li>
<li>但有的任务要求每个输入都要有对应的输出，那么不应该改变输出长度，此时就不得减小查询数量</li>
</ul>
</div>
</div>
</div>
</details>
<p>那么具体该如何减小键的数量呢？下面给出一些方法：</p>
<ul>
<li><strong>压缩注意</strong>(compressed attention)：<ul>
<li>采用 <strong>CNN</strong>，对整个序列扫描一遍，从而找出代表性的限量，缩短了序列长度</li>
</ul>
</li>
<li><strong>Linformer</strong>：<ul>
<li>将<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(N \times N\)</span><span class="heti-spacing"> </span></span>的注意矩阵和一个<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(N \times K\)</span><span class="heti-spacing"> </span></span>的矩阵相乘，从而得到让键的数量降至<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(K\)</span><span class="heti-spacing"> </span></span>个</li>
<li>实际上实现了<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(N\)</span><span class="heti-spacing"> </span></span>个向量的<strong>线性组合</strong>(linear combination)</li>
</ul>
</li>
</ul>
<div style="text-align: center">
<img src="images/lec4/13.png" width="70%/"/>
</div>
<h3 id="changing-the-order-of-matrix-multiplication">Changing the Order of Matrix Multiplication<a class="headerlink" href="#changing-the-order-of-matrix-multiplication" title="Permanent link">⚓︎</a></h3>
<p>回顾一下：从数学层面上看，自注意机制本质上就是在做下面这样的矩阵乘法。</p>
<div style="text-align: center">
<img src="images/lec4/14.png" width="70%/"/>
</div>
<p>将最后一个矩阵乘法展开，我们发现实际上做的是一个三矩阵的乘法：</p>
<div style="text-align: center">
<img src="images/lec4/15.png" width="60%/"/>
</div>
<p>在<a href="../../algorithms/ads/8.html#ordering-matrix-multiplications"><span class="heti-skip"><span class="heti-spacing"> </span>ADS<span class="heti-spacing"> </span></span></a>中，我们讨论过：选择适当的矩阵乘法顺序有时可以显著降低计算量（甚至能减小好几个量级<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。所以这里我们尝试采用不同的乘法顺序来降低计算量。下面比较两种乘法顺序的计算量：</p>
<div style="text-align: center">
<img src="images/lec4/16.png" width="70%/"/>
</div>
<p>上图是原来的矩阵乘法，时间复杂度是<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(O(N^2)\)</span><span class="heti-spacing"> </span></span>的；下图就是改变顺序后的乘法，可以看到时间复杂度降至<span><span class="heti-spacing"> </span><span class="arithmatex">\(O(N)\)</span></span>。显然我们会采用下面这种乘法顺序，那么具体该怎么做呢？我们就从其中某个输出向量<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{b}^1\)</span><span class="heti-spacing"> </span></span>来分析——其计算公式为（用到了<a href="2.html#softmax"><span><span class="heti-spacing"> </span>softmax</span></a><heti-adjacent class="heti-adjacent-half">）</heti-adjacent>：</p>
<div class="arithmatex">\[
\bm{b}^{\textcolor{green}{1}} = \sum\limits_{\textcolor{red}{i}=1}^N a_{\textcolor{green}{1}, \textcolor{red}{i}=1}' \bm{v}^{\textcolor{red}{i}} = \sum\limits_{\textcolor{red}{i}=1}^N \dfrac{\exp(\bm{q}^{\textcolor{green}{1}} \cdot \bm{k}^{\textcolor{red}{i}})}{\sum_{\textcolor{cornflowerblue}{j}=1}^N \exp(\bm{q}^{\textcolor{green}{1}} \cdot \bm{k}^{\textcolor{cornflowerblue}{j}})} \bm{v}^{\textcolor{red}{i}}
\]</div>
<div style="text-align: center">
<img src="images/lec4/17.png" width="70%/"/>
</div>
<p>继续展开这个计算公式：</p>
<div class="arithmatex">\[
\begin{align}
\bm{b}^{\textcolor{green}{1}} = \sum\limits_{\textcolor{red}{i}=1}^N a_{\textcolor{green}{1}, \textcolor{red}{i}=1}' \bm{v}^{\textcolor{red}{i}} &amp; = \sum\limits_{\textcolor{red}{i}=1}^N \dfrac{\exp(\bm{q}^{\textcolor{green}{1}} \cdot \bm{k}^{\textcolor{red}{i}})}{\sum_{\textcolor{cornflowerblue}{j}=1}^N \exp(\bm{q}^{\textcolor{green}{1}} \cdot \bm{k}^{\textcolor{cornflowerblue}{j}})} \bm{v}^{\textcolor{red}{i}} \notag \\
&amp; = \sum\limits_{\textcolor{red}{i}=1}^N \dfrac{\varphi(\bm{q}^{\textcolor{green}{1}}) \cdot \varphi(\bm{k}^{\textcolor{red}{i}})}{\sum_{\textcolor{cornflowerblue}{j}=1}^N \varphi(\bm{q}^{\textcolor{green}{1}}) \cdot \varphi(\bm{k}^{\textcolor{cornflowerblue}{j}})} \bm{v}^{\textcolor{red}{i}} \notag \\
&amp; = \dfrac{\sum\limits_{\textcolor{red}{i}=1}^N [\varphi(\bm{q}^{\textcolor{green}{1}}) \cdot \varphi(\bm{k}^{\textcolor{red}{i}})] \bm{v}^{\textcolor{red}{i}}}{\sum_{\textcolor{cornflowerblue}{j}=1}^N \varphi(\bm{q}^{\textcolor{green}{1}}) \cdot \varphi(\bm{k}^{\textcolor{cornflowerblue}{j}})} \notag
\end{align}
\]</div>
<ul>
<li>
<p>第一行到第二行，我们用了这样的公式：
    $$
    \exp(\bm{q} \cdot \bm{k}) \approx \varphi(\bm{q}) \cdot \varphi(\bm{k})
    $$</p>
<p>其中<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\varphi()\)</span><span class="heti-spacing"> </span></span>是一个接收向量，输出也是向量的转换函数（暂时不用管具体是什么）</p>
</li>
<li>
<p>对于最后一行公式中的分母部分，还可以进一步转化为：
    $$
    \varphi(\bm{q}^{\textcolor{green}{1}}) \cdot \sum_{\textcolor{cornflowerblue}{j}=1}^N \varphi(\bm{k}^{\textcolor{cornflowerblue}{j}})
    $$</p>
<p>这实际上是一个向量乘法：</p>
<p></p><div style="text-align: center">
<img src="images/lec4/18.png" width="30%/"/>
</div>
<p>黄色部分就是<span><span class="heti-spacing"> </span><span class="arithmatex">\(\sum_{\textcolor{cornflowerblue}{j}=1}^N \varphi(\bm{k}^{\textcolor{cornflowerblue}{j}})\)</span></span>，而红色则对应<span><span class="heti-spacing"> </span><span class="arithmatex">\(\varphi(\bm{q}^{\textcolor{green}{1}})\)</span></span></p>
</li>
<li>
<p>再来看分子部分（下图蓝框框出来的公式<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，我们可以像这样展开
    &gt;字太多了，不想打 <span class="arithmatex">\(\LaTeX\)</span> 公式了，所以就直接贴个截图...</p>
<p></p><div style="text-align: center">
<img src="images/lec4/19.png" width="80%/"/>
</div>
<p>用矩阵表示最后一行的式子（注意查询向量是<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(M\)</span><span class="heti-spacing"> </span></span>维的<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>：</p>
<p></p><div style="text-align: center">
<img src="images/lec4/20.png" width="80%/"/>
</div>
</li>
<li>
<p>同时考虑分子分母，现在<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{b}^1\)</span><span class="heti-spacing"> </span></span>的计算变成了多个向量的乘除法了</p>
<p></p><div style="text-align: center">
<img src="images/lec4/21.png" width="70%/"/>
</div>
</li>
</ul>
<p>上面这种处理的好处是：图中灰色方框和黄色方框对应的部分仅需计算一次，之后计算输出向量时就可以复用这些结果，从而减小计算量。</p>
<div style="text-align: center">
<img src="images/lec4/22.png" width="60%/"/>
</div>
<p>其中灰色方框内的<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(M\)</span><span class="heti-spacing"> </span></span>个向量（蓝色方块）称为<strong>模板</strong>(template)。</p>
<div style="text-align: center">
<img src="images/lec4/23.png" width="70%/"/>
</div>
<p>关于函数<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\varphi()\)</span><span class="heti-spacing"> </span></span>的具体实现，可以参考以下几篇<span><span class="heti-spacing"> </span>paper</span>：</p>
<div style="text-align: center">
<img src="images/lec4/24.png" width="60%/"/>
</div>
<h3 id="synthesizer">Synthesizer<a class="headerlink" href="#synthesizer" title="Permanent link">⚓︎</a></h3>
<p>我们还可以再脑洞大开一点——计算自注意的时候一定要先算出查询和键吗？一种称为<strong>合成器</strong><span>(synthesizer)<span class="heti-spacing"> </span></span>的技术就跳过这个步骤——它的注意权重作为网络参数，是从模型中学出来的（和<a href="#learnable-patterns-sinkhorn-sorting-network"><span class="heti-skip"><span class="heti-spacing"> </span>Sinkhorn<span class="heti-spacing"> </span></span>排序网络</a>很像<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。</p>
<div style="text-align: center">
<img src="images/lec4/25.png" width="70%/"/>
</div>
<h3 id="attention-free-techniques">Attention-free Techniques<a class="headerlink" href="#attention-free-techniques" title="Permanent link">⚓︎</a></h3>
<p>更进一步思考——处理序列的模型非要用到自注意机制吗？下面是一些尝试丢掉自注意机制的方案：</p>
<div style="text-align: center">
<img src="images/lec4/26.png" width="70%/"/>
</div></div>
<aside class="md-source-file">
<span class="md-source-file__fact">
<span class="md-icon" title="最后更新">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"></path></svg>
</span>
<span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime" title="2025年3月21日 15:57:34">2025年3月21日 15:57:34</span>
</span>
<span class="md-source-file__fact">
<span class="md-icon" title="创建日期">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"></path></svg>
</span>
<span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime" title="2025年3月2日 21:04:42">2025年3月2日 21:04:42</span>
</span>
</aside>
<p style="font-size: 30px; font-weight: 600">评论区</p>
<div>
    如果大家有什么问题或想法，欢迎在下方留言~
  </div>
<!-- Insert generated snippet here -->
<script async="" crossorigin="anonymous" data-category="Announcements" data-category-id="DIC_kwDOMAb9Zs4CfmpP" data-emit-metadata="0" data-input-position="bottom" data-lang="zh-CN" data-mapping="pathname" data-reactions-enabled="1" data-repo="noughtq/notebook" data-repo-id="R_kgDOMAb9Zg" data-strict="0" data-theme="preferred_color_scheme" src="https://giscus.app/client.js">
</script>
<!-- Synchronize Giscus theme with palette -->
<script>
    var giscus = document.querySelector("script[src*=giscus]")

    // Set palette on initial load
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
      var theme = palette.color.scheme === "slate"
        ? "transparent_dark"
        : "light"

      // Instruct Giscus to set theme
      giscus.setAttribute("data-theme", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener("DOMContentLoaded", function() {
      var ref = document.querySelector("[data-md-component=palette]")
      ref.addEventListener("change", function() {
        var palette = __md_get("__palette")
        if (palette && typeof palette.color === "object") {
          var theme = palette.color.scheme === "slate"
            ? "dark"
            : "light"

          // Instruct Giscus to change theme
          var frame = document.querySelector(".giscus-frame")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            "https://giscus.app"
          )
        }
      })
    })
  </script>
<!-- 标题计数器 -->
<link href="/css/counter.css" rel="stylesheet"/>
<!-- 主页个性化 -->
</article>
</div>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div>
<button class="md-top md-icon" data-md-component="top" hidden="" type="button">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  回到页面顶部
</button>
</main>
<footer class="md-footer">
<nav aria-label="页脚" class="md-footer__inner md-grid">
<a aria-label="上一页: Deep Learning" class="md-footer__link md-footer__link--prev" href="3.html">
<div class="md-footer__button md-icon">
<svg viewbox="0 0 320 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path></svg>
</div>
<div class="md-footer__title">
<span class="md-footer__direction">
                上一页
              </span>
<div class="md-ellipsis">
                Deep Learning
              </div>
</div>
</a>
<a aria-label="下一页: Transformer" class="md-footer__link md-footer__link--next" href="5.html">
<div class="md-footer__title">
<span class="md-footer__direction">
                下一页
              </span>
<div class="md-ellipsis">
                Transformer
              </div>
</div>
<div class="md-footer__button md-icon">
<svg viewbox="0 0 320 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg>
</div>
</a>
</nav>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright" style="margin-left: 33.5%">
<div class="md-copyright__highlight" style="text-align: center">
        Copyright © 2024-2025 <a href="https://github.com/NoughtQ">NoughtQ</a>
</div>
    
    
      Powered by
      <a href="https://www.mkdocs.org/" rel="noopener" target="_blank">
        MkDocs
      </a>
      with theme
      <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
        Material
      </a>
      modified by
      <a href="https://github.com/NoughtQ" rel="noopener" target="_blank">
        NoughtQ
      </a>
<!-- <br> -->
<div style="text-align: center;">
<a href="https://icp.gov.moe/?keyword=20252357" target="_blank">萌ICP备20252357号</a>
</div>
</div>
<div class="md-social">
<a class="md-social__link" href="https://github.com/noughtq" rel="noopener" target="_blank" title="github.com">
<svg viewbox="0 0 496 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
</a>
<a class="md-social__link" href="https://blog.noughtq.top" rel="noopener" target="_blank" title="blog.noughtq.top">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M192 32c0 17.7 14.3 32 32 32 123.7 0 224 100.3 224 224 0 17.7 14.3 32 32 32s32-14.3 32-32C512 128.9 383.1 0 224 0c-17.7 0-32 14.3-32 32m0 96c0 17.7 14.3 32 32 32 70.7 0 128 57.3 128 128 0 17.7 14.3 32 32 32s32-14.3 32-32c0-106-86-192-192-192-17.7 0-32 14.3-32 32m-96 16c0-26.5-21.5-48-48-48S0 117.5 0 144v224c0 79.5 64.5 144 144 144s144-64.5 144-144-64.5-144-144-144h-16v96h16c26.5 0 48 21.5 48 48s-21.5 48-48 48-48-21.5-48-48z"></path></svg>
</a>
<a class="md-social__link" href="mailto:noughtq666@gmail.com" rel="noopener" target="_blank" title="">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m20 8-8 5-8-5V6l8 5 8-5m0-2H4c-1.11 0-2 .89-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../..", "features": ["content.action.edit", "content.action.view", "content.code.copy", "content.code.annotate", "content.footnote.tooltips", "navigation.tabs", "navigation.top", "navigation.footer", "navigation.indexes", "navigation.tracking", "navigation.prune", "search.share"], "search": "../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script>
<script src="../../assets/javascripts/bundle.13a4f30d.min.js"></script>
<script src="../../js/anchor.js"></script>
<script src="../../js/katex.js"></script>
<script src="../../js/toc.js"></script>
<script src="../../js/typed.js"></script>
<script src="../../js/custom.js"></script>
<script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
<script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
</body>
</html>