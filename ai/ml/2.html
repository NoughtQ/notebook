<!DOCTYPE html>
<html class="no-js" lang="zh">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="NoughtQ的笔记本" name="description"/>
<meta content="NoughtQ" name="author"/>
<link href="https://notebook.noughtq.top/ai/ml/2.html" rel="canonical"/>
<link href="1.html" rel="prev"/>
<link href="3.html" rel="next"/>
<link href="../../assets/favicon.png" rel="icon"/>
<meta content="mkdocs-1.6.1, mkdocs-material-9.6.3" name="generator"/>
<title>Lec 2: Deep Learning - NoughtQ的笔记本</title>
<link href="../../assets/stylesheets/main.d7758b05.min.css" rel="stylesheet"/>
<link href="../../assets/stylesheets/palette.06af60db.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=JetBrains+Mono,+LXGW+WenKai+Screen+GB+Screen:300,300i,400,400i,700,700i%7CJetBrains+Mono,+Consolas:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"JetBrains Mono, LXGW WenKai Screen GB Screen";--md-code-font:"JetBrains Mono, Consolas"}</style>
<link href="../../css/heti.css" rel="stylesheet"/>
<link href="../../css/toc_extra.css" rel="stylesheet"/>
<link href="../../css/timeline.css" rel="stylesheet"/>
<link href="../../css/card.css" rel="stylesheet"/>
<link href="../../css/custom.css" rel="stylesheet"/>
<link href="../../css/extra_changelog.css" rel="stylesheet"/>
<link href="https://unpkg.com/katex@0/dist/katex.min.css" rel="stylesheet"/>
<link href="https://gcore.jsdelivr.net/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css" rel="stylesheet"/>
<link href="https://gcore.jsdelivr.net/npm/lxgw-wenkai-webfont@1.1.0/style.css" rel="stylesheet"/>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&amp;display=swap" rel="stylesheet"/>
<script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-43NH8CVRCJ"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-43NH8CVRCJ",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-43NH8CVRCJ",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
<script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
</head>
<body data-md-color-accent="indigo" data-md-color-primary="indigo" data-md-color-scheme="slate" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#lec-2-deep-learning">
          跳转至
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header" data-md-component="header">
<nav aria-label="页眉" class="md-header__inner md-grid">
<a aria-label="NoughtQ的笔记本" class="md-header__button md-logo" data-md-component="logo" href="../.." title="NoughtQ的笔记本">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.05 9H7.06V6h1.99V4.03H7.06v-1c0-1.11.89-1.99 1.99-1.99h5.98V8l2.47-1.5L20 8V1.04h1c1.05 0 2 .96 2 1.99V17c0 1.03-.95 2-2 2H9.05c-1.05 0-1.99-.95-1.99-2v-1h1.99v-2H7.06v-3h1.99zM1 18h2v-3H1v-2h2v-3H1V8h2V5h2v3H3v2h2v3H3v2h2v3H3v2h2v1h16v2H5a2 2 0 0 1-2-2v-1H1z"></path></svg>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            NoughtQ的笔记本
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              Lec 2: Deep Learning
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Dark Mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefer-color-scheme: dark)" data-md-color-primary="indigo" data-md-color-scheme="slate" id="__palette_0" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Dark Mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
</label>
<input aria-label="Light Mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefer-color-scheme: light)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_0" hidden="" title="Light Mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"></path></svg>
</label>
</form>
<script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="搜索" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="搜索" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
<svg viewbox="0 0 320 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path></svg>
</label>
<nav aria-label="查找" class="md-search__options">
<a aria-label="分享" class="md-search__icon md-icon" data-clipboard="" data-clipboard-text="" data-md-component="search-share" href="javascript:void(0)" tabindex="-1" title="分享">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg>
</a>
<button aria-label="清空当前内容" class="md-search__icon md-icon" tabindex="-1" title="清空当前内容" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
</button>
</nav>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="" tabindex="0">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
<ol class="md-search-result__list" role="presentation"></ol>
</div>
</div>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/noughtq/notebook" title="前往仓库">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"></path></svg>
</div>
<div class="md-source__repository">
    NoughtQ/Notebook
  </div>
</a>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<nav aria-label="标签" class="md-tabs" data-md-component="tabs">
<div class="md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../index.html">
          
  
    
  
  🏫主页

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../lang/index.html">
          
  
    
  
  🔡语言

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../math/index.html">
          
  
    
  
  📊数学相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../algorithms/index.html">
          
  
    
  
  🧮算法相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../software/index.html">
          
  
    
  
  💾软件相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../system/index.html">
          
  
    
  
  💻系统相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../web/index.html">
          
  
    
  
  🌏Web相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../sec/ctf-101/index.html">
          
  
    
  
  🛡️信息安全

        </a>
</li>
<li class="md-tabs__item md-tabs__item--active">
<a class="md-tabs__link" href="../index.html">
          
  
    
  
  🤖人工智能

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../misc/index.html">
          
  
    
  
  🗃️杂项

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../tools/index.html">
          
  
    
  
  🛠️工具

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../papers/index.html">
          
  
    
  
  📑论文阅读

        </a>
</li>
</ul>
</div>
</nav>
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="导航栏" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="NoughtQ的笔记本" class="md-nav__button md-logo" data-md-component="logo" href="../.." title="NoughtQ的笔记本">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.05 9H7.06V6h1.99V4.03H7.06v-1c0-1.11.89-1.99 1.99-1.99h5.98V8l2.47-1.5L20 8V1.04h1c1.05 0 2 .96 2 1.99V17c0 1.03-.95 2-2 2H9.05c-1.05 0-1.99-.95-1.99-2v-1h1.99v-2H7.06v-3h1.99zM1 18h2v-3H1v-2h2v-3H1V8h2V5h2v3H3v2h2v3H3v2h2v3H3v2h2v1h16v2H5a2 2 0 0 1-2-2v-1H1z"></path></svg>
</a>
    NoughtQ的笔记本
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/noughtq/notebook" title="前往仓库">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"></path></svg>
</div>
<div class="md-source__repository">
    NoughtQ/Notebook
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../index.html">
<span class="md-ellipsis">
    🏫主页
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../lang/index.html">
<span class="md-ellipsis">
    🔡语言
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../math/index.html">
<span class="md-ellipsis">
    📊数学相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../algorithms/index.html">
<span class="md-ellipsis">
    🧮算法相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../software/index.html">
<span class="md-ellipsis">
    💾软件相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../system/index.html">
<span class="md-ellipsis">
    💻系统相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../web/index.html">
<span class="md-ellipsis">
    🌏Web相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../sec/ctf-101/index.html">
<span class="md-ellipsis">
    🛡️信息安全
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_9" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../index.html">
<span class="md-ellipsis">
    🤖人工智能
    
  </span>
</a>
<label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_9_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_9">
<span class="md-nav__icon md-icon"></span>
            🤖人工智能
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_9_2" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="index.html">
<span class="md-ellipsis">
    机器学习
    
  </span>
</a>
<label class="md-nav__link" for="__nav_9_2" id="__nav_9_2_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_9_2_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_9_2">
<span class="md-nav__icon md-icon"></span>
            机器学习
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="1.html">
<span class="md-ellipsis">
    Lec 1: Intro to Machine Learning
    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
<span class="md-ellipsis">
    Lec 2: Deep Learning
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="2.html">
<span class="md-ellipsis">
    Lec 2: Deep Learning
    
  </span>
</a>
<nav aria-label="目录" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
        目录
      </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#general-guidance">
<span class="md-ellipsis">
      General Guidance
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#more-insight-into-optimization">
<span class="md-ellipsis">
      More Insight into Optimization
    </span>
</a>
<nav aria-label="More Insight into Optimization" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#backpropogation">
<span class="md-ellipsis">
      Backpropogation
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#critical-points">
<span class="md-ellipsis">
      Critical Points
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#batches">
<span class="md-ellipsis">
      Batches
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#momentum">
<span class="md-ellipsis">
      Momentum
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adaptive-learning-rate">
<span class="md-ellipsis">
      Adaptive Learning Rate
    </span>
</a>
<nav aria-label="Adaptive Learning Rate" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#adagrad">
<span class="md-ellipsis">
      Adagrad
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#rmsprop">
<span class="md-ellipsis">
      RMSProp
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#learning-rate-scheduling">
<span class="md-ellipsis">
      Learning Rate Scheduling
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#classfication">
<span class="md-ellipsis">
      Classfication
    </span>
</a>
<nav aria-label="Classfication" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#softmax">
<span class="md-ellipsis">
      Softmax
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#loss">
<span class="md-ellipsis">
      Loss
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#example-pokémondigimon-classifier">
<span class="md-ellipsis">
      Example: Pokémon/Digimon Classifier
    </span>
</a>
<nav aria-label="Example: Pokémon/Digimon Classifier" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#function-with-unknown-parameters">
<span class="md-ellipsis">
      Function with Unknown Parameters
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#loss-of-a-function">
<span class="md-ellipsis">
      Loss of a Function
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#probability-of-failure">
<span class="md-ellipsis">
      Probability of Failure
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#model-complexity">
<span class="md-ellipsis">
      Model Complexity
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="3.html">
<span class="md-ellipsis">
    Lec 3: Image as input
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="4.html">
<span class="md-ellipsis">
    Lec 4: Sequence as input
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="5.html">
<span class="md-ellipsis">
    Lec 5: Sequence to sequence
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="6.html">
<span class="md-ellipsis">
    Lec 6: Generation
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="7.html">
<span class="md-ellipsis">
    Lec 7: Recent Advance of Self-supervised Learning for NLP
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="8.html">
<span class="md-ellipsis">
    Lec 8: Self-supervised learning for Speech and Image
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="9.html">
<span class="md-ellipsis">
    Lec 9: Auto-encoder / Anomaly Detection
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="10.html">
<span class="md-ellipsis">
    Lec 10: Explainable AI
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="11.html">
<span class="md-ellipsis">
    Lec 11: Attack
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="12.html">
<span class="md-ellipsis">
    Lec 12: Adaptation
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="13.html">
<span class="md-ellipsis">
    Lec 13: Reinforcement Learning
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="14.html">
<span class="md-ellipsis">
    Lec 14: Network Compression
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="15.html">
<span class="md-ellipsis">
    Lec 15: Life-long Learning
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="16.html">
<span class="md-ellipsis">
    Lec 16: Meta Learning
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../cv/index.html">
<span class="md-ellipsis">
    计算机视觉
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../misc/index.html">
<span class="md-ellipsis">
    🗃️杂项
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../tools/index.html">
<span class="md-ellipsis">
    🛠️工具
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../papers/index.html">
<span class="md-ellipsis">
    📑论文阅读
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="目录" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
        目录
      </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#general-guidance">
<span class="md-ellipsis">
      General Guidance
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#more-insight-into-optimization">
<span class="md-ellipsis">
      More Insight into Optimization
    </span>
</a>
<nav aria-label="More Insight into Optimization" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#backpropogation">
<span class="md-ellipsis">
      Backpropogation
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#critical-points">
<span class="md-ellipsis">
      Critical Points
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#batches">
<span class="md-ellipsis">
      Batches
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#momentum">
<span class="md-ellipsis">
      Momentum
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adaptive-learning-rate">
<span class="md-ellipsis">
      Adaptive Learning Rate
    </span>
</a>
<nav aria-label="Adaptive Learning Rate" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#adagrad">
<span class="md-ellipsis">
      Adagrad
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#rmsprop">
<span class="md-ellipsis">
      RMSProp
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#learning-rate-scheduling">
<span class="md-ellipsis">
      Learning Rate Scheduling
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#classfication">
<span class="md-ellipsis">
      Classfication
    </span>
</a>
<nav aria-label="Classfication" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#softmax">
<span class="md-ellipsis">
      Softmax
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#loss">
<span class="md-ellipsis">
      Loss
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#example-pokémondigimon-classifier">
<span class="md-ellipsis">
      Example: Pokémon/Digimon Classifier
    </span>
</a>
<nav aria-label="Example: Pokémon/Digimon Classifier" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#function-with-unknown-parameters">
<span class="md-ellipsis">
      Function with Unknown Parameters
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#loss-of-a-function">
<span class="md-ellipsis">
      Loss of a Function
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#probability-of-failure">
<span class="md-ellipsis">
      Probability of Failure
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#model-complexity">
<span class="md-ellipsis">
      Model Complexity
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<a class="md-content__button md-icon" href="https://github.com/noughtq/notebook/edit/master/docs/ai/ml/2.md" title="编辑此页">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
</a>
<a class="md-content__button md-icon" href="https://github.com/noughtq/notebook/raw/master/docs/ai/ml/2.md" title="查看本页的源代码">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
</a>
<h1 id="lec-2-deep-learning">Lec 2: Deep Learning<a class="headerlink" href="#lec-2-deep-learning" title="Permanent link">⚓︎</a></h1>
<div style="margin-top: -30px; font-size: 0.9em; opacity: 0.7;">
<p><span class="twemoji"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10h-2a8 8 0 0 1-8 8 8 8 0 0 1-8-8 8 8 0 0 1 8-8zm6.78 1a.7.7 0 0 0-.48.2l-1.22 1.21 2.5 2.5L20.8 5.7c.26-.26.26-.7 0-.95L19.25 3.2c-.13-.13-.3-.2-.47-.2m-2.41 2.12L9 12.5V15h2.5l7.37-7.38z"></path></svg></span> 约<span class="heti-skip"><span class="heti-spacing"> </span>11386<span class="heti-spacing"> </span></span>个字 <span class="twemoji"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 20c4.42 0 8-3.58 8-8s-3.58-8-8-8-8 3.58-8 8 3.58 8 8 8m0-18c5.5 0 10 4.5 10 10s-4.5 10-10 10C6.47 22 2 17.5 2 12S6.5 2 12 2m.5 11H11V7h1.5v4.26l3.7-2.13.75 1.3z"></path></svg></span> 预计阅读时间<span class="heti-skip"><span class="heti-spacing"> </span>57<span class="heti-spacing"> </span></span>分钟</p>
</div>
<h2 id="general-guidance">General Guidance<a class="headerlink" href="#general-guidance" title="Permanent link">⚓︎</a></h2>
<p>我们可以将深度学习的任务（尤其是这门课的<span><span class="heti-spacing"> </span>HW</span>）简单地抽象为：</p>
<ul>
<li>给定一组训练数据<span><span class="heti-spacing"> </span><span class="arithmatex">\(\{(\bm{x^1}, \hat{y}^1), (\bm{x^2}, \hat{y}^2), \dots, (\bm{x^N}, \hat{y}^N)\}\)</span></span>，以及一组测试数据<span><span class="heti-spacing"> </span><span class="arithmatex">\(\{\bm{x^{N+1}}, \bm{x^{N+2}}, \dots, \bm{x^{N+M}}\}\)</span></span></li>
<li>
<p>在训练数据上进行训练：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/35.png" width="80%/"/>
</div>
</li>
<li>
<p>使用效果最好的模型<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(y = f_{\bm{\theta}^*}(\bm{x})\)</span><span class="heti-spacing"> </span></span>来标注测试数据，得到预测结果<span><span class="heti-spacing"> </span><span class="arithmatex">\(\{y^{N+1}, y^{N+2}, \dots, y^{N+M}\}\)</span></span>，然后上传到<span class="heti-skip"><span class="heti-spacing"> </span>Kaggle<span class="heti-spacing"> </span></span>评分</p>
</li>
</ul>
<p>那我们该如何得到一个较好的模型呢？或者说当我们遇到一个效果不好的模型时，我们该如何改进呢？下面给出一张总体的方向图（相当<span><span class="heti-spacing"> </span>nice</span><heti-adjacent class="heti-adjacent-half">！</heti-adjacent>）</p>
<div style="text-align: center">
<img src="images/lec2/36.png" width="80%/"/>
</div>
<p>可以看到，如果某个模型无论在训练数据，还是在测试数据上的损失都很小的话，那么这个模型的表现就很好了。但只要有一种损失很大的话，就需要通过各种方法来改进我们的模型，那下面就来逐一分析改善模型的思路：</p>
<ul>
<li>
<p><strong>模型偏移</strong><span class="heti-skip"><span class="heti-spacing"> </span>(model bias)<span class="heti-spacing"> </span></span>的问题</p>
<ul>
<li>
<p>原因：模型过于简单。比如有一个简单的模型<span><span class="heti-spacing"> </span><span class="arithmatex">\(y = f_{\bm{\theta}}(\bm{x})\)</span></span>，通过训练不断寻找合适的参数，获得<span><span class="heti-spacing"> </span><span class="arithmatex">\(f_{\bm{\theta}^1}(\bm{x}), f_{\bm{\theta}^2}(\bm{x}), \dots\)</span></span>。最后得到一个损失尽可能小的模型<span><span class="heti-spacing"> </span><span class="arithmatex">\(f_{\bm{\theta}^*}(\bm{x})\)</span></span>，但实际上相比其他模型<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(f^*(\bm{x})\)</span><span class="heti-spacing"> </span></span>它的损失还是很大。下图形象说明了这一点：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/37.png" width="50%/"/>
</div>
<p>这就好比在大海里捞针，但实际上这根针并不在这片海域里<span><span class="heti-spacing"> </span>...</span></p>
</li>
<li>
<p>解决方法：重新设计模型，使其变得更有弹性<span><span class="heti-spacing"> </span>(flexible)</span>，具体做法可以是：</p>
<ul>
<li>采取更多的特征点<span><span class="heti-spacing"> </span>(features)</span></li>
<li>进行更深的深度学习（使用更多的神经元或层）</li>
</ul>
<p></p><div style="text-align: center">
<img src="images/lec2/38.png" width="70%/"/>
</div>
</li>
</ul>
</li>
<li>
<p><strong>优化</strong><span class="heti-skip"><span class="heti-spacing"> </span>(optimization)<span class="heti-spacing"> </span></span>的问题</p>
<ul>
<li>
<p>产生较大的损失有时不一定意味着模型的问题，也可能是优化策略选的不够好。比如前面介绍的梯度下降法就有可能陷入局部最小的问题，从而无法找到真正的最小值，下图形象展示了这一点：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/39.png" width="80%/"/>
</div>
<p>这就好比在大海里捞针，但就是找不到这根针</p>
</li>
<li>
<p>解决方法：使用更强大的优化技术（之后会介绍）</p>
</li>
</ul>
</li>
</ul>
<details class="note">
<summary>如何判断是模型偏移问题还是优化问题？</summary>
<ul>
<li>
<p>比较不同模型的效果</p>
<p></p><div style="text-align: center">
<img src="images/lec2/40.png" width="80%/"/>
</div>
<p>如图所示，光看测试数据的结果，如果更深的网络效果更差，那么除了优化问题外还有可能是过拟合问题。然而，如果在训练数据上，更深的网络效果还是更差，那么就应该是优化问题了。</p>
</li>
<li>
<p>从更浅（层数更少）的网络（或其他模型）入手，更方便优化</p>
</li>
<li>如果更深的网络并没有在训练数据上得到更小的损失，那么就是优化问题了<ul>
<li>因为相比浅的网络，深的网络更具弹性。我们可以让网络中的某些层对数据不做任何改动，这样可以让该网络等价于层数更少的网络。因此通过对模型的修改是可以让更深的网络得到不高于更浅的网络的损失，那么上述问题就应该来自于优化方法的局限了。</li>
</ul>
</li>
</ul>
</details>
<ul>
<li>
<p><strong>过拟合</strong><span class="heti-skip"><span class="heti-spacing"> </span>(overfitting)<span class="heti-spacing"> </span></span>的问题：在训练数据上损失很小，但在测试数据上损失较大</p>
<ul>
<li>一个极端的例子：假如有训练数据<span><span class="heti-spacing"> </span><span class="arithmatex">\(\{(\bm{x^1}, \hat{y}^1), (\bm{x^2}, \hat{y}^2), \dots, (\bm{x^N}, \hat{y}^N)\}\)</span></span>，使用模型<span><span class="heti-spacing"> </span><span class="arithmatex">\(f(\bm{x}) = \begin{cases}\hat{y}^i &amp; \exists \bm{x^i} = \bm{x} \\ random &amp; otherwise\end{cases}\)</span></span>。显然这个模型没什么用，但是它在训练数据上是没有损失的（因为就是简单的<span class="heti-skip"><span class="heti-spacing"> </span>copy<span class="heti-spacing"> </span></span>训练数据<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，但在测试数据上损失很大（全部随机数，<del>所以也有极小概率遇到损失很小的情况</del>）</li>
<li>
<p>原因：假设已知真实的数据分布（也就是最理想模型应有的样子，但无法直接观测；且由于数据是离散的，所以用虚线表示）以及训练数据（曲线上的部分点<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。</p>
<p></p><div style="text-align: center">
<img src="images/lec2/41.png" width="40%/"/>
</div>
<p>如果我们采取弹性较大的模型，并且让该模型恰好经过这些训练数据点，那么我们可能会得到以下结果：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/42.png" width="40%/"/>
</div>
<p>由于模型的弹性够大，对于非训练数据上的其他位置，它有着很高的自由变化程度，因而测试数据上的损失就会变得很大了，如下图所示：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/43.png" width="70%/"/>
</div>
</li>
<li>
<p>解决方法：</p>
<ul>
<li>
<p>使用更多的训练数据：这是一种简单有效的方法（但<span class="heti-skip"><span class="heti-spacing"> </span>HW<span class="heti-spacing"> </span></span>不能这样做）</p>
<p></p><div style="text-align: center">
<img src="images/lec2/44.png" width="70%/"/>
</div>
</li>
<li>
<p><strong>数据增强</strong><span><span class="heti-spacing"> </span>(data augmentation)</span>：在训练数据的基础上得到一些稍作修改的副本，从而形成更多的训练数据</p>
<ul>
<li>但是不要有太大或没意义的修改，比如对于一幅图像，对其左右翻转，适当缩放得到的图像都是<span class="heti-skip"><span class="heti-spacing"> </span>OK<span class="heti-spacing"> </span></span>的，但是上下翻转的图像就没什么实际意义了，因为通常人类不会阅读上下颠倒的图像</li>
</ul>
<p></p><div style="text-align: center">
<img src="images/lec2/45.png" width="90%/"/>
</div>
</li>
<li>
<p>给模型一些<strong>限制</strong><span><span class="heti-spacing"> </span>(constrain)</span>，降低其弹性</p>
<ul>
<li>
<p>举例：比如规定模型就是一个二次函数，那么会得到以下结果：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/46.png" width="90%/"/>
</div>
<p>可以看到模型效果会好很多。</p>
</li>
<li>
<p>具体来说，可以通过以下方法来限制模型：</p>
<ul>
<li>减少未知参数，或让网络的层之间共享一些参数<ul>
<li>后者就是<strong>卷积神经网络</strong><span class="heti-skip"><span class="heti-spacing"> </span>(CNN)<span class="heti-spacing"> </span></span>的做法，它是<strong>全连接</strong><span class="heti-skip"><span class="heti-spacing"> </span>(fully-connected)<span class="heti-spacing"> </span></span>神经网络（也就是之前介绍的一般的神经网络）的特殊情况，之后会详细介绍</li>
</ul>
</li>
<li>采用更少的特征点</li>
<li>提早结束训练<span><span class="heti-spacing"> </span>(early stopping)</span></li>
<li>正则化<span><span class="heti-spacing"> </span>(regularization)</span></li>
<li>dropout</li>
</ul>
</li>
<li>
<p>当然，限制不能太多，否则模型就会在训练数据上得到更大的损失，问题就回到了<strong>模型偏移</strong>上了</p>
<p></p><div style="text-align: center">
<img src="images/lec2/47.png" width="90%/"/>
</div>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>综上，我们需要权衡好模型偏移（与损失成正比）和模型的复杂性（或弹性）的关系，从而让模型在训练数据和测试数据上的损失都尽可能地小。下图展示了二者的关系：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/48.png" width="80%/"/>
</div>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">交叉验证<span><span class="heti-spacing"> </span>(cross validation)</span></p>
<p>在深度学习的作业平台<span class="heti-skip"><span class="heti-spacing"> </span>kaggle<span class="heti-spacing"> </span></span>上，我们用提供的训练数据得到一些模型，然后用公开的<span class="heti-skip"><span class="heti-spacing"> </span>(public)<span class="heti-spacing"> </span></span>测试数据上使用这些模型。如果发现其中某个模型的损失很小，并不意味着这个模型能够在私有的<span class="heti-skip"><span class="heti-spacing"> </span>(private)<span class="heti-spacing"> </span></span>测试数据（另一批测试数据，但是要在<span class="heti-skip"><span class="heti-spacing"> </span>ddl<span class="heti-spacing"> </span></span>之后才会用）上具备相似的损失量。</p>
<p></p><div style="text-align: center">
<img src="images/lec2/49.png" width="80%/"/>
</div>
<p>还是拿之前介绍过的极端例子为例：<span class="arithmatex">\(f_k(\bm{x}) = \begin{cases}\hat{y}^i &amp; \exists \bm{x^i} = \bm{x} \\ random &amp; otherwise\end{cases}\)</span>，其中<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(k\)</span><span class="heti-spacing"> </span></span>表示训练次数。如果训练次数足够大，那就有可能遇到某次训练下的模型，碰巧在公共的测试数据上损失很小的情况，而实际上在私有的测试数据上的表现情况是随机的、未知的。所以仅根据公有测试数据的结果对模型效果做判断是不明智的。这也说明了有些模型即使很烂，但在某些测试数据集上的表现远超人类的异常现象。</p>
<p>改进的做法是将原来的训练数据集分为两部分：一部分仍然作为训练数据，另一部分作为验证<span class="heti-skip"><span class="heti-spacing"> </span>(validation)<span class="heti-spacing"> </span></span>数据，不参与模型的训练中。当在这些训练数据中训练出一些模型后，我们将模型放到验证数据集上跑，观察损失有多大。根据这一损失选择的模型，无论是在公有的还是私有的测试数据集上，得到的损失应该是差不多的。尽管如此，还是不建议仅参考公有测试数据的损失来改进模型，因为这样可能出现过拟合的问题。</p>
<p></p><div style="text-align: center">
<img src="images/lec2/50.png" width="80%/"/>
</div>
<p>如果不清除该如何从原训练数据集中分出一部分作为验证数据，那么可以采用<strong><span class="heti-skip"><span class="heti-spacing"> </span>N<span class="heti-spacing"> </span></span>折交叉验证</strong><span><span class="heti-spacing"> </span>(N-fold cross validation)</span>：</p>
<ul>
<li>将训练数据集分为<span class="heti-skip"><span class="heti-spacing"> </span>N<span class="heti-spacing"> </span></span>等分</li>
<li>取其中<span class="heti-skip"><span class="heti-spacing"> </span>1<span class="heti-spacing"> </span></span>份作为验证数据，余下<span class="heti-skip"><span class="heti-spacing"> </span>N - 1<span class="heti-spacing"> </span></span>份作为训练数据，在此基础上训练模型</li>
<li>然后取另<span class="heti-skip"><span class="heti-spacing"> </span>1<span class="heti-spacing"> </span></span>份作为验证数据，余下<span class="heti-skip"><span class="heti-spacing"> </span>N - 1<span class="heti-spacing"> </span></span>份作为训练数据，在此基础上训练模型</li>
<li>以此类推，直到每份数据都曾作为验证数据过，此时得到模型的<span class="heti-skip"><span class="heti-spacing"> </span>N<span class="heti-spacing"> </span></span>次训练下的损失，取平均值作为最终的损失</li>
<li>比较不同模型的损失，取损失最小的模型，在整个训练数据集上重新训练，得到的模型用在测试数据上，这样在公有或私有的测试数据上的损失也差不多</li>
</ul>
<p></p><div style="text-align: center">
<img src="images/lec2/51.png" width="80%/"/>
</div>
</div>
</li>
<li>
<p><strong>错误匹配</strong><span class="heti-skip"><span class="heti-spacing"> </span>(mismatch)<span class="heti-spacing"> </span></span>的问题</p>
<ul>
<li>原因：训练数据和测试数据的分布不同</li>
<li>此时需要注意数据是如何生成的</li>
<li>除了<span class="heti-skip"><span class="heti-spacing"> </span>HW11<span class="heti-spacing"> </span></span>外，绝大多数<span class="heti-skip"><span class="heti-spacing"> </span>HW<span class="heti-spacing"> </span></span>没有这个问题</li>
</ul>
</li>
</ul>
<h2 id="more-insight-into-optimization">More Insight into Optimization<a class="headerlink" href="#more-insight-into-optimization" title="Permanent link">⚓︎</a></h2>
<h3 id="backpropogation">Backpropogation<a class="headerlink" href="#backpropogation" title="Permanent link">⚓︎</a></h3>
<p>在深度学习中，未知参数<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\theta = \{w_1, w_2, \dots, b_1, b_2, \dots\}\)</span><span class="heti-spacing"> </span></span>通常会包含很多项。在使用梯度下降法时，每次都要对里面的每一项求偏微分；而且如果训练多次的话，计算量就会变得相当地大，因而效率就很低了。为了提高计算的效率，我们引入了一种新的计算技巧：<strong>反向传播</strong><span><span class="heti-spacing"> </span>(backpropogation)</span>。</p>
<details class="info">
<summary>数学基础：偏微分的链式法则</summary>
<ul>
<li>
<p>情况<span><span class="heti-spacing"> </span>1</span>：若<span><span class="heti-spacing"> </span><span class="arithmatex">\(y = g(x), z = h(y)\)</span></span></p>
<div class="arithmatex">\[
\dfrac{dz}{dx} = \dfrac{dz}{dy} \dfrac{dy}{dx}
\]</div>
</li>
<li>
<p>情况<span><span class="heti-spacing"> </span>2</span>：若<span><span class="heti-spacing"> </span><span class="arithmatex">\(x = g(s), y = h(s) z = k(x, y)\)</span></span></p>
<div class="arithmatex">\[
\dfrac{dz}{ds} = \dfrac{\partial z}{\partial x} \dfrac{\partial x}{\partial s} + \dfrac{\partial z}{\partial y} \dfrac{\partial y}{\partial s}
\]</div>
</li>
</ul>
</details>
<p>记第<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(n\)</span><span class="heti-spacing"> </span></span>项数据<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(x^n\)</span><span class="heti-spacing"> </span></span>在神经网络中，训练得到的预测值<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(y^n\)</span><span class="heti-spacing"> </span></span>与实际值<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\hat{y}^n\)</span><span class="heti-spacing"> </span></span>之差为<span><span class="heti-spacing"> </span><span class="arithmatex">\(C^n\)</span></span>，并且记损失<span><span class="heti-spacing"> </span><span class="arithmatex">\(L(\theta) = \sum\limits_{n=1}^n C^n(\theta)\)</span></span>（<span><span class="arithmatex">\(N\)</span><span class="heti-spacing"> </span></span>项数据的误差和<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。因此，对其中的一项参数<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(w\)</span><span class="heti-spacing"> </span></span>求偏微分就是<span><span class="heti-spacing"> </span><span class="arithmatex">\(\dfrac{\partial L(\theta)}{\partial w} = \sum\limits_{n=1}^N \dfrac{\partial C^n(\theta)}{\partial w}\)</span></span>，这样我们将求梯度的问题从对损失<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(L\)</span><span class="heti-spacing"> </span></span>求偏导转换为对每个数据对应的误差<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(C^n\)</span><span class="heti-spacing"> </span></span>求偏导了。</p>
<p>现在考虑如何计算<span><span class="heti-spacing"> </span><span class="arithmatex">\(\dfrac{\partial C}{\partial w}\)</span></span>——根据链式法则，我们可以将其转化为<span><span class="heti-spacing"> </span><span class="arithmatex">\(\dfrac{\partial z}{\partial w} \dfrac{\partial C}{\partial z}\)</span></span>。其中这个<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(z\)</span><span class="heti-spacing"> </span></span>是通过数据输入、权重和偏移计算得到的，如下图所示（简化版本<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>：</p>
<div style="text-align: center">
<img src="images/lec2/27.png" width="70%/"/>
</div>
<p>现在计算就被分为两部分了，这两部分同时也是反向传播的组成部分：</p>
<ul>
<li><strong>前递</strong><span><span class="heti-spacing"> </span>(forward pass)</span>：对所有的参数，计算<span><span class="heti-spacing"> </span><span class="arithmatex">\(\dfrac{\partial z}{\partial w}\)</span></span></li>
<li><strong>后递</strong><span><span class="heti-spacing"> </span>(backward pass)</span>：对所有的激活函数输入值<span><span class="heti-spacing"> </span><span class="arithmatex">\(z\)</span></span>，计算<span><span class="heti-spacing"> </span><span class="arithmatex">\(\dfrac{\partial C}{\partial z}\)</span></span></li>
</ul>
<p>其中前递的计算相当简单：<span><span class="arithmatex">\(\dfrac{\partial z}{\partial w}\)</span><span class="heti-spacing"> </span></span>的值就是与权重相关联的输入数据，因此在训练的过程中，我们顺手就可以将这些值个计算出来。拿上图的例子来说，<span class="arithmatex">\(\dfrac{\partial z}{\partial w_1} = x_1, \dfrac{\partial z}{\partial w_2} = x_2\)</span>。</p>
<details class="example">
<summary>例子</summary>
<p>对于更多次的训练，道理也是一样简单。</p>
<p></p><div style="text-align: center">
<img src="images/lec2/28.png" width="80%/"/>
</div>
</details>
<p>难点在于计算后递，乍看上去好像无从下手。事实上，后递的计算又一次用到了链式法则：<span class="arithmatex">\(\dfrac{\partial C}{\partial z} = \dfrac{\partial a}{\partial z} \dfrac{\partial C}{\partial a}\)</span>，其中<span><span class="heti-spacing"> </span><span class="arithmatex">\(a = \sigma(z)\)</span></span>（<span>sigmoid<span class="heti-spacing"> </span></span>函数<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。</p>
<ul>
<li>
<p><span class="arithmatex">\(\dfrac{\partial a}{\partial z} = \sigma'(z)\)</span>，因为我们直到<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(z\)</span><span class="heti-spacing"> </span></span>的值（前面的计算中已经算过了<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，所以<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\sigma'(z)\)</span><span class="heti-spacing"> </span></span>就是一个常量</p>
<details class="info">
<summary><span>sigmoid<span class="heti-spacing"> </span></span>函数及其导函数的图像</summary>
<p></p><div style="text-align: center">
<img src="images/lec2/29.png" width="50%/"/>
</div>
</details>
</li>
<li>
<p><span class="arithmatex">\(\dfrac{\partial C}{\partial a}\)</span>：又又一次用到链式法则，借助下图来理解这个过程：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/30.png" width="80%/"/>
</div>
<p>可以看到，<span><span class="arithmatex">\(a\)</span><span class="heti-spacing"> </span></span>的值会影响到<span><span class="heti-spacing"> </span><span class="arithmatex">\(z', z''\)</span></span>，因此<span><span class="heti-spacing"> </span><span class="arithmatex">\(\dfrac{\partial C}{\partial a} = \dfrac{\partial z'}{\partial a} \dfrac{\partial C}{\partial z'} + \dfrac{\partial z''}{\partial a} \dfrac{\partial C}{\partial z''}\)</span></span>。其中<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\dfrac{\partial z'}{\partial a}, \dfrac{\partial z''}{\partial a}\)</span><span class="heti-spacing"> </span></span>的值很容易计算，就是对应的权重<span><span class="heti-spacing"> </span><span class="arithmatex">\(w_3, w_4\)</span></span>，而<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\dfrac{\partial C}{\partial z'}, \dfrac{\partial C}{\partial z''}\)</span><span class="heti-spacing"> </span></span>的值我们先暂时假设它们是已知量</p>
</li>
</ul>
<p>总结一下：<span class="arithmatex">\(\dfrac{\partial C}{\partial z} = \sigma'(z) \Big[w_3 \dfrac{\partial C}{\partial z'} + w_4 \dfrac{\partial C}{\partial z''}\Big]\)</span>。最后有待求解的就是刚才假设已知的那两个偏导数，这里我们分为两种情况讨论：</p>
<ol>
<li>
<p>如果它们位于最后一次的训练过程中（即在输出层中<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，那么：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/31.png" width="80%/"/>
</div>
<div class="arithmatex">\[
\dfrac{\partial C}{\partial z'} = \dfrac{\partial y_1}{\partial z'} \dfrac{\partial C}{\partial y_1}
\quad
\dfrac{\partial C}{\partial z''} = \dfrac{\partial y_2}{\partial z''} \dfrac{\partial C}{\partial y_2}
\]</div>
<p>这些量都是相当好计算的，所以结果自然就求出来了</p>
</li>
<li>
<p>如果是在中间的训练过程中，那计算思路如下图所示：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/32.png" width="80%/"/>
</div>
<p>以<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(z'\)</span><span class="heti-spacing"> </span></span>为例稍作解释：要求<span><span class="heti-spacing"> </span><span class="arithmatex">\(\dfrac{\partial C}{\partial z'}\)</span></span>，可以像求<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\dfrac{\partial C}{\partial z}\)</span><span class="heti-spacing"> </span></span>那样继续借助后面的计算值<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(z_a, z_b\)</span><span class="heti-spacing"> </span></span>求解，连计算思路都是一样的。如此往复计算，直到计算到达输出层（即第一种情况时<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，我们可以依次往前计算前面所有的这些偏导数了！</p>
</li>
</ol>
<p>事实上，在计算形如<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\dfrac{\partial C}{\partial z}\)</span><span class="heti-spacing"> </span></span>这类偏导数时，我们可以直接通过输出值<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(y_1, y_2, \dots\)</span><span class="heti-spacing"> </span></span>按反方向计算，这也就是该算法名称的由来。</p>
<div style="text-align: center">
<img src="images/lec2/33.png" width="80%/"/>
</div>
<details class="abstract">
<summary>总结</summary>
<p></p><div style="text-align: center">
<img src="images/lec2/34.png" width="70%/"/>
</div>
</details>
<h3 id="critical-points">Critical Points<a class="headerlink" href="#critical-points" title="Permanent link">⚓︎</a></h3>
<p>在训练模型的时候，我们可能会遇到模型的损失无法进一步减小，也就是无法继续优化的情况。以梯度下降法为例，当<strong>梯度接近<span class="heti-skip"><span class="heti-spacing"> </span>0<span class="heti-spacing"> </span></span></strong>的时候就会出现这种情况，如下图所示：</p>
<div style="text-align: center">
<img src="images/lec2/52.png" width="60%/"/>
</div>
<p>当梯度为<span class="heti-skip"><span class="heti-spacing"> </span>0<span class="heti-spacing"> </span></span>时，除了前面介绍过的<strong>局部最小</strong><span class="heti-skip"><span class="heti-spacing"> </span>(local minima)<span class="heti-spacing"> </span></span>的情况，还有一种情况是位于<strong>鞍点</strong><span><span class="heti-spacing"> </span>(saddle point)</span>：在某些方向上它是局部最小值，但是在另外的方向上它是局部最大值，因而可以逃脱局部最小值的困境。从形状看像一个马鞍，因而得名。我们将这两种情况统称为<strong>关键点</strong><span><span class="heti-spacing"> </span>(critical point)</span>。</p>
<div style="text-align: center">
<img src="images/lec2/54.png" width="70%/"/>
</div>
<p>那么现在的问题是：已知梯度为<span><span class="heti-spacing"> </span>0</span>，我们该如何区分这属于哪一类关键点呢？这里就要用到一些微积分和线性代数的知识了。</p>
<p>首先要利用<strong>泰勒级数</strong><span class="heti-skip"><span class="heti-spacing"> </span>(Taylor Series)<span class="heti-spacing"> </span></span>进行近似表示。假设某个参数下的损失为<span><span class="heti-spacing"> </span><span class="arithmatex">\(L(\bm{\theta})\)</span></span>，并且已知<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{\theta}'\)</span><span class="heti-spacing"> </span></span>附近很小的一个范围内的有一个<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{\theta}\)</span></span>，那么<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(L(\bm{\theta})\)</span><span class="heti-spacing"> </span></span>可以被表示为：</p>
<div class="arithmatex">\[
L(\bm{\theta}) = L(\bm{\theta}') + (\bm{\theta} - \bm{\theta}')^T \bm{g} + \dfrac{1}{2} (\bm{\theta} - \bm{\theta}')^T H (\bm{\theta} - \bm{\theta}')
\]</div>
<p>其中：</p>
<ul>
<li><strong>梯度</strong><span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{g} = \nabla L(\bm{\theta}')\)</span><span class="heti-spacing"> </span></span>是一个向量，向量内元素<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{g}_i = \dfrac{\partial L(\bm{\theta}')}{\partial \bm{\theta}_i}\)</span></span></li>
<li><a href="https://en.wikipedia.org/wiki/Hessian_matrix"><strong>黑塞矩阵</strong></a><span class="heti-skip"><span class="heti-spacing"> </span>(Hessian matrix) <span class="arithmatex">\(H\)</span><span class="heti-spacing"> </span></span>的元素为<span><span class="heti-spacing"> </span><span class="arithmatex">\(H_{ij} = \dfrac{\partial^2}{\partial \bm{\theta}_i \partial \bm{\theta}_j} L(\bm{\theta}')\)</span></span></li>
</ul>
<div style="text-align: center">
<img src="images/lec2/55.png" width="40%/"/>
</div>
<p>如果遇到关键点的话，<span class="arithmatex">\(\bm{g} = 0\)</span>，那么中间项就可以消掉，只剩下：</p>
<div class="arithmatex">\[
L(\bm{\theta}) = L(\bm{\theta}') + \dfrac{1}{2} (\bm{\theta} - \bm{\theta}')^T H (\bm{\theta} - \bm{\theta}')
\]</div>
<p>我们正是通过最后一项来判断关键点的类型。为了便于后续讲述，先令<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{v}^T H \bm{v} = (\bm{\theta} - \bm{\theta}')^T H (\bm{\theta} - \bm{\theta}')\)</span></span>。对于所有的<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{v}\)</span></span>，如果：</p>
<ul>
<li><span class="arithmatex">\(\bm{v}^T H \bm{v} &gt; 0\)</span>，那么对于在<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{\theta}'\)</span><span class="heti-spacing"> </span></span>附近的<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{\theta}\)</span></span>，满足<span><span class="heti-spacing"> </span><span class="arithmatex">\(L(\bm{\theta}) &gt; L(\bm{\theta}')\)</span></span>，因此这是一个<strong>局部最小点</strong><ul>
<li>等价表述：当<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(H\)</span><span class="heti-spacing"> </span></span>为<strong>正定</strong><span class="heti-skip"><span class="heti-spacing"> </span>(positive definite)<span class="heti-spacing"> </span></span>矩阵，即所有特征值<span class="heti-skip"><span class="heti-spacing"> </span>(eigen values)<span class="heti-spacing"> </span></span>均为正数时，表明到达了局部最小点</li>
</ul>
</li>
<li><span class="arithmatex">\(\bm{v}^T H \bm{v} &lt; 0\)</span>，那么对于在<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{\theta}'\)</span><span class="heti-spacing"> </span></span>附近的<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{\theta}\)</span></span>，满足<span><span class="heti-spacing"> </span><span class="arithmatex">\(L(\bm{\theta}) &lt; L(\bm{\theta}')\)</span></span>，因此这是一个<strong>局部最大点</strong><ul>
<li>等价表述：当<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(H\)</span><span class="heti-spacing"> </span></span>为<strong>负定</strong><span class="heti-skip"><span class="heti-spacing"> </span>(negative definite)<span class="heti-spacing"> </span></span>矩阵，即所有特征值均为负数时，表明到达了局部最大点</li>
</ul>
</li>
<li>有时<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{v}^T H \bm{v} &gt; 0\)</span></span>，而有时<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{v}^T H \bm{v} &lt; 0\)</span></span>，那么这是一个<strong>鞍点</strong><ul>
<li>等价表述：当<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(H\)</span><span class="heti-spacing"> </span></span>的特征值有正有负时，表明到达了鞍点</li>
</ul>
</li>
</ul>
<div style="text-align: center">
<img src="images/lec2/56.png" width="80%/"/>
</div>
<details class="example">
<summary>例子</summary>
<p>假设有一个很烂的模型<span><span class="heti-spacing"> </span><span class="arithmatex">\(y = w_1 w_2 x\)</span></span>，且已知训练数据<span><span class="heti-spacing"> </span><span class="arithmatex">\((x, \hat{y}) = (1, 1)\)</span></span>（仅<span class="heti-skip"><span class="heti-spacing"> </span>1<span class="heti-spacing"> </span></span>组<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。那么该模型的误差曲面<span class="heti-skip"><span class="heti-spacing"> </span>(error surface)<span class="heti-spacing"> </span></span>如下所示（图上已标出局部最小点和鞍点<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/57.png" width="60%/"/>
</div>
<p>用<span class="heti-skip"><span class="heti-spacing"> </span>MSE<span class="heti-spacing"> </span></span>计算损失为：<span class="arithmatex">\(L = (\hat{y} - w_1 w_2 x)^2 = (1 - w_1 w_2)^2\)</span>。然后计算每个参数关于损失的一阶和二阶微分，分别作为梯度和黑塞矩阵的元素：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/58.png" width="70%/"/>
</div>
<p>当<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(w_1 = w_2 = 0\)</span><span class="heti-spacing"> </span></span>时，发现梯度为<span><span class="heti-spacing"> </span>0</span>，那么该点就是关键点。而且此时黑塞矩阵<span><span class="heti-spacing"> </span><span class="arithmatex">\(H = \begin{bmatrix} 0 &amp; -2 \\ -2 &amp; 0\end{bmatrix}\)</span></span>，特征值<span><span class="heti-spacing"> </span><span class="arithmatex">\(\lambda_1 = 2, \lambda_2 = -2\)</span></span>，因此可以判定该点为<strong>鞍点</strong>。</p>
</details>
<p>对于鞍点而言，黑塞矩阵除了能够用于判断外，还能为我们指示离开鞍点，即更新参数的方向。现在假设<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{u}\)</span><span class="heti-spacing"> </span></span>是<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(H\)</span><span class="heti-spacing"> </span></span>的一个特征向量，<span><span class="arithmatex">\(\lambda\)</span><span class="heti-spacing"> </span></span>是<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(u\)</span><span class="heti-spacing"> </span></span>的特征值，那么：</p>
<div class="arithmatex">\[
\bm{u}^T H \bm{u} = \bm{u}^T (\lambda \bm{u}) = \lambda \|\bm{u}\|^2
\]</div>
<p>因此，如果<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\lambda &lt; 0\)</span><span class="heti-spacing"> </span></span>的话，那么<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{u}^T H \bm{u} &lt; 0\)</span></span>。回到前面关于损失的泰勒展开式<span><span class="heti-spacing"> </span><span class="arithmatex">\(L(\bm{\theta}) = L(\bm{\theta}') + \dfrac{1}{2} (\bm{\theta} - \bm{\theta}')^T H (\bm{\theta} - \bm{\theta}')\)</span></span>，令<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{u} = \bm{\theta} - \bm{\theta}'\)</span></span>，那么<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{\theta} = \bm{\theta}' + \bm{u}\)</span></span>。由于此时<span><span class="heti-spacing"> </span><span class="arithmatex">\(L(\bm{\theta}) &lt; L(\bm{\theta}')\)</span></span>，即<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{\theta}'\)</span><span class="heti-spacing"> </span></span>位于局部最大点，那么从该点沿<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{u}\)</span><span class="heti-spacing"> </span></span>的方向走的话，损失就会变小，从而离开局部最大点。总之，通过黑塞矩阵的特征向量，我们总能设法离开鞍点。</p>
<details class="example">
<summary>例子</summary>
<p>接着前面的例子，第二个特征值为<span><span class="heti-spacing"> </span><span class="arithmatex">\(\lambda_2 = -2\)</span></span>，对应特征向量<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{u} = \begin{bmatrix}1 \\ 1\end{bmatrix}\)</span></span>。只要沿着<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(u\)</span><span class="heti-spacing"> </span></span>的方向更新参数，损失就会减小，如下面的误差曲面所示：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/59.png" width="50%/"/>
</div>
</details>
<p>实践上，一般并不会实现这种方法，因为计算二次微分、特征值和特征向量等等运算太过复杂，效率不高。</p>
<hr/>
<p>那么鞍点和局部最小点这两种关键点，哪个更为常见呢？</p>
<div style="text-align: center">
<img src="images/lec2/60.png" width="80%/"/>
</div>
<p>可以看到，二维平面上的局部最小点，可能到三维乃至更高位的空间内就是一个鞍点，因为更多的维数意味着更多的逃离局部最小的可能。所以当参数足够多的时候，<strong>鞍点</strong>的数量可能相对更多一些。</p>
<p>事实上，上述理论有实践经验的支撑，如下图所示：</p>
<div style="text-align: center">
<img src="images/lec2/61.png" width="80%/"/>
</div>
<p>其中横轴表示<strong>最小值比率</strong><span><span class="heti-spacing"> </span>(minimum ratio)</span>，即局部最小点在所有关键点中的占比，计算公式为：</p>
<div class="arithmatex">\[
\text{Minimum ratio} = \dfrac{\text{Number of Positive Eigen values}}{\text{Number of Eigen values}}
\]</div>
<p>可以看到，当我们通过不断的训练将损失降得尽可能低时，最小值比率维持在<span class="heti-skip"><span class="heti-spacing"> </span>0.5-0.6<span class="heti-spacing"> </span></span>之间，也就是说始终没法达到真正的局部最小值（最小值比率<span><span class="heti-spacing"> </span>= 1</span><heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，因此局部最小点在具体实现中属于罕见的情况。</p>
<h3 id="batches">Batches<a class="headerlink" href="#batches" title="Permanent link">⚓︎</a></h3>
<p>前面提到过，对于参数量很大的模型，会使用分<strong>批</strong><span class="heti-skip"><span class="heti-spacing"> </span>(batch)<span class="heti-spacing"> </span></span>使用梯度下降法来更新参数，而不是一次性对所有参数求梯度。下面就通过对比更新小批量和大批量参数的效果，来说明为何需要分批更新。</p>
<p>假如模型有<span class="heti-skip"><span class="heti-spacing"> </span>N = 20<span class="heti-spacing"> </span></span>的参数，现提供两种（极端的）大小：大批的大小为<span><span class="heti-spacing"> </span>N</span>，小批的大小为<span><span class="heti-spacing"> </span>1</span>。下面通过误差曲面来分析两者的优化效果：</p>
<div style="text-align: center">
<img src="images/lec2/63.png" width="80%/"/>
</div>
<ul>
<li>大批量：更新时间长（要处理<span class="heti-skip"><span class="heti-spacing"> </span>20<span class="heti-spacing"> </span></span>个参数<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，但是优化效果很好（powerful）</li>
<li>小批量：更新时间很短（<span>1<span class="heti-spacing"> </span></span>次只更新<span class="heti-skip"><span class="heti-spacing"> </span>1<span class="heti-spacing"> </span></span>个参数<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，但是有很多噪点（noisy）</li>
</ul>
<p>因此直接看貌似看不出两者有哪个能占据很大优势。其实上面只是做了理论层面的分析，实际上在深度学习中常常用到<span><span class="heti-spacing"> </span>GPU</span>，而<span class="heti-skip"><span class="heti-spacing"> </span>GPU<span class="heti-spacing"> </span></span>的一大优势在于支持<strong>并行计算</strong>，因此我们得重新考虑时间上的估量了：</p>
<ul>
<li>
<p>大批量并不意味着需要花更长的时间计算梯度（除非批量太大）</p>
<p></p><div style="text-align: center">
<img src="images/lec2/64.png" width="70%/"/>
</div>
</li>
<li>
<p>在一个时期<span class="heti-skip"><span class="heti-spacing"> </span>(epoch)<span class="heti-spacing"> </span></span>内，小批量反而需要花费更多的时间</p>
<p></p><div style="text-align: center">
<img src="images/lec2/65.png" width="80%/"/>
</div>
</li>
</ul>
<p>尽管小批量在时间上并不占据太大的优势，但是它的强大之处在于能确保训练模型的精度，使模型具备更好的表现。</p>
<details class="example">
<summary>例子</summary>
<p></p><div style="text-align: center">
<img src="images/lec2/66.png" width="80%/"/>
</div>
</details>
<p>之所以大批量不好，并不是因为模型偏移或过拟合，而是优化上的不足：</p>
<div style="text-align: center">
<img src="images/lec2/67.png" width="70%/"/>
</div>
<ul>
<li>对于大批量，尤其是全批量，未知参数关于损失的函数通常只有少数几条甚至一条，因此每次更新都顺着几乎既定的路径走。如果遇到了关键点，那么就很容易卡在那里不动了</li>
<li>对于小批量，由于每次只更新一小部分，因此会产生不同的损失函数。即使模型在优化的时候卡在某个损失函数上，只需更新一下，就跳到另一个损失函数，从而有很大概率避开了关键点</li>
</ul>
<p>下面从另一个角度解释原因。对于训练数据上的损失函数，不同的局部最小点也有好坏之分，我们会认为狭窄的局部最小点是不利于测试的，而平坦的局部最小点相对而言比较好一些。如下图所示，假如训练数据上和测试数据上的损失函数有些错误匹配<span><span class="heti-spacing"> </span>(mismatch)</span>，那么局部最小点的好坏之分就表现得非常明显了：平坦的局部最小点在测试数据上没多大变动，而狭窄的局部最小点在测试数据上的损失就一下子变得很高。对于小批量而言，由于它的更新策略更具灵活性（noisy<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，因此它很容易跳脱这种狭窄的局部最小点；而对于大批量而言，它的行为有些“循规蹈矩”，因此很容易陷入狭窄的局部最小点而无法逃脱。所以通过小批量优化得到的模型表现会更好。</p>
<div style="text-align: center">
<img src="images/lec2/68.png" width="70%/"/>
</div>
<div class="admonition abstract">
<p class="admonition-title">总结</p>
<p></p><div style="text-align: center">
<img src="images/lec2/69.png" width="80%/"/>
</div>
<p>批量大小是一个<strong>超参数</strong><span><span class="heti-spacing"> </span>(hyperparameter)</span>，我们可以根据需要进行手工调整。</p>
</div>
<h3 id="momentum">Momentum<a class="headerlink" href="#momentum" title="Permanent link">⚓︎</a></h3>
<p>在梯度下降法中，当梯度接近于<span class="heti-skip"><span class="heti-spacing"> </span>0<span class="heti-spacing"> </span></span>的时候，就会停在这些点上了。但是在物理世界中，即使某段路相当平坦（鞍点<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，甚至是一个小坑（局部最小点<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，由于物体具有惯性，因此还会继续移动，不会马上停下来。这给我们改进梯度下降法提供了一个不错的想法——引入<strong>动量</strong><span class="heti-skip"><span class="heti-spacing"> </span>(momentum)<span class="heti-spacing"> </span></span>的概念，让之前的移动过程影响当下的移动过程，有助于摆脱局部最小的困境。</p>
<details class="tip">
<summary>对比</summary>
<p></p><div style="text-align: center">
<img src="images/lec2/62.png" width="80%/"/>
</div>
<p></p><div style="text-align: center">
<img src="images/lec2/70.png" width="80%/"/>
</div>
</details>
<p>在正式介绍该方法前，先来总结一下一般的梯度下降法，如下图所示。由于比较直观，这里就不再赘述了。</p>
<div style="text-align: center">
<img src="images/lec2/71.png" width="80%/"/>
</div>
<p>现在引入动量，那么每步的移动就是上一步的移动与当前梯度的差，具体来说：</p>
<ul>
<li>令初始点为<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{\theta}^0\)</span></span>，移动量<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{m}^0 = 0\)</span></span>，梯度<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{g}^0\)</span></span></li>
<li>第一步移动<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{m}^1 = \lambda \bm{m}^0 - \eta \bm{g}^0\)</span></span>，下一个目标点<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{\theta}^1 = \bm{\theta}^0 + \bm{m}^1\)</span></span>，然后计算梯度<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{g}^1\)</span></span></li>
<li>第二步移动<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{m}^2 = \lambda \bm{m}^1 - \eta \bm{g}^1\)</span></span>，下一个目标点<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{\theta}^2 = \bm{\theta}^1 + \bm{m}^2\)</span></span>，然后计算梯度<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{g}^2\)</span></span></li>
<li>以此类推<span><span class="heti-spacing"> </span>...</span></li>
</ul>
<p>上述过程如下所示：</p>
<div style="text-align: center">
<img src="images/lec2/72.png" width="50%/"/>
</div>
<p>观察发现，第<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(i\)</span><span class="heti-spacing"> </span></span>次移动量为前几次梯度<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{g}^0, \bm{g}^1, \dots, \bm{g}^{i-1}\)</span><span class="heti-spacing"> </span></span>的加权和，比如：</p>
<div class="arithmatex">\[
\begin{align}
\bm{m}^0 &amp; = \bm{0} \notag \\
\bm{m}^1 &amp; = - \eta \bm{g}^0 \notag \\
\bm{m}^2 &amp; = -\lambda \eta \bm{g}^0 - \eta \bm{g}^1 \notag
\end{align}
\]</div>
<p>有了动量后，对于本节最开始提到的例子，现在的优化过程就可能像下图一样，不会被某些关键点给困住。</p>
<div style="text-align: center">
<img src="images/lec2/73.png" width="80%/"/>
</div>
<h3 id="adaptive-learning-rate">Adaptive Learning Rate<a class="headerlink" href="#adaptive-learning-rate" title="Permanent link">⚓︎</a></h3>
<p>不少人会认为在训练过程中，如果模型无法得到进一步的优化（假定使用梯度下降法优化<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，那么应该是参数到达关键点附近的位置。然而，实际上并不一定是这种情况<span><span class="heti-spacing"> </span>...</span></p>
<div style="text-align: center">
<img src="images/lec2/1.png" width="80%/"/>
</div>
<p>如上图所示，虽然损失值随着迭代次数的增加而逐渐减小，最终趋于稳定，但是梯度的模值仍然不断跳动，这就说明参数并不一定抵达关键点，而有可能出现了像左图所示的情况：由于每次迭代更新的步幅（学习速率）过大，导致参数点在“山坡”之间来回跳动，但此时的梯度值并不接近于<span><span class="heti-spacing"> </span>0</span>。接下来再看一个例子：</p>
<details class="example" open="open">
<summary>例子</summary>
<p>假如有一张凸<span class="heti-skip"><span class="heti-spacing"> </span>(convex)<span class="heti-spacing"> </span></span>的误差曲面，如下所示：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/2.png" width="50%/"/>
</div>
<p>下面是不同学习速率下模型的优化过程：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/3.png" width="90%/"/>
</div>
<ul>
<li>如果学习速率过大，每次迭代更新的步幅就会很大，有些“刹不住车”，难以减小损失</li>
<li>如果学习速率过小，每次迭代更新的步幅就会很小，因此当参数点到达了平缓的（梯度较小）区域时，更新速度就会变得很慢很慢，效率就太低了</li>
</ul>
</details>
<p>综上，如果学习速率保持不变，就很难达到理想的优化效果。因此，不同的参数应当有不同的学习速率，以适应具体情况：</p>
<div style="text-align: center">
<img src="images/lec2/4.png" width="60%/"/>
</div>
<ul>
<li>当参数在陡峭的误差曲面上的时候，学习速率应该变得更小</li>
<li>当参数在平坦的误差曲面上的时候，学习速率应该变得更大</li>
</ul>
<div class="admonition info">
<p class="admonition-title">注</p>
<p>方便起见，在本节之后的分析中我们仅考虑参数集<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{\theta}\)</span><span class="heti-spacing"> </span></span>的其中一个参数<span><span class="heti-spacing"> </span><span class="arithmatex">\(\theta_i\)</span></span>。</p>
</div>
<h4 id="adagrad">Adagrad<a class="headerlink" href="#adagrad" title="Permanent link">⚓︎</a></h4>
<p>在原来的第<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(t\)</span><span class="heti-spacing"> </span></span>次迭代优化中，参数更新的式子为：<span class="arithmatex">\(\theta_i^{t+1} = \theta_i^t - \eta \bm{g}_i^t\)</span>，其中<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{g}_i^t = \dfrac{\partial L}{\partial \bm{\theta}_i} \Big|_{\bm{\theta} = \bm{\theta}^t}\)</span></span>，此时的学习速率是固定的<span><span class="heti-spacing"> </span><span class="arithmatex">\(\eta\)</span></span>。为了让学习速率与参数有关，我们将迭代更新的式子转化为：</p>
<div class="arithmatex">\[
\theta_i^{t+1} = \theta_i^t - \dfrac{\eta}{\sigma_i^t} \bm{g}_i^t
\]</div>
<p>其中<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\sigma_i^t\)</span><span class="heti-spacing"> </span></span>的值为前<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(t\)</span><span class="heti-spacing"> </span></span>次（<span class="arithmatex">\(0 \sim t - 1\)</span>）迭代的梯度的<strong>均方根</strong><span><span class="heti-spacing"> </span>(root mean square)</span>，即：</p>
<div class="arithmatex">\[
\sigma_i^t = \sqrt{\dfrac{1}{t+1}\sum\limits_{i=0}^t (\bm{g}_i^t)^2}
\]</div>
<p>实际上，这种优化方法称为<strong><span><span class="heti-spacing"> </span>Adagrad</span></strong>。这样，对于不同的参数，便可以做到：</p>
<div style="text-align: center">
<img src="images/lec2/5.png" width="40%/"/>
</div>
<h4 id="rmsprop">RMSProp<a class="headerlink" href="#rmsprop" title="Permanent link">⚓︎</a></h4>
<p>然而，上述方法仍然有缺陷。对于更为复杂的误差曲面，即使是同一个参数，沿同一个方向（梯度）变化时，所经过的误差曲面有平坦的，也有陡峭的。上述方法在这种情况下仍然保持相同的学习速率，而这是不符合我们预期的。</p>
<div style="text-align: center">
<img src="images/lec2/6.png" width="70%/"/>
</div>
<p>所以，接下来引入一种更好的优化方法，称为<strong><span><span class="heti-spacing"> </span>RMSProp</span></strong>。它的迭代更新公式与<span class="heti-skip"><span class="heti-spacing"> </span>Adagrad<span class="heti-spacing"> </span></span>类似，区别在于<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\sigma_i^t\)</span><span class="heti-spacing"> </span></span>的公式发生了变化：</p>
<div class="arithmatex">\[
\sigma_i^t = \sqrt{\alpha (\sigma_i^{t-1})^2 + (1 - \alpha)(\bm{g}_i^t)^2} 
\]</div>
<p>此时<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\sigma_i^t\)</span><span class="heti-spacing"> </span></span>的值不仅和当前的梯度有关，还受之前迭代的梯度的影响（可通过展开<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\sigma_i^t\)</span><span class="heti-spacing"> </span></span>发现<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。此外，我们可以通过超参数<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\alpha \in (0, 1)\)</span><span class="heti-spacing"> </span></span>来调整<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\sigma_i^t\)</span><span class="heti-spacing"> </span></span>中<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{g}_i^t\)</span><span class="heti-spacing"> </span></span>以及<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\sigma_i^{t-1}\)</span><span class="heti-spacing"> </span></span>的比例。如果<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\alpha\)</span><span class="heti-spacing"> </span></span>较小，那么当前梯度对<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\sigma_i^t\)</span><span class="heti-spacing"> </span></span>的影响更大，而之前的梯度影响更小。下图展现了使用<span class="heti-skip"><span class="heti-spacing"> </span>RMSProp<span class="heti-spacing"> </span></span>优化的大致效果：</p>
<div style="text-align: center">
<img src="images/lec2/7.png" width="70%/"/>
</div>
<p>将前面介绍过的动量方法与<span class="heti-skip"><span class="heti-spacing"> </span>RMSProp<span class="heti-spacing"> </span></span>结合起来，我们便可以得到一种很常用的优化方法，称为<strong><span><span class="heti-spacing"> </span>Adam</span></strong>，其算法实现如下所示：</p>
<div style="text-align: center">
<img src="images/lec2/8.png" width="90%/"/>
</div>
<h4 id="learning-rate-scheduling">Learning Rate Scheduling<a class="headerlink" href="#learning-rate-scheduling" title="Permanent link">⚓︎</a></h4>
<p>回到前面提到过的凸误差曲面的例子，如果采用<span class="heti-skip"><span class="heti-spacing"> </span>Adagrad<span class="heti-spacing"> </span></span>方法的话，结果为：</p>
<div style="text-align: center">
<img src="images/lec2/9.png" width="50%/"/>
</div>
<p>可以看到，使用这种方法可以使模型获得最优的参数，但是在优化过程中会出现红圈所示的一些“爆炸”的情况。这是因为刚开始的时候沿竖直方向的梯度很大，但后来梯度变得很小（来到了平坦的区域<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>；而<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\sigma_i^t\)</span><span class="heti-spacing"> </span></span>会计算之前迭代过程中计算的所有梯度值，所以在平坦区域时，<span><span class="arithmatex">\(\sigma_i^t\)</span><span class="heti-spacing"> </span></span>会累积一些沿竖直方向很小的梯度，那么有的时候学习速率就会变得很大，因此就会一下子“飞出去”，偏离预期更新的路径。</p>
<p>但是该优化方法有办法修正这一偏离，因为当偏出去后，梯度就会变大，那么学习速率就会减小（好比有股“摩擦力”<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，于是就又慢慢回到正轨了。</p>
<p>这样的优化并不完美，还有改进的空间，使其不会出现这么多的偏离现象，具体做法是使用一种叫做<strong>学习速率调度</strong><span class="heti-skip"><span class="heti-spacing"> </span>(learning rate scheduling)<span class="heti-spacing"> </span></span>的方法，让学习速率不仅随梯度（空间）变化，还随<strong>迭代次数</strong>（时间）的变化而变化（下面记第<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(t\)</span><span class="heti-spacing"> </span></span>次迭代的学习速率为<span><span class="heti-spacing"> </span><span class="arithmatex">\(\eta^t\)</span></span><heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。这里介绍两种不同的方法：</p>
<ul>
<li>
<p><strong>学习速率衰减</strong><span><span class="heti-spacing"> </span>(learning rate decay)</span>：由于随着训练的深入，模型逐渐靠近最合适的参数目标，因此学习速率会随迭代次数的增加而下降，如下图所示：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/10.png" width="30%/"/>
</div>
<p>在上面的凸误差曲面的例子中，采用该方法后的优化过程为：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/11.png" width="50%/"/>
</div>
<p>现在的优化过程变得非常“丝滑”了<span><span class="heti-spacing"> </span>~</span></p>
</li>
<li>
<p><strong>热身</strong><span><span class="heti-spacing"> </span>(warm up)</span>：随着迭代次数的增加，学习速率先增后减</p>
<p></p><div style="text-align: center">
<img src="images/lec2/12.png" width="30%/"/>
</div>
<p>之所以这样设定，是因为在训练刚开始的时候，还不清楚具体的误差曲面应该长什么样子，此时得到的<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\sigma_i^t\)</span><span class="heti-spacing"> </span></span>就有可能偏离过大，因而需要更小的学习速率以降低影响。而随着训练的进行，会逐渐摸清楚误差曲面大致的样子，因此学习速率会逐渐上升。</p>
<ul>
<li>残差神经网络和<span class="heti-skip"><span class="heti-spacing"> </span>Transformer<span class="heti-spacing"> </span></span>都提到了这种方法</li>
<li><span>RAdam -&gt; Adam +<span class="heti-spacing"> </span></span>热身</li>
</ul>
</li>
</ul>
<div class="admonition abstract">
<p class="admonition-title">总结</p>
<p>结合前面提到的各种改进优化的技巧，可以得到以下式子：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/13.png" width="70%/"/>
</div>
<p><span><span class="arithmatex">\(\sigma_i^t\)</span><span class="heti-spacing"> </span></span>和<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{m}_i^t\)</span><span class="heti-spacing"> </span></span>都与前面的梯度有关，但是一个在分母，一个在分子上。然而这并不意味着两者的效果会相互抵消，因为前者是标量，只影响学习速率的大小；而后者是向量，需要考虑方向，因此两者无法完全抵消。</p>
</div>
<h2 id="classfication">Classfication<a class="headerlink" href="#classfication" title="Permanent link">⚓︎</a></h2>
<p>前面介绍的深度学习研究的都是<strong>回归</strong><span class="heti-skip"><span class="heti-spacing"> </span>(regression)<span class="heti-spacing"> </span></span>问题，即模型根据输入数据集，输出单个标量<span><span class="heti-spacing"> </span><span class="arithmatex">\(y\)</span></span>，并于真实值（标签）<span><span class="arithmatex">\(\hat{y}\)</span><span class="heti-spacing"> </span></span>比较。现在我们来探讨<strong>分类</strong><span class="heti-skip"><span class="heti-spacing"> </span>(classification)<span class="heti-spacing"> </span></span>问题，它与回归的不同在于：模型会从多个选项（称为类别<span><span class="heti-spacing"> </span>(classes)</span>）中挑选一个作为输出。</p>
<p>那我们是否可以将分类问题转化为回归问题呢？因为输出的都是单个量，那么也许可以尝试用单个标量表示挑选出来的类别；并且规定每个选项都用一个数值表示，便于将输出<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(y\)</span><span class="heti-spacing"> </span></span>与标签<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\hat{y}\)</span><span class="heti-spacing"> </span></span>比较。然而，这样会带来一个明显的问题：类别之间的关系可不像数值之间的大小这么直接，也许类别之间的关系很复杂，也许压根没有关系，那么<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\hat{y} - y\)</span><span class="heti-spacing"> </span></span>也就无法准确表示输出类别与真实类别之间的关系，所以这不是一个好方法。</p>
<p>实际上，我们采用<strong>独热向量</strong><span class="heti-skip"><span class="heti-spacing"> </span>(one-hot vector)<span class="heti-spacing"> </span></span>来表示每个类别。独热向量的特点是：向量里面的元素只有<span class="heti-skip"><span class="heti-spacing"> </span>0<span class="heti-spacing"> </span></span>和<span><span class="heti-spacing"> </span>1</span>，而且只有其中一位是<span><span class="heti-spacing"> </span>1</span>，其余位均为<span><span class="heti-spacing"> </span>0</span>。对于任意两个不同类别的独热向量，它们值为<span class="heti-skip"><span class="heti-spacing"> </span>1<span class="heti-spacing"> </span></span>的元素是不同的。这种方法的好处在于：任意不同两个类别之间的区别是一致的，因为它们对应的独热向量之间的距离是一样的，这样便于输出类别与真实类别的比较。</p>
<div style="text-align: center">
<img src="images/lec2/14.png" width="40%/"/>
</div>
<p>因为向量里面包含多个标量，那么模型就要从输出单个值转变为输出多个值，因此需要稍微改变一下模型的样子，如下所示：</p>
<div style="text-align: center">
<img src="images/lec2/15.png" width="80%/"/>
</div>
<p>重点关注输出部分的改变。以上图为例，现在的输出由<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(y_1, y_2, y_3\)</span><span class="heti-spacing"> </span></span>三部分构成，每个输出都有各自不同的权重和偏移，但接收的都是来自激活函数的输出。</p>
<p>现在来比对一下回归和分类两类问题对应的模型（方程<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>：</p>
<div style="text-align: center">
<img src="images/lec2/16.png" width="70%/"/>
</div>
<p>相比回归模型，分类模型有一些小变化：</p>
<ul>
<li>由于输出量从标量变为向量，因此对应地，权重从向量<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{c}^T\)</span><span class="heti-spacing"> </span></span>变为矩阵<span><span class="heti-spacing"> </span><span class="arithmatex">\(W'\)</span></span>，偏移从标量<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(b\)</span><span class="heti-spacing"> </span></span>变成向量<span><span class="heti-spacing"> </span><span class="arithmatex">\(\bm{b}'\)</span></span></li>
<li>最后，还要将模型的输出值放到<strong><span class="heti-skip"><span class="heti-spacing"> </span>softmax<span class="heti-spacing"> </span></span>函数</strong>做进一步的转化。这是因为通过模型得到的输出量可以是任意值，而我们要求的输出量中每个元素只能是<span class="heti-skip"><span class="heti-spacing"> </span>0<span class="heti-spacing"> </span></span>和<span><span class="heti-spacing"> </span>1</span>。而<span class="heti-skip"><span class="heti-spacing"> </span>softmax<span class="heti-spacing"> </span></span>就帮我们做了这步转化，它能将任何值映射到<span class="heti-skip"><span class="heti-spacing"> </span>0<span class="heti-spacing"> </span></span>或<span class="heti-skip"><span class="heti-spacing"> </span>1<span class="heti-spacing"> </span></span>上</li>
</ul>
<h3 id="softmax">Softmax<a class="headerlink" href="#softmax" title="Permanent link">⚓︎</a></h3>
<p>下面大致介绍一下<span class="heti-skip"><span class="heti-spacing"> </span>softmax<span class="heti-spacing"> </span></span>函数的原理。假设原始的输出向量中包含<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(y_1, y_2, y_3\)</span><span class="heti-spacing"> </span></span>三个元素，它们在<span class="heti-skip"><span class="heti-spacing"> </span>softmax<span class="heti-spacing"> </span></span>函数的输出分别为<span><span class="heti-spacing"> </span><span class="arithmatex">\(y_1', y_2', y_3'\)</span></span>，那么<span class="heti-skip"><span class="heti-spacing"> </span>softmax<span class="heti-spacing"> </span></span>的计算公式为：</p>
<div class="arithmatex">\[
y_i' = \dfrac{exp(y_i)}{\sum_j exp(y_i)}
\]</div>
<p>下图形象展示了这一过程：</p>
<div style="text-align: center">
<img src="images/lec2/17.png" width="70%/"/>
</div>
<p><span><span class="arithmatex">\(y_i'\)</span><span class="heti-spacing"> </span></span>还有以下性质：</p>
<ul>
<li><span class="arithmatex">\(y_i' \in (0, 1)\)</span></li>
<li><span class="arithmatex">\(\sum_i y_i' = 1\)</span></li>
<li>对于原始输出向量中的两个元素，它们之间的差距大小也能在对应的<span class="heti-skip"><span class="heti-spacing"> </span>softmax<span class="heti-spacing"> </span></span>结果中体现出来</li>
</ul>
<details class="example">
<summary>例子</summary>
<p></p><div style="text-align: center">
<img src="images/lec2/18.png" width="70%/"/>
</div>
</details>
<p>实际上，如果只有两个类别的话，<span>sigmoid<span class="heti-spacing"> </span></span>函数与<span class="heti-skip"><span class="heti-spacing"> </span>softmax<span class="heti-spacing"> </span></span>函数是等价的，但在这种情况下<span class="heti-skip"><span class="heti-spacing"> </span>sigmoid<span class="heti-spacing"> </span></span>函数会用的更多一些（虽然两者并没有孰优孰劣<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。</p>
<h3 id="loss">Loss<a class="headerlink" href="#loss" title="Permanent link">⚓︎</a></h3>
<div style="text-align: center">
<img src="images/lec2/19.png" width="80%/"/>
</div>
<p>完善好我们的分类模型的输出后，现在来考虑如何计算损失。</p>
<ul>
<li>损失<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(L\)</span><span class="heti-spacing"> </span></span>还是可以用误差的均值计算</li>
<li>
<p>误差<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(e\)</span><span class="heti-spacing"> </span></span>也可以继续沿用回归模型中的<span><span class="heti-spacing"> </span>MSE</span>，即<span><span class="heti-spacing"> </span><span class="arithmatex">\(e = \sum\limits_i (\hat{\bm{y}}_i - \bm{y}_i')^2\)</span></span>。然而，在分类模型中，更好的误差计算方法是<strong>交叉熵</strong><span><span class="heti-spacing"> </span>(cross-entropy)</span>，即<span><span class="heti-spacing"> </span><span class="arithmatex">\(e = -\sum\limits_i \hat{\bm{y}}_i \ln \bm{y}_i'\)</span></span>。虽然两者都能正确反映误差的大小，但是交叉熵更适用于分类模型的优化过程。下面通过一个例子来解释原因（严谨的数学推导过程见<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2015_2/Lecture/Deep%20More%20(v2).ecm.mp4/index.html">过去的视频</a><heti-adjacent class="heti-adjacent-half">）</heti-adjacent>：</p>
<details class="example">
<summary>例子</summary>
<p>给定以下分类模型：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/20.png" width="80%/"/>
</div>
<ul>
<li>方便起见，规定输出量的范围为：<span class="arithmatex">\(y_1, y_2 \in [-10, 10]\)</span>，<span class="arithmatex">\(y_3 = -1000\)</span>（很小的常量，通过<span class="heti-skip"><span class="heti-spacing"> </span>softmax<span class="heti-spacing"> </span></span>后就接近于<span><span class="heti-spacing"> </span>0</span>）</li>
<li>标签为<span><span class="heti-spacing"> </span><span class="arithmatex">\(\begin{bmatrix}1 \\ 0 \\ 0\end{bmatrix}\)</span></span></li>
</ul>
<p>现在来比较通过<span class="heti-skip"><span class="heti-spacing"> </span>MSE<span class="heti-spacing"> </span></span>和交叉熵得到的误差曲面：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/21.png" width="90%/"/>
</div>
<p>可以看到：</p>
<ul>
<li>当<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(y_1\)</span><span class="heti-spacing"> </span></span>很大，<span><span class="arithmatex">\(y_2\)</span><span class="heti-spacing"> </span></span>很小的时候，两者得到的损失都很小；当<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(y_1\)</span><span class="heti-spacing"> </span></span>很小，<span><span class="arithmatex">\(y_2\)</span><span class="heti-spacing"> </span></span>很大的时候，两者得到的损失都很大</li>
<li>然而，如果初始参数位于图上蓝点所示位置，并且使用一般的梯度下降法（即学习速率不变）的话，由于<span class="heti-skip"><span class="heti-spacing"> </span>MSE<span class="heti-spacing"> </span></span>的误差曲面过于平坦，即梯度很小，因此就会卡在原地不走了；而交叉熵的误差曲面较为陡峭，即有一定的梯度，因此可以继续走下去</li>
</ul>
<p>所以在分类模型中，交叉熵的计算方法更适用于优化过程。</p>
</details>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">注</p>
<ul>
<li>最小化交叉熵<span class="heti-skip"><span class="heti-spacing"> </span>==<span class="heti-spacing"> </span></span>最大化可能性<span><span class="heti-spacing"> </span>(likelihood)</span></li>
<li>改变损失函数也许就能降低优化的难度</li>
</ul>
</div>
<h3 id="example-pokémondigimon-classifier">Example: Pokémon/Digimon Classifier<a class="headerlink" href="#example-pokémondigimon-classifier" title="Permanent link">⚓︎</a></h3>
<p>下面将以宝可梦<span class="heti-skip"><span class="heti-spacing"> </span>/<span class="heti-spacing"> </span></span>数码宝贝分类器为例，加深对机器学习原理的认识，同时也会学习到<strong>模型复杂度</strong><span class="heti-skip"><span class="heti-spacing"> </span>(model complexity)<span class="heti-spacing"> </span></span>对训练的影响。</p>
<h4 id="function-with-unknown-parameters">Function with Unknown Parameters<a class="headerlink" href="#function-with-unknown-parameters" title="Permanent link">⚓︎</a></h4>
<p>回顾一下训练的过程——第一步是<u>基于我们的领域知识，去找一个带有未知参数的函数</u>。这个函数大致长这样：</p>
<div style="text-align: center">
<img src="images/lec2/74.png" width="50%/"/>
</div>
<p>现在从宝可梦和数码宝贝中进行随机采样，观察它们的特征：</p>
<div style="text-align: center">
<img src="images/lec2/75.png" width="70%/"/>
</div>
<p>比对两者的画风，不难发现数码宝贝的线条较为复杂，而宝可梦的线条更为简单。为了验证这一发现的正确性，可以用一些图像处理工具来提取这些图片的线条：</p>
<div style="text-align: center">
<img src="images/lec2/76.png" width="70%/"/>
</div>
<p>使用边缘检测工具后，可以得到中间的图片，其中白色的像素表示精灵的线条。很明显，上面的宝可梦的线条比下面的数码宝贝的线条少得多。根据这一发现，我们可以确定大致的函数形式为：</p>
<div style="text-align: center">
<img src="images/lec2/77.png" width="70%/"/>
</div>
<ul>
<li>函数用<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(f_h\)</span><span class="heti-spacing"> </span></span>表示，其中<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(h\)</span><span class="heti-spacing"> </span></span>是一个阈值，作为划分宝可梦和数码宝贝的依据：当超过该阈值时，说明该图像表示的是数码宝贝，否则的话就是宝可梦<ul>
<li>并且假定阈值的取值范围为<span><span class="heti-spacing"> </span><span class="arithmatex">\(H = \{1, 2, \dots, 10000\}\)</span></span>（图片最多有<span class="heti-skip"><span class="heti-spacing"> </span>10000<span class="heti-spacing"> </span></span>个像素点<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，因此<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(|H|\)</span><span class="heti-spacing"> </span></span>可以表示候选函数的个数，即模型的“复杂度”(model "complexity")</li>
</ul>
</li>
<li>另外用函数<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(e\)</span><span class="heti-spacing"> </span></span>表示边缘检测工具计算得到的线条像素数</li>
</ul>
<h4 id="loss-of-a-function">Loss of a Function<a class="headerlink" href="#loss-of-a-function" title="Permanent link">⚓︎</a></h4>
<p>接下来进入训练的第二步：<u>对于给定的数据，定义一个损失函数</u>。</p>
<ul>
<li>假如给定一个数据集<span><span class="heti-spacing"> </span><span class="arithmatex">\(D = \{(x^1, \hat{y}^1), (x^2, \hat{y}^2), \dots, (x^N, \hat{y}^N)\}\)</span></span>，其中<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(x^n\)</span><span class="heti-spacing"> </span></span>表示输入的图片，<span><span class="arithmatex">\(y^n\)</span><span class="heti-spacing"> </span></span>表示输出的宝可梦<span class="heti-skip"><span class="heti-spacing"> </span>/<span class="heti-spacing"> </span></span>数码宝贝的选项</li>
<li>
<p>此时损失函数是一个关于阈值<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(h\)</span><span class="heti-spacing"> </span></span>和数据集<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(D\)</span><span class="heti-spacing"> </span></span>的函数，即：</p>
<div class="arithmatex">\[
L(h, D) = \dfrac{1}{N} \sum\limits_{n=1}^N l(h, x^n, \hat{y}^n)
\]</div>
<ul>
<li>每个数据的损失（或者说误差）可以用一个简单的函数计算：<span class="arithmatex">\(I(f_h(x^n) \ne \hat{y}^n) = \begin{cases}1 &amp; \text{If } f_h(x^n) \ne \hat{y}^n \\ 0 &amp; \text{Otherwise}\end{cases}\)</span></li>
<li>那么损失函数的取值范围为<span><span class="heti-spacing"> </span><span class="arithmatex">\([0, 1]\)</span></span>，因此这里的损失函数又称为<strong>错误率</strong><span><span class="heti-spacing"> </span>(error rate)</span></li>
<li>当然，由于这是一个分类问题，所以也可以用<strong>交叉熵</strong>来计算误差。但为了方便后续的解释，而且这是一个二项分类问题，所以后面还是按照这里定义的误差函数来分析</li>
</ul>
</li>
</ul>
<p>如果我们能够收集世界上所有的宝可梦和数码宝贝，由此构建了一个数据集<span><span class="heti-spacing"> </span><span class="arithmatex">\(\textcolor{red}{D_{all}}\)</span></span>，那么就能从中找出最佳的阈值<span><span class="heti-spacing"> </span><span class="arithmatex">\(\textcolor{red}{h^{all}}\)</span></span>，即<span><span class="heti-spacing"> </span><span class="arithmatex">\(\textcolor{red}{h^{all}} = arg \min\limits_h L(h, \textcolor{red}{D_{all}})\)</span></span>。然而，事实上我们只能从<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\textcolor{red}{D_{all}}\)</span><span class="heti-spacing"> </span></span>中收集部分的例子<span><span class="heti-spacing"> </span><span class="arithmatex">\(\textcolor{cornflowerblue}{D_{train}} = \{(x^1, \hat{y}^1), (x^2, \hat{y}^2), \dots, (x^N, \hat{y}^N)\}\)</span></span>，其中<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\{x^n, \hat{y}^n\} \sim \textcolor{red}{D_{all}}\)</span><span class="heti-spacing"> </span></span>是<strong>独立同分布</strong>的<span><span class="heti-spacing"> </span>(i.i.d.)</span>；而从<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\textcolor{cornflowerblue}{D_{train}}\)</span><span class="heti-spacing"> </span></span>上得到阈值为<span><span class="heti-spacing"> </span><span class="arithmatex">\(\textcolor{cornflowerblue}{h^{train}} = arg \min\limits_{h} L(h, \textcolor{cornflowerblue}{D_{train}})\)</span></span>。</p>
<p>所以，<span><span class="arithmatex">\(\textcolor{red}{h^{all}}\)</span><span class="heti-spacing"> </span></span>只是我们的<strong>理想</strong>，但<strong>现实</strong>是我们只能通过机器学习得到<span><span class="heti-spacing"> </span><span class="arithmatex">\(\textcolor{cornflowerblue}{h^{train}}\)</span></span>（<del>理想很丰满，现实很骨感（悲）</del><heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。因此，机器学习的目标就是要让现实尽可能地接近理想，在这个例子中就是要让<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(L(\textcolor{cornflowerblue}{h^{train}}, \textcolor{red}{D_{all}})\)</span><span class="heti-spacing"> </span></span>与<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(L(\textcolor{red}{h^{all}}, \textcolor{red}{D_{all}})\)</span><span class="heti-spacing"> </span></span>尽可能地接近。下面通过一个例子来加深对“理想和现实”的认识：</p>
<details class="example" open="open">
<summary>例子</summary>
<div class="tabbed-set tabbed-alternate" data-tabs="1:3"><input checked="checked" id="__tabbed_1_1" name="__tabbed_1" type="radio"/><input id="__tabbed_1_2" name="__tabbed_1" type="radio"/><input id="__tabbed_1_3" name="__tabbed_1" type="radio"/><div class="tabbed-labels"><label for="__tabbed_1_1">理想</label><label for="__tabbed_1_2">现实<span class="heti-skip"><span class="heti-spacing"> </span>1<span class="heti-spacing"> </span></span></label><label for="__tabbed_1_3">现实<span><span class="heti-spacing"> </span>2</span></label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p>假设我们收集到了世界上所有的宝可梦和数码宝贝。下面给出了相应的数据分布图，并计算了最佳阈值及其损失：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/78.png" width="40%/"/>
</div>
<div class="admonition warning">
<p class="admonition-title">注意</p>
<p>要清楚实际上是不可能获得所有数据的，这里只是一个假设。在现实中，一般会用测试数据<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(D_{test}\)</span><span class="heti-spacing"> </span></span>作为<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(D_{all}\)</span><span class="heti-spacing"> </span></span>的代表，因为<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(D_{test}\)</span><span class="heti-spacing"> </span></span>可以反映<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(D_{all}\)</span><span class="heti-spacing"> </span></span>的数据特征。</p>
</div>
</div>
<div class="tabbed-block">
<p>现在随机采样得到<span class="heti-skip"><span class="heti-spacing"> </span>200<span class="heti-spacing"> </span></span>只宝可梦和数码宝贝，得到以下数据分布图，并计算了最佳阈值及其损失：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/79.png" width="40%/"/>
</div>
<details class="question" open="open">
<summary>思考</summary>
<p>诶，怎么<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(L(\textcolor{cornflowerblue}{h^{train1}}, \textcolor{cornflowerblue}{D_{train1}})\)</span><span class="heti-spacing"> </span></span>比<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(L(\textcolor{red}{h^{all}}, \textcolor{red}{D_{all}})\)</span><span class="heti-spacing"> </span></span>还要低呢？事实上，这样的比较是没有意义的，因为这两个损失值是根据不同的数据集计算得到的，两者没有可比性。</p>
<p>我们应该关注的是<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(L(\textcolor{cornflowerblue}{h^{train1}}, \textcolor{red}{D_{all}})\)</span><span class="heti-spacing"> </span></span>的值。在这个例子中，其值为<span><span class="heti-spacing"> </span>0.28</span>，与<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(L(\textcolor{red}{h^{all}}, \textcolor{red}{D_{all}})\)</span><span class="heti-spacing"> </span></span>一致，这说明了“现实与理想”近乎一样<span><span class="heti-spacing"> </span>~</span></p>
<p>然而，这只是最理想的情况，后面的“现实”可没有这么“理想”！</p>
</details>
</div>
<div class="tabbed-block">
<p>现在另外随机采样得到<span class="heti-skip"><span class="heti-spacing"> </span>200<span class="heti-spacing"> </span></span>只宝可梦和数码宝贝，得到以下数据分布图，并计算了最佳阈值及其损失：</p>
<p></p><div style="text-align: center">
<img src="images/lec2/80.png" width="40%/"/>
</div>
<p>通过前面的分析，我们知道即使<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(L(\textcolor{cornflowerblue}{h^{train2}}, \textcolor{cornflowerblue}{D_{train2}})\)</span><span class="heti-spacing"> </span></span>再小也无济于事，效果好不好还得看<span><span class="heti-spacing"> </span><span class="arithmatex">\(L(\textcolor{cornflowerblue}{h^{train2}}, \textcolor{red}{D_{all}})\)</span></span>——其结果为<span><span class="heti-spacing"> </span>0.37</span>，比<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(L(\textcolor{red}{h^{all}}, \textcolor{red}{D_{all}})\)</span><span class="heti-spacing"> </span></span>大好多，这说明了本次训练的效果比较糟糕。</p>
</div>
</div>
</div>
</details>
<p>下面从定量的角度分析训练的目标——我们希望<span><span class="heti-spacing"> </span><span class="arithmatex">\(L(\textcolor{cornflowerblue}{h^{train}}, \textcolor{red}{D_{all}}) - L(\textcolor{red}{h^{all}}, \textcolor{red}{D_{all}}) \le \delta\)</span></span>，其中<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\delta\)</span><span class="heti-spacing"> </span></span>是我们人为设定的范围——如果希望理想和现实很接近，那么就调小这个值。接下来就要考虑什么样的<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\textcolor{cornflowerblue}{D_{train}}\)</span><span class="heti-spacing"> </span></span>能够满足这一不等式的要求呢？这里直接给出结论：</p>
<div class="arithmatex">\[
\forall \textcolor{green}{h} \in H, |L(\textcolor{green}{h}, \textcolor{cornflowerblue}{D_{train}}) - L(\textcolor{green}{h}, \textcolor{red}{D_{all}})| \le \dfrac{\delta}{2}
\]</div>
<p>当这一不等式满足时，我们希望的不等式条件就能成立。此时在对任意<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\textcolor{green}{h}\)</span><span class="heti-spacing"> </span></span>求解损失<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(L\)</span><span class="heti-spacing"> </span></span>时，这个<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\textcolor{cornflowerblue}{D_{train}}\)</span><span class="heti-spacing"> </span></span>是<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\textcolor{red}{D_{all}}\)</span><span class="heti-spacing"> </span></span>的一个不错的代表。</p>
<details class="proof">
<summary>证明</summary>
<div class="arithmatex">\[
\begin{align}
L(\textcolor{cornflowerblue}{h^{train}}, \textcolor{red}{D_{all}}) &amp; \le L(\textcolor{cornflowerblue}{h^{train}}, \textcolor{cornflowerblue}{D_{train}}) + \dfrac{\delta}{2} \notag \\
&amp; \le L(\textcolor{red}{h^{all}}, \textcolor{cornflowerblue}{D_{train}}) + \dfrac{\delta}{2} \quad \quad (\text{Because } \textcolor{cornflowerblue}{h^{train}} = arg \min\limits_{h} L(h, \textcolor{cornflowerblue}{D_{train}}))\notag \\
&amp; \le L(\textcolor{red}{h^{all}}, \textcolor{red}{D_{all}}) + \dfrac{\delta}{2} + \dfrac{\delta}{2} = L(\textcolor{red}{h^{all}}, \textcolor{red}{D_{all}}) + \delta \notag
\end{align}
\]</div>
</details>
<p>更一般地，我们希望采样得到一个好的数据集<span><span class="heti-spacing"> </span><span class="arithmatex">\(\textcolor{cornflowerblue}{D_{train}}\)</span></span>，满足：</p>
<div class="arithmatex">\[
\forall \textcolor{green}{h} \in H, |L(\textcolor{green}{h}, \textcolor{cornflowerblue}{D_{train}}) - L(\textcolor{green}{h}, \textcolor{red}{D_{all}})| \le \varepsilon
\]</div>
<p>那么我们便想知道采样到一个<strong>坏的</strong>数据集<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\textcolor{cornflowerblue}{D_{train}}\)</span><span class="heti-spacing"> </span></span>的概率是多少，下面就来讨论这一问题</p>
<h4 id="probability-of-failure">Probability of Failure<a class="headerlink" href="#probability-of-failure" title="Permanent link">⚓︎</a></h4>
<div class="admonition info">
<p class="admonition-title">注</p>
<p>下面的讨论不局限于特定的模型、特定的数据分布和特定的损失函数，我们讨论的是很一般的情况。</p>
</div>
<p>假设我们随机采样多次，得到多组如下所示的训练数据集。其中用蓝色的点表示好的数据集，黄色的点表示坏的数据集。那么黄色点的数量占所有点的数量就是采样到坏的数据集的概率<span><span class="heti-spacing"> </span><span class="arithmatex">\(P(\textcolor{cornflowerblue}{D_{train}}\ is\ \bm{bad})\)</span></span>。</p>
<div style="text-align: center">
<img src="images/lec2/81.png" width="70%/"/>
</div>
<p>如果<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\textcolor{cornflowerblue}{D_{train}}\)</span><span class="heti-spacing"> </span></span>是坏的话，那么至少有一个阈值<span><span class="heti-spacing"> </span><span class="arithmatex">\(\textcolor{green}{h}\)</span></span>，使得<span><span class="heti-spacing"> </span><span class="arithmatex">\(\forall \textcolor{green}{h} \in H, |L(\textcolor{green}{h}, \textcolor{cornflowerblue}{D_{train}}) - L(\textcolor{green}{h}, \textcolor{red}{D_{all}})| &gt; \varepsilon\)</span></span>。下图展示了其中一种可能的情况：</p>
<div style="text-align: center">
<img src="images/lec2/82.png" width="70%/"/>
</div>
<p>用数学语言描述为：</p>
<div class="arithmatex">\[
\begin{align}
P(\textcolor{cornflowerblue}{D_{train}}\ is\ \bm{bad}) &amp; = \bigcup\limits_{h \in H} P(\textcolor{cornflowerblue}{D_{train}}\ is\ \bm{bad}\ due\ to\ \textcolor{green}{h}) \notag \\
&amp; \le \sum\limits_{h \in H} P(\textcolor{cornflowerblue}{D_{train}}\ is\ \bm{bad}\ due\ to\ \textcolor{green}{h}) \notag
\end{align}
\]</div>
<ul>
<li>为了计算方便，我们将集合的并运算转化为求和公式。这一放缩比较大胆，因为求和的结果很可能超过<span><span class="heti-spacing"> </span>1</span>，但为了后续讲解的方便，所以就不再深究这个问题了</li>
<li>
<p>下面用数学公式和一些图形来回顾何为“<span class="arithmatex">\(\textcolor{cornflowerblue}{D_{train}}\ is\ \bm{bad}\ due\ to\ \textcolor{green}{h}\)</span>”</p>
<p></p><div style="text-align: center">
<img src="images/lec2/83.png" width="70%/"/>
</div>
</li>
<li>
<p>用<a href="https://zh.wikipedia.org/wiki/%E9%9C%8D%E5%A4%AB%E4%B8%81%E4%B8%8D%E7%AD%89%E5%BC%8F"><strong>霍夫丁不等式</strong></a>计算这个概率值：</p>
<div class="arithmatex">\[
P(\textcolor{cornflowerblue}{D_{train}}\ is\ \bm{bad}\ due\ to\ \textcolor{green}{h}) \le 2exp(-2N \varepsilon^2)
\]</div>
<ul>
<li>该不等式要求损失值<span><span class="heti-spacing"> </span><span class="arithmatex">\(L \in [0, 1]\)</span></span></li>
<li><span><span class="arithmatex">\(N\)</span><span class="heti-spacing"> </span></span>表示<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\textcolor{cornflowerblue}{D_{train}}\)</span><span class="heti-spacing"> </span></span>中的样例数</li>
</ul>
</li>
</ul>
<p>继续化简式子：</p>
<div class="arithmatex">\[
\begin{align}
P(\textcolor{cornflowerblue}{D_{train}}\ is\ \bm{bad}) &amp; = \bigcup\limits_{h \in H} P(\textcolor{cornflowerblue}{D_{train}}\ is\ \bm{bad}\ due\ to\ \textcolor{green}{h}) \notag \\
&amp; \le \sum\limits_{h \in H} P(\textcolor{cornflowerblue}{D_{train}}\ is\ \bm{bad}\ due\ to\ \textcolor{green}{h}) \notag
&amp; \le \sum\limits_{h \in H} 2exp(-2N \varepsilon^2) \notag \\
&amp; = |H| \cdot 2exp(-2N \varepsilon^2) \notag
\end{align}
\]</div>
<p>很明显，要想减小概率<span><span class="heti-spacing"> </span><span class="arithmatex">\(P(\textcolor{cornflowerblue}{D_{train}}\ is\ \bm{bad})\)</span></span>，可以做的调整有：</p>
<ul>
<li>
<p>增大<span><span class="heti-spacing"> </span><span class="arithmatex">\(N\)</span></span></p>
<p></p><div style="text-align: center">
<img src="images/lec2/84.png" width="70%/"/>
</div>
</li>
<li>
<p>减小<span><span class="heti-spacing"> </span><span class="arithmatex">\(|H|\)</span></span></p>
<p></p><div style="text-align: center">
<img src="images/lec2/85.png" width="70%/"/>
</div>
</li>
</ul>
<details class="example">
<summary>例子</summary>
<p></p><div style="text-align: center">
<img src="images/lec2/86.png" width="60%/"/>
</div>
<p>如果我们希望<span><span class="heti-spacing"> </span><span class="arithmatex">\(P(\textcolor{cornflowerblue}{D_{train}}\ is\ \bm{bad}) \le \delta\)</span></span>，需要多少的训练样本呢？</p>
<div class="arithmatex">\[
|H| \cdot 2exp(-2N\varepsilon^2) \le \delta \Rightarrow N \ge \dfrac{\log(\frac{2|H|}{\delta})}{2\varepsilon}
\]</div>
<p>假设<span><span class="heti-spacing"> </span><span class="arithmatex">\(|H| = 10000, \delta = 0.1, \varepsilon = 0.1\)</span></span>，可以解得<span><span class="heti-spacing"> </span><span class="arithmatex">\(N \ge 610\)</span></span></p>
</details>
<h4 id="model-complexity">Model Complexity<a class="headerlink" href="#model-complexity" title="Permanent link">⚓︎</a></h4>
<div class="arithmatex">\[
P(\textcolor{cornflowerblue}{D_{train}}\ is\ \bm{bad}\ due\ to\ \textcolor{green}{h}) \le 2exp(-2N \varepsilon^2)
\]</div>
<p>再次观察霍夫丁不等式：其中的<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(|H|\)</span><span class="heti-spacing"> </span></span>表示可选的函数个数。然而，可能会出现一种情况：函数的参数是连续的，那么<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(|H|\)</span><span class="heti-spacing"> </span></span>不就是<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\infty\)</span><span class="heti-spacing"> </span></span>了吗，此时上面的不等式也就没有用了。关于这个问题，有以下几种解答：</p>
<ul>
<li>计算机能做的事情都是离散的（<del>好像很有道理</del>）</li>
<li>使用<a href="https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension"><span><span class="heti-spacing"> </span>VC-dimension</span></a>（本笔记中不会涉及到）</li>
</ul>
<p>既然可以通过减小<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(|H|\)</span><span class="heti-spacing"> </span></span>来降低<span><span class="heti-spacing"> </span><span class="arithmatex">\(P(\textcolor{cornflowerblue}{D_{train}})\)</span></span>，但为什么实际上我们不会让<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(|H|\)</span><span class="heti-spacing"> </span></span>变得很小？原因在于：更小的<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(H\)</span><span class="heti-spacing"> </span></span>意味着更少的候选函数（模型<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，那么<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(\textcolor{red}{h^{all}} = arg \min\limits_{h \in H} L(h, D_{all})\)</span><span class="heti-spacing"> </span></span>的变动余地也很小，这可能导致<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(L(\textcolor{red}{h^{all}}, \textcolor{red}{D_{all}})\)</span><span class="heti-spacing"> </span></span>变得更大。此时让<span class="heti-skip"><span class="heti-spacing"> </span><span class="arithmatex">\(L(\textcolor{cornflowerblue}{h^{train}}, \textcolor{red}{D_{all}})\)</span><span class="heti-spacing"> </span></span>再接近<span><span class="heti-spacing"> </span><span class="arithmatex">\(L(\textcolor{red}{h^{all}}, \textcolor{red}{D_{all}})\)</span></span>，前者的值会跟着后者的值一起变得很大，那么训练效果就不是很好了。下图形象地说明这一问题：</p>
<div style="text-align: center">
<img src="images/lec2/87.png" width="70%/"/>
</div>
<p>那么，是否存在一种“鱼和熊掌可以兼得”的方法呢？答案是有的，请见之后对深度学习进一步的介绍！</p>
<aside class="md-source-file">
<span class="md-source-file__fact">
<span class="md-icon" title="最后更新">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"></path></svg>
</span>
<span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime">2025年3月2日 21:04:42</span>
</span>
<span class="md-source-file__fact">
<span class="md-icon" title="创建日期">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"></path></svg>
</span>
<span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime">2025年3月2日 21:04:42</span>
</span>
</aside>
<p style="font-size: 30px; font-weight: 600">评论区</p>
<div>
    如果大家有什么问题或想法，欢迎在下方留言~
  </div>
<!-- Insert generated snippet here -->
<script async="" crossorigin="anonymous" data-category="Announcements" data-category-id="DIC_kwDOMAb9Zs4CfmpP" data-emit-metadata="0" data-input-position="bottom" data-lang="zh-CN" data-mapping="pathname" data-reactions-enabled="1" data-repo="noughtq/notebook" data-repo-id="R_kgDOMAb9Zg" data-strict="0" data-theme="preferred_color_scheme" src="https://giscus.app/client.js">
</script>
<!-- Synchronize Giscus theme with palette -->
<script>
    var giscus = document.querySelector("script[src*=giscus]")

    // Set palette on initial load
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
      var theme = palette.color.scheme === "slate"
        ? "transparent_dark"
        : "light"

      // Instruct Giscus to set theme
      giscus.setAttribute("data-theme", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener("DOMContentLoaded", function() {
      var ref = document.querySelector("[data-md-component=palette]")
      ref.addEventListener("change", function() {
        var palette = __md_get("__palette")
        if (palette && typeof palette.color === "object") {
          var theme = palette.color.scheme === "slate"
            ? "dark"
            : "light"

          // Instruct Giscus to change theme
          var frame = document.querySelector(".giscus-frame")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            "https://giscus.app"
          )
        }
      })
    })
  </script>
<!-- 标题计数器 -->
<link href="/css/counter.css" rel="stylesheet"/>
<!-- 主页个性化 -->
</article>
</div>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div>
<button class="md-top md-icon" data-md-component="top" hidden="" type="button">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  回到页面顶部
</button>
</main>
<footer class="md-footer">
<nav aria-label="页脚" class="md-footer__inner md-grid">
<a aria-label="上一页: Lec 1: Intro to Machine Learning" class="md-footer__link md-footer__link--prev" href="1.html">
<div class="md-footer__button md-icon">
<svg viewbox="0 0 320 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path></svg>
</div>
<div class="md-footer__title">
<span class="md-footer__direction">
                上一页
              </span>
<div class="md-ellipsis">
                Lec 1: Intro to Machine Learning
              </div>
</div>
</a>
<a aria-label="下一页: Lec 3: Image as input" class="md-footer__link md-footer__link--next" href="3.html">
<div class="md-footer__title">
<span class="md-footer__direction">
                下一页
              </span>
<div class="md-ellipsis">
                Lec 3: Image as input
              </div>
</div>
<div class="md-footer__button md-icon">
<svg viewbox="0 0 320 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg>
</div>
</a>
</nav>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
<div class="md-copyright__highlight">
        Copyright © 2024-2025 <a href="https://github.com/NoughtQ">NoughtQ</a>
</div>
    
    
      Powered by
      <a href="https://www.mkdocs.org/" rel="noopener" target="_blank">
        MkDocs
      </a>
      with theme
      <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
        Material
      </a>
      modified by
      <a href="https://github.com/NoughtQ" rel="noopener" target="_blank">
        NoughtQ
      </a>
</div>
<div class="md-social">
<a class="md-social__link" href="https://github.com/noughtq" rel="noopener" target="_blank" title="github.com">
<svg viewbox="0 0 496 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
</a>
<a class="md-social__link" href="https://blog.noughtq.top" rel="noopener" target="_blank" title="blog.noughtq.top">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M192 32c0 17.7 14.3 32 32 32 123.7 0 224 100.3 224 224 0 17.7 14.3 32 32 32s32-14.3 32-32C512 128.9 383.1 0 224 0c-17.7 0-32 14.3-32 32m0 96c0 17.7 14.3 32 32 32 70.7 0 128 57.3 128 128 0 17.7 14.3 32 32 32s32-14.3 32-32c0-106-86-192-192-192-17.7 0-32 14.3-32 32m-96 16c0-26.5-21.5-48-48-48S0 117.5 0 144v224c0 79.5 64.5 144 144 144s144-64.5 144-144-64.5-144-144-144h-16v96h16c26.5 0 48 21.5 48 48s-21.5 48-48 48-48-21.5-48-48z"></path></svg>
</a>
<a class="md-social__link" href="mailto:noughtq666@gmail.com" rel="noopener" target="_blank" title="">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m20 8-8 5-8-5V6l8 5 8-5m0-2H4c-1.11 0-2 .89-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../..", "features": ["content.action.edit", "content.action.view", "content.code.copy", "content.code.annotate", "header.autohide", "navigation.tabs", "navigation.top", "navigation.footer", "navigation.indexes", "navigation.tracking", "navigation.prune", "search.share"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
<script src="../../assets/javascripts/bundle.f1b6f286.min.js"></script>
<script src="../../js/anchor.js"></script>
<script src="../../js/katex.js"></script>
<script src="../../js/toc.js"></script>
<script src="../../js/typed.js"></script>
<script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
<script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
</body>
</html>