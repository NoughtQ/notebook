<!DOCTYPE html>
<html class="no-js" lang="zh">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="NoughtQ的笔记本，主要记录一些 CS 相关的笔记" name="description"/>
<meta content="NoughtQ" name="author"/>
<link href="https://notebook.noughtq.top/ai/genai/3.html" rel="canonical"/>
<link href="2.html" rel="prev"/>
<link href="4.html" rel="next"/>
<link href="../../feed_rss_created.xml" rel="alternate" title="RSS 订阅" type="application/rss+xml"/>
<link href="../../feed_rss_updated.xml" rel="alternate" title="已更新内容的 RSS 订阅" type="application/rss+xml"/>
<link href="../../assets/favicon.png" rel="icon"/>
<meta content="mkdocs-1.6.1, mkdocs-material-9.6.18" name="generator"/>
<title>LLM Training - NoughtQ的笔记本</title>
<link href="../../assets/stylesheets/main.7e37652d.min.css" rel="stylesheet"/>
<link href="../../assets/stylesheets/palette.06af60db.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=JetBrains+Mono,+LXGW+WenKai+Screen+GB+Screen:300,300i,400,400i,700,700i%7CJetBrains+Mono,+Consolas:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"JetBrains Mono, LXGW WenKai Screen GB Screen";--md-code-font:"JetBrains Mono, Consolas"}</style>
<link href="../../css/heti.css" rel="stylesheet"/>
<link href="../../css/toc_extra.css" rel="stylesheet"/>
<link href="../../css/timeline.css" rel="stylesheet"/>
<link href="../../css/card.css" rel="stylesheet"/>
<link href="../../css/custom.css" rel="stylesheet"/>
<link href="../../css/extra_changelog.css" rel="stylesheet"/>
<link href="../../css/header.css" rel="stylesheet"/>
<link href="../../css/sidebar.css" rel="stylesheet"/>
<link href="https://unpkg.com/katex@0/dist/katex.min.css" rel="stylesheet"/>
<link href="https://gcore.jsdelivr.net/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css" rel="stylesheet"/>
<link href="https://gcore.jsdelivr.net/npm/lxgw-wenkai-webfont@1.1.0/style.css" rel="stylesheet"/>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&amp;display=swap" rel="stylesheet"/>
<script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-43NH8CVRCJ"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-43NH8CVRCJ",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-43NH8CVRCJ",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
<script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
</head>
<body data-md-color-accent="indigo" data-md-color-primary="indigo" data-md-color-scheme="slate" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#llm-training">
          跳转至
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header" data-md-component="header">
<nav aria-label="页眉" class="md-header__inner md-grid">
<a aria-label="NoughtQ的笔记本" class="md-header__button md-logo" data-md-component="logo" href="../.." title="NoughtQ的笔记本">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.05 9H7.06V6h1.99V4.03H7.06v-1c0-1.11.89-1.99 1.99-1.99h5.98V8l2.47-1.5L20 8V1.04h1c1.05 0 2 .96 2 1.99V17c0 1.03-.95 2-2 2H9.05c-1.05 0-1.99-.95-1.99-2v-1h1.99v-2H7.06v-3h1.99zM1 18h2v-3H1v-2h2v-3H1V8h2V5h2v3H3v2h2v3H3v2h2v3H3v2h2v1h16v2H5a2 2 0 0 1-2-2v-1H1z"></path></svg>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            NoughtQ的笔记本
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              LLM Training
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Dark Mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefer-color-scheme: dark)" data-md-color-primary="indigo" data-md-color-scheme="slate" id="__palette_0" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Dark Mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
</label>
<input aria-label="Light Mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefer-color-scheme: light)" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_0" hidden="" title="Light Mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"></path></svg>
</label>
</form>
<script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="搜索" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="搜索" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
<svg viewbox="0 0 256 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L77.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z" fill="currentColor"></path></svg>
</label>
<nav aria-label="查找" class="md-search__options">
<a aria-label="分享" class="md-search__icon md-icon" data-clipboard="" data-clipboard-text="" data-md-component="search-share" href="javascript:void(0)" tabindex="-1" title="分享">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg>
</a>
<button aria-label="清空当前内容" class="md-search__icon md-icon" tabindex="-1" title="清空当前内容" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
</button>
</nav>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="" tabindex="0">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
<ol class="md-search-result__list" role="presentation"></ol>
</div>
</div>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/noughtq/notebook" title="前往仓库">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4" fill="currentColor"></path></svg>
</div>
<div class="md-source__repository">
    NoughtQ/Notebook
  </div>
</a>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<nav aria-label="标签" class="md-tabs" data-md-component="tabs">
<div class="md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../index.html">
          
  
  
    
  
  🏫主页

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../lang/index.html">
          
  
  
    
  
  🔡语言

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../math/index.html">
          
  
  
    
  
  📊数学相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../algorithms/index.html">
          
  
  
    
  
  🧮算法相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../software/index.html">
          
  
  
    
  
  💾软件相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../system/index.html">
          
  
  
    
  
  💻系统相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../web/index.html">
          
  
  
    
  
  🌏Web相关

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../sec/ctf-101/index.html">
          
  
  
    
  
  🛡️信息安全

        </a>
</li>
<li class="md-tabs__item md-tabs__item--active">
<a class="md-tabs__link" href="../index.html">
          
  
  
    
  
  🤖人工智能

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../misc/index.html">
          
  
  
    
  
  🗃️杂项

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../tools/index.html">
          
  
  
    
  
  🛠️工具

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../papers/index.html">
          
  
  
    
  
  📑论文阅读

        </a>
</li>
</ul>
</div>
</nav>
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="导航栏" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="NoughtQ的笔记本" class="md-nav__button md-logo" data-md-component="logo" href="../.." title="NoughtQ的笔记本">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.05 9H7.06V6h1.99V4.03H7.06v-1c0-1.11.89-1.99 1.99-1.99h5.98V8l2.47-1.5L20 8V1.04h1c1.05 0 2 .96 2 1.99V17c0 1.03-.95 2-2 2H9.05c-1.05 0-1.99-.95-1.99-2v-1h1.99v-2H7.06v-3h1.99zM1 18h2v-3H1v-2h2v-3H1V8h2V5h2v3H3v2h2v3H3v2h2v3H3v2h2v1h16v2H5a2 2 0 0 1-2-2v-1H1z"></path></svg>
</a>
    NoughtQ的笔记本
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/noughtq/notebook" title="前往仓库">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4" fill="currentColor"></path></svg>
</div>
<div class="md-source__repository">
    NoughtQ/Notebook
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../index.html">
<span class="md-ellipsis">
    🏫主页
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../lang/index.html">
<span class="md-ellipsis">
    🔡语言
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../math/index.html">
<span class="md-ellipsis">
    📊数学相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../algorithms/index.html">
<span class="md-ellipsis">
    🧮算法相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../software/index.html">
<span class="md-ellipsis">
    💾软件相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../system/index.html">
<span class="md-ellipsis">
    💻系统相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../web/index.html">
<span class="md-ellipsis">
    🌏Web相关
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../sec/ctf-101/index.html">
<span class="md-ellipsis">
    🛡️信息安全
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_9" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../index.html">
<span class="md-ellipsis">
    🤖人工智能
    
  </span>
</a>
<label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_9_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_9">
<span class="md-nav__icon md-icon"></span>
            🤖人工智能
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../ml/index.html">
<span class="md-ellipsis">
    机器学习
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_9_3" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="index.html">
<span class="md-ellipsis">
    生成式人工智能
    
  </span>
</a>
<label class="md-nav__link" for="__nav_9_3" id="__nav_9_3_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_9_3_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_9_3">
<span class="md-nav__icon md-icon"></span>
            生成式人工智能
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="1.html">
<span class="md-ellipsis">
    Introduction
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="2.html">
<span class="md-ellipsis">
    Prompt Engineering
    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
<span class="md-ellipsis">
    LLM Training
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="3.html">
<span class="md-ellipsis">
    LLM Training
    
  </span>
</a>
<nav aria-label="目录" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
        目录
      </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#pretrain">
<span class="md-ellipsis">
      Pretrain
    </span>
</a>
<nav aria-label="Pretrain" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#challenge-from-parameters">
<span class="md-ellipsis">
      Challenge from Parameters
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#self-supervised-learning">
<span class="md-ellipsis">
      Self-Supervised Learning
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#problem">
<span class="md-ellipsis">
      Problem
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#instruction-fine-tuning">
<span class="md-ellipsis">
      Instruction Fine-tuning
    </span>
</a>
<nav aria-label="Instruction Fine-tuning" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#different-paths-of-fine-tuning">
<span class="md-ellipsis">
      Different Paths of Fine-tuning
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#open-source-models">
<span class="md-ellipsis">
      Open Source Models
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#rlhf">
<span class="md-ellipsis">
      RLHF
    </span>
</a>
<nav aria-label="RLHF" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#insturction-fine-tuning-vs-rlhf">
<span class="md-ellipsis">
      Insturction Fine-tuning v.s. RLHF
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#language-model-vs-alphago">
<span class="md-ellipsis">
      Language Model v.s. AlphaGo
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#reward-model">
<span class="md-ellipsis">
      Reward Model
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#difficulty">
<span class="md-ellipsis">
      Difficulty
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="4.html">
<span class="md-ellipsis">
    AI Agent
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="5.html">
<span class="md-ellipsis">
    Explainability
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="6.html">
<span class="md-ellipsis">
    Evaluation
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="7.html">
<span class="md-ellipsis">
    Security
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="8.html">
<span class="md-ellipsis">
    Generation Strategies
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="9.html">
<span class="md-ellipsis">
    Image and Video Generation
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="10.html">
<span class="md-ellipsis">
    Voice Mode of GPT-4o
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="11.html">
<span class="md-ellipsis">
    Inner Workings of Transformer-based LM
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="12.html">
<span class="md-ellipsis">
    Competitors of Transformer
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../cv/index.html">
<span class="md-ellipsis">
    计算机视觉导论
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../misc/index.html">
<span class="md-ellipsis">
    🗃️杂项
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../tools/index.html">
<span class="md-ellipsis">
    🛠️工具
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../papers/index.html">
<span class="md-ellipsis">
    📑论文阅读
    
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="目录" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
        目录
      </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#pretrain">
<span class="md-ellipsis">
      Pretrain
    </span>
</a>
<nav aria-label="Pretrain" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#challenge-from-parameters">
<span class="md-ellipsis">
      Challenge from Parameters
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#self-supervised-learning">
<span class="md-ellipsis">
      Self-Supervised Learning
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#problem">
<span class="md-ellipsis">
      Problem
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#instruction-fine-tuning">
<span class="md-ellipsis">
      Instruction Fine-tuning
    </span>
</a>
<nav aria-label="Instruction Fine-tuning" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#different-paths-of-fine-tuning">
<span class="md-ellipsis">
      Different Paths of Fine-tuning
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#open-source-models">
<span class="md-ellipsis">
      Open Source Models
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#rlhf">
<span class="md-ellipsis">
      RLHF
    </span>
</a>
<nav aria-label="RLHF" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#insturction-fine-tuning-vs-rlhf">
<span class="md-ellipsis">
      Insturction Fine-tuning v.s. RLHF
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#language-model-vs-alphago">
<span class="md-ellipsis">
      Language Model v.s. AlphaGo
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#reward-model">
<span class="md-ellipsis">
      Reward Model
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#difficulty">
<span class="md-ellipsis">
      Difficulty
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<a class="md-content__button md-icon" href="https://github.com/noughtq/notebook/edit/master/docs/ai/genai/3.md" rel="edit" title="编辑此页">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
</a>
<a class="md-content__button md-icon" href="https://github.com/noughtq/notebook/raw/master/docs/ai/genai/3.md" title="查看本页的源代码">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
</a>
<div><h1 id="llm-training">LLM Training<a class="headerlink" href="#llm-training" title="Permanent link">⚓︎</a></h1>
<div style="margin-top: -30px; font-size: 0.9em; opacity: 0.7;">
<p><span class="twemoji"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10h-2a8 8 0 0 1-8 8 8 8 0 0 1-8-8 8 8 0 0 1 8-8zm6.78 1a.7.7 0 0 0-.48.2l-1.22 1.21 2.5 2.5L20.8 5.7c.26-.26.26-.7 0-.95L19.25 3.2c-.13-.13-.3-.2-.47-.2m-2.41 2.12L9 12.5V15h2.5l7.37-7.38z"></path></svg></span> 约<span class="heti-skip"><span class="heti-spacing"> </span>4544<span class="heti-spacing"> </span></span>个字 <span class="twemoji"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 20c4.42 0 8-3.58 8-8s-3.58-8-8-8-8 3.58-8 8 3.58 8 8 8m0-18c5.5 0 10 4.5 10 10s-4.5 10-10 10C6.47 22 2 17.5 2 12S6.5 2 12 2m.5 11H11V7h1.5v4.26l3.7-2.13.75 1.3z"></path></svg></span> 预计阅读时间<span class="heti-skip"><span class="heti-spacing"> </span>23<span class="heti-spacing"> </span></span>分钟</p>
</div>
<p>本讲将详细介绍大语言模型训练的三个阶段（其实已经在<a href="1.html#gpt-3">前面</a>简单提过一点了<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>：</p>
<ol>
<li>自我学习，累积实力——<strong>预训练</strong>(pre-train)（<strong>自监督学习</strong>(self-supervised learning)）</li>
<li>名师指点，发挥潜力——<strong>指令微调</strong>(instruction fine-tuning)（<strong>监督学习</strong>(supervised learning)）</li>
<li>参与实战，打磨技巧——<strong>来自人类反馈的强化学习</strong>(reinforcement learning from human feedback, RLHF)</li>
</ol>
<p>不要被这些看似高大上的名词唬住了——对语言模型而言，所有的这些阶段都是在学文字接龙，只是训练数据有所不同。</p>
<h2 id="pretrain">Pretrain<a class="headerlink" href="#pretrain" title="Permanent link">⚓︎</a></h2>
<h3 id="challenge-from-parameters">Challenge from Parameters<a class="headerlink" href="#challenge-from-parameters" title="Permanent link">⚓︎</a></h3>
<p>我们知道，训练的过程就是对模型的<strong>优化</strong>(optimization)，而优化的目标就是找到一组好的参数。但有时训练可能会失败（根据已有的训练数据无法找到更好的参数<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，此时就要重新设定<strong>超参数</strong>(hyperparameter)（即无法通过训练得到，只能人为设置的参数；人们常说的“调参”中的“参”指的是超参数<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，然后再次训练。由于训练的时候难免会换多组超参数，不断尝试训练，所以<strong>算力</strong>的支持在优化中至关重要。</p>
<div style="text-align: center">
<img src="images/lec3/1.png" width="50%/"/>
</div>
<p>但即便找到了让模型训练成功的超参数，模型在测试中也有失败的可能。这种现象就是机器学习中常见的“<strong>过拟合</strong>”(overfitting)。比如对于下图的猫狗分类器，在训练数据中是<span class="heti-skip"><span class="heti-spacing"> </span>OK<span class="heti-spacing"> </span></span>的，但是一做测试就寄了。这是因为分类器从训练数据中学到了错误的参数——把黑色的动物当做猫，把黄色的动物当做狗。所以它在测试中看到黄色的猫时，就将其误判成狗了。</p>
<div style="text-align: center">
<img src="images/lec3/2.png" width="60%/"/>
</div>
<p>所以我们不要想当然地将人类的思维类比到机器学习上——机器学习只管找到的参数有没有符合训练数据，不管学到的参数（对人类而言）是否合理。</p>
<p>要想让模型找到比较“合理”的参数，一种方法是增加训练数据的多样性（比如<strong>数据增强</strong><heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。对于上面的例子，就可以往训练数据中放黄色的猫和黑色的狗，这样至少不会让分类器把颜色作为区分猫狗的唯一标准。</p>
<div style="text-align: center">
<img src="images/lec3/3.png" width="55%/"/>
</div>
<p>另一种方法是设置好<strong>初始参数</strong>（注意不是超参数，这些参数会在训练过程中不断更新<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。由于模型会基于初始参数的配置来看待训练数据，所以优化后的模型参数可能和初始参数比较接近。通常我们不清楚该如何设置初始参数，所以一种常见做法是随机设置初始参数，这种方法叫做<strong>从头训练</strong>(train from scratch)。</p>
<div style="text-align: center">
<img src="images/lec3/4.png" width="60%/"/>
</div>
<p>但我们也可以自己找到比较好的初始参数，让优化后的参数是比较合理的。我们把这样的初始参数当做给模型的“先验知识”。现在的问题就变成了到哪里去找“好”的参数呢？这便是<span class="heti-skip"><span class="heti-spacing"> </span>LLM<span class="heti-spacing"> </span></span>训练的第一阶段。</p>
<h3 id="self-supervised-learning">Self-Supervised Learning<a class="headerlink" href="#self-supervised-learning" title="Permanent link">⚓︎</a></h3>
<p>我们知道语言模型的训练就是让模型学会做文字接龙。那么模型到底需要多少数据才能学会这一本领呢？看模型是否掌握文字接龙的本领，除了看它是否理解<strong>语言知识</strong>外，还要看它是否理解<strong>世界知识</strong>（比如是否符合自然规律等等<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。下面便是有关这一标准的<a href="https://arxiv.org/abs/2011.04946">研究</a>结果：</p>
<div style="text-align: center">
<img src="images/lec3/5.png" width="70%/"/>
</div>
<p>可以看到，相比语言知识，要学到复杂有层次的世界知识，就需要大量的训练数据。我们可以拿任何的文字数据作为训练数据使用，而网络正是大量文字资料的主要来源。根据网上爬下来的大量数据对模型训练的时候，人工的干涉很少，因此我们称这种学习方式为<strong>自监督学习</strong>(self-supervised learning)。</p>
<div style="text-align: center">
<img src="images/lec3/6.png" width="60%/"/>
</div>
<p>虽说“人工的干涉很少”，但这不代表我们不对来自网络的数据做任何处理。在将网络数据作为训练数据前，我们要对数据进行清理，具体包括（这些方法来自<span class="heti-skip"><span class="heti-spacing"> </span>DeepMind<span class="heti-spacing"> </span></span>的一项<a href="https://arxiv.org/abs/2112.11446">研究</a><heti-adjacent class="heti-adjacent-half">）</heti-adjacent>：</p>
<ul>
<li><strong>内容过滤</strong>：过滤有害内容</li>
<li><strong>文本提取</strong>：去除像<span class="heti-skip"><span class="heti-spacing"> </span>HTML<span class="heti-spacing"> </span></span>标签这样对模型训练无意义的符号</li>
<li>
<p><strong>质量过滤</strong>：去除“低质量”的数据</p>
<ul>
<li>质量的高低可以用专门的“数据质量”分类器来区别，像<span><span class="heti-spacing"> </span>GPT-3</span>，The Pile，<span>PaLM<span class="heti-spacing"> </span></span>等模型正是这样做的</li>
<li>可参考的一个标准：高质量的语句在数据训练时会被多次复用</li>
</ul>
</li>
<li>
<p><strong>数据去重</strong></p>
<ul>
<li>
<p>因为网络中有很多重复的数据，比如一篇题为 <a href="https://arxiv.org/abs/2107.06499"><em>Deduplicating Training Data Makes Language Models Better</em></a> 论文指出，像以下这个文段在网络数据中出现了六万多次</p>
<p></p><div style="text-align: center">
<img src="images/lec3/7.png" width="50%/"/>
</div>
</li>
</ul>
</li>
<li>
<p><strong>测试<span class="heti-skip"><span class="heti-spacing"> </span>-<span class="heti-spacing"> </span></span>设置过滤</strong>：为了实验的严谨性</p>
</li>
</ul>
<p>需要注意的是，并不是所有网络数据都能直接拿来作为训练数据使用。随意爬取网络数据可能会涉及到法律问题。</p>
<div style="text-align: center">
<img src="images/lec3/8.png" width="70%/"/>
</div>
<h3 id="problem">Problem<a class="headerlink" href="#problem" title="Permanent link">⚓︎</a></h3>
<p>回顾一下<span class="heti-skip"><span class="heti-spacing"> </span>GPT-1<span class="heti-spacing"> </span></span>到<span class="heti-skip"><span class="heti-spacing"> </span>GPT-3<span class="heti-spacing"> </span></span>的<a href="1.html#history">发展史</a>，读者会发现尽管模型的规模和训练数据量翻了好几个量级，但是模型的表现也没有明显提升。而这样的问题在其他大模型中同样存在，如下图所示：</p>
<div style="text-align: center">
<img src="images/lec3/9.png" width="60%/"/>
</div>
<p>那为什么语言模型就不能好好回答我们的问题呢？我们不能怪罪模型，因为训练的时候我们也没有教它回答问题啊。所以语言模型从网络数据中学到很多东西，却不知道使用方法；就好像一个人修炼了上乘内功，却不知道如何运用出来。这个时候，就要进入训练的第二阶段——名师指点，发挥潜力！</p>
<h2 id="instruction-fine-tuning">Instruction Fine-tuning<a class="headerlink" href="#instruction-fine-tuning" title="Permanent link">⚓︎</a></h2>
<p>模型接收大量网络数据的洗礼后，接下来应该接收人类老师的教导了——我们提供问题和答案，让模型学会在接收用户提问后能够准确作答的能力。</p>
<div style="text-align: center">
<img src="images/lec3/10.png" width="60%/"/>
</div>
<p>我们称这种学习方式为<strong>监督学习</strong>(supervised-learning)。它的一个缺点是数据标注（为每个问题配对一个正确答案）需要耗费大量人力成本。</p>
<p>也许细心的读者会注意到，图片中的输入输出都包含了<span><span class="heti-spacing"> </span>"USER"</span>，"AI"。之所以要特别标注出来，是因为如果不标注的话，训练起来就好像模型自问自答一样，并不是为用户问题而作答。也许读者会想，我们平时用<span class="heti-skip"><span class="heti-spacing"> </span>ChatGPT<span class="heti-spacing"> </span></span>的时候怎么没看到这样的标注呢？实际上可能存在这样的标注，只是平台将这种标注设为不可见罢了（下图的<span class="heti-skip"><span class="heti-spacing"> </span>"USER"<span class="heti-spacing"> </span></span>和<span class="heti-skip"><span class="heti-spacing"> </span>"AI"<span class="heti-spacing"> </span></span>标注是自己写上去的<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。</p>
<div style="text-align: center">
<img src="images/lec3/11.png" width="70%/"/>
</div>
<hr/>
<p>如果只靠人类老师来教模型的话（即仅使用监督学习<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，由于人力贵，无法收集很多数据，因此模型可能很容易从训练数据中学到错误的参数（在训练数据中是没有问题，但一测试就露馅了<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。</p>
<div style="text-align: center">
<img src="images/lec3/12.png" width="55%/"/>
</div>
<div style="text-align: center">
<img src="images/lec3/13.png" width="65%/"/>
</div>
<p>所以用到第一阶段的参数作为初始参数就相当关键了：先让模型在大量网络数据中做训练（称为<strong>预训练</strong>(pre-train)<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，用训练得到的参数作为初始参数；再用人类标注好的数据（数量相对较少）来优化模型，这样得到的参数不会和第一阶段的参数相差很多，并且模型不会仅凭简单的规则来做文字接龙。因此，第二阶段被叫做<strong>指令微调</strong>(instruction fine-tuning)。</p>
<div style="text-align: center">
<img src="images/lec3/14.png" width="60%/"/>
</div>
<div style="text-align: center">
<img src="images/lec3/17.png" width="70%/"/>
</div>
<p>为了让两个阶段最终得到的参数相差不大，人们发明了一种叫做<strong>适配器</strong><span>(adapter)<span class="heti-spacing"> </span></span>的技术，比如<span><span class="heti-spacing"> </span>LoRA</span>。它的做法是保留第一阶段得到的参数，但增加少量额外的参数；第二阶段的训练只更新新增的参数，固定原参数不变。这样两阶段的参数就比较相近了。</p>
<div style="text-align: center">
<img src="images/lec3/15.png" width="60%/"/>
</div>
<p>除<span class="heti-skip"><span class="heti-spacing"> </span>LoRA<span class="heti-spacing"> </span></span>外，还有其他不同的适配器（固定或插入不同的参数<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，下面仅列举一些，不再细讲。</p>
<div style="text-align: center">
<img src="images/lec3/16.png" width="70%/"/>
</div>
<p>由于预训练后的模型参数里蕴含了非常复杂的规则，能做非常复杂的事情，所以再对模型做好优化后就有可能让模型具备很强的“举一反三”能力。比如模型在多种语言上做预训练后，只要教它某一种语言的特定任务后，它就能自动学会其他语言的相同任务（这个例子讲过很多遍了<span><span class="heti-spacing"> </span>...</span><heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。</p>
<div style="text-align: center">
<img src="images/lec3/18.png" width="60%/"/>
</div>
<div style="text-align: center">
<img src="images/lec3/19.png" width="60%/"/>
</div>
<h3 id="different-paths-of-fine-tuning">Different Paths of Fine-tuning<a class="headerlink" href="#different-paths-of-fine-tuning" title="Permanent link">⚓︎</a></h3>
<p>微调的路线可分为两类：</p>
<ul>
<li>
<p>路线一：打造多个专才模型</p>
<p></p><div style="text-align: center">
<img src="images/lec3/20.png" width="45%/"/>
</div>
<details class="example">
<summary>例子</summary>
<div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="" id="__tabbed_1_1" name="__tabbed_1" type="radio"/><input id="__tabbed_1_2" name="__tabbed_1" type="radio"/><div class="tabbed-labels"><label for="__tabbed_1_1">例<span class="heti-skip"><span class="heti-spacing"> </span>1<span class="heti-spacing"> </span></span></label><label for="__tabbed_1_2">例<span><span class="heti-spacing"> </span>2</span></label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p></p><div style="text-align: center">
<img src="images/lec3/22.png" width="60%/"/>
</div>
</div>
<div class="tabbed-block">
<p>基于<span class="heti-skip"><span class="heti-spacing"> </span>BERT<span class="heti-spacing"> </span></span>微调出用于不同任务的模型</p>
<p></p><div style="text-align: center">
<img src="images/lec3/23.jpg" width="70%/"/>
</div>
<p>各个<span class="heti-skip"><span class="heti-spacing"> </span>BERT<span class="heti-spacing"> </span></span>变体的表现：</p>
<p></p><div style="text-align: center">
<img src="images/lec3/24.png" width="80%/"/>
</div>
</div>
</div>
</div>
</details>
</li>
<li>
<p>路线二：直接打造一个通才</p>
<p></p><div style="text-align: center">
<img src="images/lec3/25.png" width="60%/"/>
</div>
<details class="example">
<summary>例子</summary>
<div class="tabbed-set tabbed-alternate" data-tabs="2:4"><input checked="" id="__tabbed_2_1" name="__tabbed_2" type="radio"/><input id="__tabbed_2_2" name="__tabbed_2" type="radio"/><input id="__tabbed_2_3" name="__tabbed_2" type="radio"/><input id="__tabbed_2_4" name="__tabbed_2" type="radio"/><div class="tabbed-labels"><label for="__tabbed_2_1">例<span class="heti-skip"><span class="heti-spacing"> </span>1<span class="heti-spacing"> </span></span></label><label for="__tabbed_2_2">例<span class="heti-skip"><span class="heti-spacing"> </span>2<span class="heti-spacing"> </span></span></label><label for="__tabbed_2_3">例<span class="heti-skip"><span class="heti-spacing"> </span>3<span class="heti-spacing"> </span></span></label><label for="__tabbed_2_4">例<span><span class="heti-spacing"> </span>4</span></label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p>李宏毅老师团队早在<span class="heti-skip"><span class="heti-spacing"> </span>19<span class="heti-spacing"> </span></span>年就做过相关的<a href="https://arxiv.org/abs/1909.03329v2">研究</a>（当时只有<span><span class="heti-spacing"> </span>GPT-2</span><heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。为了让模型成为通才，他们采用的方式是让模型一个任务一个任务地学（<a href="../ml/13.html">终身学习</a><span><span class="heti-spacing"> </span>(life-long learning)</span><heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。但这样做的一个问题是模型可能会忘记之前学过的任务。所以这个研究的一大贡献是设计了一种有趣的方法，让模型能够复习它学过的知识。</p>
<p></p><div style="text-align: center">
<img src="images/lec3/26.png" width="60%/"/>
</div>
</div>
<div class="tabbed-block">
<p><span>Google<span class="heti-spacing"> </span></span>在<span class="heti-skip"><span class="heti-spacing"> </span>21<span class="heti-spacing"> </span></span>年尝试打造过一个通才模型，叫做<a href="https://arxiv.org/abs/2109.01652"><span><span class="heti-spacing"> </span>FLAN</span></a><span><span class="heti-spacing"> </span>(Finetuned Langauge Net)</span>，包含了以下训练任务。</p>
<p></p><div style="text-align: center">
<img src="images/lec3/27.png" width="80%/"/>
</div>
<blockquote>
<p>注：一个月后，<span>Hugging Face<span class="heti-spacing"> </span></span>也开发了一个类似的模型<a href="https://arxiv.org/abs/2110.08207"><span><span class="heti-spacing"> </span>T0</span></a>。</p>
</blockquote>
<p>下图展示了<span class="heti-skip"><span class="heti-spacing"> </span>FLAN<span class="heti-spacing"> </span></span>的表现，可以看到它在各项任务中优于<span><span class="heti-spacing"> </span>GPT-3</span>：</p>
<p></p><div style="text-align: center">
<img src="images/lec3/28.png" width="70%/"/>
</div>
</div>
<div class="tabbed-block">
<p>之后，<span>Google<span class="heti-spacing"> </span></span>更进一步，用更多的任务（<span>1.8K<span class="heti-spacing"> </span></span>个任务）来训练模型（研究名为 <a href="https://arxiv.org/abs/2210.11416"><em>Scaling Instruction-Fine-tuned Language Models</em></a><heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。</p>
<p></p><div style="text-align: center">
<img src="images/lec3/29.png" width="70%/"/>
</div>
<p>实验结果如下：</p>
<p></p><div style="text-align: center">
<img src="images/lec3/30.png" width="70%/"/>
</div>
</div>
<div class="tabbed-block">
<p><span>OpenAI<span class="heti-spacing"> </span></span>也发表了相关文章<a href="https://arxiv.org/abs/2203.02155"><span><span class="heti-spacing"> </span>Instruct GPT</span></a>。它在<span class="heti-skip"><span class="heti-spacing"> </span>GPT-3<span class="heti-spacing"> </span></span>上做微调后（绿色柱子）的表现比<span class="heti-skip"><span class="heti-spacing"> </span>FLAN<span class="heti-spacing"> </span></span>还好。</p>
<p></p><div style="text-align: center">
<img src="images/lec3/31.png" width="40%/"/>
</div>
<p>之所以微调后的<span class="heti-skip"><span class="heti-spacing"> </span>GPT-3<span class="heti-spacing"> </span></span>表现更好，是因为<span class="heti-skip"><span class="heti-spacing"> </span>FLAN<span class="heti-spacing"> </span></span>训练数据产生的方式比较死板——采用自然语言推理的前提、假设的模板；而<span class="heti-skip"><span class="heti-spacing"> </span>GPT-3<span class="heti-spacing"> </span></span>是一个线上的服务，有真实用户，因此它可以拿真实用户使用语言模型的数据来训练。</p>
<p></p><div style="text-align: center">
<img src="images/lec3/32.png" width="70%/"/>
</div>
<hr/>
<p>该研究还揭示了一个道理：指令微调起到画龙点睛的作用。实际上指令微调无需大量数据，比如在这个研究中他们只用了一万多份数据：</p>
<p></p><div style="text-align: center">
<img src="images/lec3/33.png" width="70%/"/>
</div>
<p>之后也有类似的发现：</p>
<ul>
<li>
<p><a href="https://arxiv.org/abs/2307.09288">LLaMA2</a></p>
<p></p><div style="text-align: center">
<img src="images/lec3/34.png" width="70%/"/>
</div>
</li>
<li>
<p><a href="https://arxiv.org/abs/2305.11206">LIMA: Less Is More for Alignment</a>：只用<span class="heti-skip"><span class="heti-spacing"> </span>1k<span class="heti-spacing"> </span></span>样例，在<span class="heti-skip"><span class="heti-spacing"> </span>43%<span class="heti-spacing"> </span></span>的情况下其表现不低于<span><span class="heti-spacing"> </span>GPT-4</span></p>
</li>
</ul>
</div>
</div>
</div>
</details>
</li>
</ul>
<hr/>
<p>接着例<span class="heti-skip"><span class="heti-spacing"> </span>4<span class="heti-spacing"> </span></span>的发现——我们能否自己做指令微调呢？答案是不能，因为虽然这些研究表明数据量可以不大，但我们手边可没有高质量的训练数据啊。所以有些人想到了另一个办法：以<span class="heti-skip"><span class="heti-spacing"> </span>ChatGPT<span class="heti-spacing"> </span></span>为师，即对<span class="heti-skip"><span class="heti-spacing"> </span>ChatGPT<span class="heti-spacing"> </span></span>做逆向工程。下面是<a href="https://arxiv.org/abs/2212.10560"><span class="heti-skip"><span class="heti-spacing"> </span>Self-Instruct<span class="heti-spacing"> </span></span></a>给出的具体步骤：</p>
<div style="text-align: center">
<img src="images/lec3/35.png" width="70%/"/>
</div>
<p>当然，这种方法不一定有效，也有相关研究（<a href="https://arxiv.org/abs/2305.15717">The False Promise of Imitating Proprietary LLMs</a>）对其提出质疑。</p>
<p>另外，以<span class="heti-skip"><span class="heti-spacing"> </span>ChatGPT<span class="heti-spacing"> </span></span>为师还有一定的风险——你也许违背了<span class="heti-skip"><span class="heti-spacing"> </span>OpenAI<span class="heti-spacing"> </span></span>的用户使用条款。</p>
<div style="text-align: center">
<img src="images/lec3/36.png" width="70%/"/>
</div>
<h3 id="open-source-models">Open Source Models<a class="headerlink" href="#open-source-models" title="Permanent link">⚓︎</a></h3>
<p>现在即便有了来自<span class="heti-skip"><span class="heti-spacing"> </span>ChatGPT<span class="heti-spacing"> </span></span>的高质量的数据用于指令微调，但我们仍然没有自己的初始参数，因为<span class="heti-skip"><span class="heti-spacing"> </span>OpenAI<span class="heti-spacing"> </span></span>和<span class="heti-skip"><span class="heti-spacing"> </span>Google<span class="heti-spacing"> </span></span>的模型基本上是闭源的（<del>不过最近<span class="heti-skip"><span class="heti-spacing"> </span>OpenAI<span class="heti-spacing"> </span></span>公布了<a href="https://github.com/openai/gpt-oss">开源模型</a>，时隔<span class="heti-skip"><span class="heti-spacing"> </span>6<span class="heti-spacing"> </span></span>年终于<span class="heti-skip"><span class="heti-spacing"> </span>open<span class="heti-spacing"> </span></span>了</del><heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。这便是开源模型登场的时候了。下面就以<span class="heti-skip"><span class="heti-spacing"> </span>Meta<span class="heti-spacing"> </span></span>的<span class="heti-skip"><span class="heti-spacing"> </span>LLaMA<span class="heti-spacing"> </span></span>为例展开介绍（<del>课程开设于<span class="heti-skip"><span class="heti-spacing"> </span>24<span class="heti-spacing"> </span></span>年上半年，当时国内开源模型可能还没那么知名</del><heti-adjacent class="heti-adjacent-half">）</heti-adjacent>（<a href="https://arxiv.org/abs/2302.13971">LLaMA 1</a>，<a href="https://arxiv.org/abs/2307.09288">LLaMA 2</a><heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。</p>
<div style="text-align: center">
<img src="images/lec3/37.png" width="60%/"/>
</div>
<p>下面是一些基于<span class="heti-skip"><span class="heti-spacing"> </span>LLaMA<span class="heti-spacing"> </span></span>微调后的模型：</p>
<ul>
<li><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpaca</a></li>
<li><a href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna</a></li>
</ul>
<div style="text-align: center">
<img src="images/lec3/38.png" width="80%/"/>
</div>
<p>还有更多！</p>
<div style="text-align: center">
<img src="images/lec3/39.png" width="80%/"/>
</div>
<blockquote>
<p><a href="https://arxiv.org/abs/2303.18223">图片来源</a></p>
</blockquote>
<h2 id="rlhf">RLHF<a class="headerlink" href="#rlhf" title="Permanent link">⚓︎</a></h2>
<p>第三阶段是让模型进入实战，用到的技术是<strong>来自人类反馈的强化学习</strong>(RLHF)。这里的“人类反馈”就是我们在用大模型时看到模型输出下面的👍和👎。收集到足够多的反馈后，未来再对模型参数进行微调。</p>
<div style="text-align: center">
<img src="images/lec3/40.png" width="70%/"/>
</div>
<p><span>RL<span class="heti-spacing"> </span></span>大致要做的就是通过微调参数，让输出正确答案的概率更高。具体原理可阅读笔者之前的<a href="../ml/11.html">强化学习笔记</a>。</p>
<div style="text-align: center">
<img src="images/lec3/41.png" width="60%/"/>
</div>
<h3 id="insturction-fine-tuning-vs-rlhf">Insturction Fine-tuning v.s. RLHF<a class="headerlink" href="#insturction-fine-tuning-vs-rlhf" title="Permanent link">⚓︎</a></h3>
<p>从人类产生训练数据的角度看，<span>RLHF<span class="heti-spacing"> </span></span>相比指令微调更省力，因为后者需要大量标注数据，很耗人力；而前者只需对模型的输出提供反馈即可，相对比较轻松。</p>
<div style="text-align: center">
<img src="images/lec3/42.png" width="60%/"/>
</div>
<p>并且有时人类不容易构造正确答案，但是容易判断答案的好坏，这也是<span class="heti-skip"><span class="heti-spacing"> </span>RLHF<span class="heti-spacing"> </span></span>的一大优势。</p>
<div style="text-align: center">
<img src="images/lec3/43.png" width="60%/"/>
</div>
<hr/>
<p>另外，从机器学习的角度看：</p>
<ul>
<li>指令微调<ul>
<li>模型要学的就是如何接下一个字</li>
<li>假设每次接龙都是对的，那么模型生成的结果就会好</li>
<li>所以到目前为止，模型对于生成的结果没有全局的考虑</li>
</ul>
</li>
<li>RLHF<ul>
<li>模型进入新的“思考模式”</li>
<li>学习对生成结果做全局考虑</li>
<li>此时，即便每次接龙都是对的，结果也不一定是最好的</li>
</ul>
</li>
</ul>
<h3 id="language-model-vs-alphago">Language Model v.s. AlphaGo<a class="headerlink" href="#language-model-vs-alphago" title="Permanent link">⚓︎</a></h3>
<p>将语言模型和<span class="heti-skip"><span class="heti-spacing"> </span>AlphaGo<span class="heti-spacing"> </span></span>做比较，你会发现两者有不小的相似性。</p>
<div style="text-align: center">
<img src="images/lec3/46.png" width="50%/"/>
</div>
<p>展开来看：</p>
<div style="text-align: center">
<img src="images/lec3/47.png" width="70%/"/>
</div>
<blockquote>
<p>注：为了简化说明，此处不考虑多轮对话。</p>
</blockquote>
<p>所以，就从它们一连串的输出结果来看，两者看起来还是很像的。虽然<span class="heti-skip"><span class="heti-spacing"> </span>AlphaGo<span class="heti-spacing"> </span></span>下围棋的每一步是分类问题，但整体上看也是一种生成式学习。</p>
<div style="text-align: center">
<img src="images/lec3/48.png" width="60%/"/>
</div>
<p>语言模型和<span class="heti-skip"><span class="heti-spacing"> </span>AlphaGo<span class="heti-spacing"> </span></span>之所以有这些相似性，是因为：</p>
<ul>
<li>
<p><span>AlphaGo<span class="heti-spacing"> </span></span>根据棋谱学习，人类老师下哪里就跟着下哪里</p>
<p></p><div style="text-align: center">
<img src="images/lec3/49.png" width="60%/"/>
</div>
</li>
<li>
<p>语言模型在第一阶段（预训练）和第二阶段（指令微调）的时候，也是跟着人类老师说的做</p>
</li>
</ul>
<p><span>AlphaGo<span class="heti-spacing"> </span></span>如果仅凭棋谱学习的话，那也只能打赢一般的选手。如果需要击败顶尖高手的话，需要借助<span class="heti-skip"><span class="heti-spacing"> </span>RL<span class="heti-spacing"> </span></span>的力量。具体来说：</p>
<ul>
<li>如果<span class="heti-skip"><span class="heti-spacing"> </span>AlphaGo<span class="heti-spacing"> </span></span>完成“棋局接龙”后发现自己赢了，那就会得到正反馈，之后就要提高产生这些走法的概率</li>
<li>如果输的话就降低这些走法的概率</li>
</ul>
<blockquote>
<p>可直接从围棋规则中判断输赢。</p>
</blockquote>
<div style="text-align: center">
<img src="images/lec3/50.png" width="70%/"/>
</div>
<p>语言模型的<span class="heti-skip"><span class="heti-spacing"> </span>RL<span class="heti-spacing"> </span></span>也是类似的过程。不过反馈机制和下围棋很不一样，需要模型产生多个答案后，再由人类对答案的好坏做个排序，这也是比较常用的方式。之所以要模型产生多个答案，是因为仅看单个答案，人类很难判断其好坏。</p>
<div style="text-align: center">
<img src="images/lec3/51.png" width="70%/"/>
</div>
<h3 id="reward-model">Reward Model<a class="headerlink" href="#reward-model" title="Permanent link">⚓︎</a></h3>
<p>不过人类的时间精力有限，能提供的反馈也是有限的。所以对语言模型而言，如何有效利用人类反馈是一个关键的议题。一种做法是根据人类的反馈，创建一个虚拟的人类，语言模型在<span class="heti-skip"><span class="heti-spacing"> </span>RL<span class="heti-spacing"> </span></span>时就直接问这个虚拟的人类，而不是真的人类。这种“虚拟的人类”就是<strong>奖励模型</strong>(reward model)。</p>
<p>训练奖励模型的过程为：先取得一些人类的反馈；然后奖励模型将语言模型的答案作为输入，其输出应满足人类反馈的约束（比如答案<span class="heti-skip"><span class="heti-spacing"> </span>A<span class="heti-spacing"> </span></span>的分数大于答案<span class="heti-skip"><span class="heti-spacing"> </span>B<span class="heti-spacing"> </span></span>的分数<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。</p>
<div style="text-align: center">
<img src="images/lec3/52.png" width="70%/"/>
</div>
<p>训练好奖励模型后，人类就只采纳那个能在奖励模型中得分最高的那个答案。</p>
<div style="text-align: center">
<img src="images/lec3/53.png" width="60%/"/>
</div>
<hr/>
<p>但现在奖励模型更常见的用法是让语言模型直接根据奖励模型学习，即直接将语言模型的输出丢给奖励模型，看奖励模型给出什么样的分数。如果分数高，通过微调参数让语言模型输出这个答案的概率变大，反之则变小。</p>
<div style="text-align: center">
<img src="images/lec3/54.png" width="60%/"/>
</div>
<div style="text-align: center">
<img src="images/lec3/55.png" width="60%/"/>
</div>
<hr/>
<p>回到前面<a href="https://arxiv.org/abs/2203.02155"><span class="heti-skip"><span class="heti-spacing"> </span>Instruct GPT<span class="heti-spacing"> </span></span></a>的研究，当时我们没有考虑最高的那根柱子，它对应的就是<span class="heti-skip"><span class="heti-spacing"> </span>RL<span class="heti-spacing"> </span></span>后的模型表现。而右图的曲线图表明，即便是参数量较少的模型，在<span class="heti-skip"><span class="heti-spacing"> </span>RL<span class="heti-spacing"> </span></span>后其表现仍能胜过参数量大的模型。</p>
<div style="text-align: center">
<img src="images/lec3/56.png" width="70%/"/>
</div>
<p>不过也有<a href="https://arxiv.org/abs/2009.01325">研究</a>表明，模型过度向“虚拟人类”（奖励模型）学习是有害的（准确率不升反降<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。右图展示的是过度向“虚拟人类”学习后模型给出的摘要，可以看到输出的内容乱七八糟，且经常出现<span><span class="heti-spacing"> </span>"???"</span>，<span>"pls"<span class="heti-spacing"> </span></span>等符号（可能是奖励模型给出现这些符号的摘要打高分了<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。</p>
<div style="text-align: center">
<img src="images/lec3/57.png" width="70%/"/>
</div>
<p>事实上，今天<span class="heti-skip"><span class="heti-spacing"> </span>ChatGPT<span class="heti-spacing"> </span></span>很多不尽人意的表现，也有可能是过度跟“虚拟人类”学习的结果。John Schulman（<span>OpenAI<span class="heti-spacing"> </span></span>的联合创始人）在<span class="heti-skip"><span class="heti-spacing"> </span>ICML 2023<span class="heti-spacing"> </span></span>上的讲话指出了这个问题。</p>
<div style="text-align: center">
<img src="images/lec3/58.png" width="60%/"/>
</div>
<p>由于向“虚拟人类”学习方法的缺陷，因此有人尝试新的思路，发明了新的算法。下面是其中的一些研究成果，其有效性还尚待时间的验证。</p>
<ul>
<li>
<p><a href="https://arxiv.org/abs/2305.18290">DPO</a></p>
<p></p><div style="text-align: center">
<img src="images/lec3/59.png" width="80%/"/>
</div>
</li>
<li>
<p><a href="https://arxiv.org/abs/2402.01306">KTO</a></p>
<p></p><div style="text-align: center">
<img src="images/lec3/60.png" width="60%/"/>
</div>
</li>
</ul>
<hr/>
<p>未来（也许现在就可以<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>，当<span class="heti-skip"><span class="heti-spacing"> </span>AI<span class="heti-spacing"> </span></span>足够强大时，我们可以将<span><span class="heti-spacing"> </span>RLHF -&gt; RLAIF</span>，也就是让<span class="heti-skip"><span class="heti-spacing"> </span>AI<span class="heti-spacing"> </span></span>代替人类对语言模型提供反馈。</p>
<div style="text-align: center">
<img src="images/lec3/61.png" width="60%/"/>
</div>
<p>这里的<span class="heti-skip"><span class="heti-spacing"> </span>AI<span class="heti-spacing"> </span></span>也可以就是要进行<span class="heti-skip"><span class="heti-spacing"> </span>RL<span class="heti-spacing"> </span></span>的语言模型自身，因为语言模型具备反省的能力（<a href="2.html#magic-spell">上一讲</a>介绍过<heti-adjacent class="heti-adjacent-half">）</heti-adjacent>。</p>
<p>相关研究：</p>
<ul>
<li><a href="https://arxiv.org/abs/2212.08073/">https://arxiv.org/abs/2212.08073/</a></li>
<li><a href="https://arxiv.org/abs/2304.03277/">https://arxiv.org/abs/2304.03277/</a></li>
<li><a href="https://arxiv.org/abs/2309.00267/">https://arxiv.org/abs/2309.00267/</a></li>
<li><a href="https://arxiv.org/abs/2401.10020/">https://arxiv.org/abs/2401.10020/</a></li>
</ul>
<h3 id="difficulty">Difficulty<a class="headerlink" href="#difficulty" title="Permanent link">⚓︎</a></h3>
<p>强化学习的一大难题是：什么是“好”这件事没有一个标准的答案<heti-adjacent class="heti-adjacent-quarter">。</heti-adjacent>“好”的概念是多层次的，它既可以指有用性，也可以指安全性，还可以是其他的含义。</p>
<div style="text-align: center">
<img src="images/lec3/62.png" width="60%/"/>
</div>
<p><a href="https://arxiv.org/abs/2307.09288"><span>LLaMA<span class="heti-spacing"> </span></span>的做法</a>就是同时考虑有用性和安全性——训练了两个模型，分别考虑这两个指标。但不同的模型有不同的评估标准。</p>
<div style="text-align: center">
<img src="images/lec3/63.png" width="70%/"/>
</div>
<p>事实上有时人类自己也都无法判断好坏的状况。</p>
<div style="text-align: center">
<img src="images/lec3/64.png" width="70%/"/>
</div></div>
<aside class="md-source-file">
<span class="md-source-file__fact">
<span class="md-icon" title="最后更新">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"></path></svg>
</span>
<span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime" title="2025年8月23日 18:28:24">2025年8月23日 18:28:24</span>
</span>
<span class="md-source-file__fact">
<span class="md-icon" title="创建日期">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"></path></svg>
</span>
<span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime" title="2025年8月23日 18:28:24">2025年8月23日 18:28:24</span>
</span>
</aside>
<p style="font-size: 30px; font-weight: 600">评论区</p>
<div>
    如果大家有什么问题或想法，欢迎在下方留言~
  </div>
<!-- Insert generated snippet here -->
<script async="" crossorigin="anonymous" data-category="Announcements" data-category-id="DIC_kwDOMAb9Zs4CfmpP" data-emit-metadata="0" data-input-position="bottom" data-lang="zh-CN" data-mapping="pathname" data-reactions-enabled="1" data-repo="noughtq/notebook" data-repo-id="R_kgDOMAb9Zg" data-strict="0" data-theme="preferred_color_scheme" src="https://giscus.app/client.js">
</script>
<!-- Synchronize Giscus theme with palette -->
<script>
    var giscus = document.querySelector("script[src*=giscus]")

    // Set palette on initial load
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
      var theme = palette.color.scheme === "slate"
        ? "transparent_dark"
        : "light"

      // Instruct Giscus to set theme
      giscus.setAttribute("data-theme", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener("DOMContentLoaded", function() {
      var ref = document.querySelector("[data-md-component=palette]")
      ref.addEventListener("change", function() {
        var palette = __md_get("__palette")
        if (palette && typeof palette.color === "object") {
          var theme = palette.color.scheme === "slate"
            ? "dark"
            : "light"

          // Instruct Giscus to change theme
          var frame = document.querySelector(".giscus-frame")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            "https://giscus.app"
          )
        }
      })
    })
  </script>
<!-- 标题计数器 -->
<link href="/css/counter.css" rel="stylesheet"/>
<!-- 主页个性化 -->
</article>
</div>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div>
<button class="md-top md-icon" data-md-component="top" hidden="" type="button">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  回到页面顶部
</button>
</main>
<footer class="md-footer">
<nav aria-label="页脚" class="md-footer__inner md-grid">
<a aria-label="上一页: Prompt Engineering" class="md-footer__link md-footer__link--prev" href="2.html">
<div class="md-footer__button md-icon">
<svg viewbox="0 0 256 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L77.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z" fill="currentColor"></path></svg>
</div>
<div class="md-footer__title">
<span class="md-footer__direction">
                上一页
              </span>
<div class="md-ellipsis">
                Prompt Engineering
              </div>
</div>
</a>
<a aria-label="下一页: AI Agent" class="md-footer__link md-footer__link--next" href="4.html">
<div class="md-footer__title">
<span class="md-footer__direction">
                下一页
              </span>
<div class="md-ellipsis">
                AI Agent
              </div>
</div>
<div class="md-footer__button md-icon">
<svg viewbox="0 0 256 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z" fill="currentColor"></path></svg>
</div>
</a>
</nav>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright" style="margin-left: 33.5%">
<div class="md-copyright__highlight" style="text-align: center">
        Copyright © 2024-2025 <a href="https://github.com/NoughtQ">NoughtQ</a>
</div>
    
    
      Powered by
      <a href="https://www.mkdocs.org/" rel="noopener" target="_blank">
        MkDocs
      </a>
      with theme
      <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
        Material
      </a>
      modified by
      <a href="https://github.com/NoughtQ" rel="noopener" target="_blank">
        NoughtQ
      </a>
<!-- <br> -->
<div style="text-align: center;">
<a href="https://icp.gov.moe/?keyword=20252357" target="_blank">萌ICP备20252357号</a>
</div>
</div>
<div class="md-social">
<a class="md-social__link" href="https://noughtq.top" rel="noopener" target="_blank" title="noughtq.top">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M277.8 8.6c-12.3-11.4-31.3-11.4-43.5 0l-224 208c-9.6 9-12.8 22.9-8 35.1S18.8 272 32 272h16v176c0 35.3 28.7 64 64 64h288c35.3 0 64-28.7 64-64V272h16c13.2 0 25-8.1 29.8-20.3s1.6-26.2-8-35.1zM240 320h32c26.5 0 48 21.5 48 48v96H192v-96c0-26.5 21.5-48 48-48" fill="currentColor"></path></svg>
</a>
<a class="md-social__link" href="https://blog.noughtq.top" rel="noopener" target="_blank" title="blog.noughtq.top">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M224 24c0-13.3 10.7-24 24-24 145.8 0 264 118.2 264 264 0 13.3-10.7 24-24 24s-24-10.7-24-24c0-119.3-96.7-216-216-216-13.3 0-24-10.7-24-24M80 96c26.5 0 48 21.5 48 48v224c0 26.5 21.5 48 48 48s48-21.5 48-48-21.5-48-48-48c-8.8 0-16-7.2-16-16v-64c0-8.8 7.2-16 16-16 79.5 0 144 64.5 144 144s-64.5 144-144 144S32 447.5 32 368V144c0-26.5 21.5-48 48-48m168 0c92.8 0 168 75.2 168 168 0 13.3-10.7 24-24 24s-24-10.7-24-24c0-66.3-53.7-120-120-120-13.3 0-24-10.7-24-24s10.7-24 24-24" fill="currentColor"></path></svg>
</a>
<a class="md-social__link" href="https://github.com/noughtq" rel="noopener" target="_blank" title="github.com">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg>
</a>
<a class="md-social__link" href="mailto:noughtq666@gmail.com" rel="noopener" target="_blank" title="">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m20 8-8 5-8-5V6l8 5 8-5m0-2H4c-1.11 0-2 .89-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../..", "features": ["content.action.edit", "content.action.view", "content.code.copy", "content.code.annotate", "content.footnote.tooltips", "navigation.tabs", "navigation.top", "navigation.footer", "navigation.indexes", "navigation.tracking", "navigation.prune", "search.share"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script>
<script src="../../assets/javascripts/bundle.92b07e13.min.js"></script>
<script src="../../js/anchor.js"></script>
<script src="../../js/katex.js"></script>
<script src="../../js/toc.js"></script>
<script src="../../js/typed.js"></script>
<script src="../../js/custom.js"></script>
<script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
<script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
</body>
</html>