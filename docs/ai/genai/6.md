---
counter: true
---

# Evaluation

本讲将介绍和语言模型能力评估相关的话题。首先来看评估的基本思路：对于语言模型 A 和 B，喂给它们相同的输入，看它们给出的答案是否和标准答案接近。其中这组输入和对应的标准答案被称为**基准测试集**(benchmark corpus)。

<div style="text-align: center">
    <img src="images/lec6/1.png" width=70%/>
</div>


## MMLU

看似这个过程很简单，但存在一个问题：如何根据标准答案决定输出是否正确？因为模型可以给出不同的答案（即便是同一个模型），并且有些题目可能答案不唯一。那么有人会想，考选择题总不会有问题吧？确实有人构造了一个仅包含选择题的测试集——[**MMLU**](https://arxiv.org/abs/2009.03300)（全称大规模多任务语言理解(massive multitask language understanding)），里面包含了各种学科的题目。下面是从中挑选出来的一些题目：

<div style="text-align: center">
    <img src="images/lec6/2.png" width=70%/>
</div>

但很奇怪的是，在不同研究中（下图不同列），同一模型在 MMLU 中的得分相差很大：

<div style="text-align: center">
    <img src="images/lec6/3.png" width=60%/>
</div>

所以就连选择题都很难好衡量模型能力的高低。也许可以这样做：除了输入题目外，还要提醒模型只输出选项，不要输出其他内容。

<div style="text-align: center">
    <img src="images/lec6/4.png" width=60%/>
</div>

但是这种提示工程上的方法不见得完全奏效——有些模型的能力很强，但就是不太听话，喜欢直接输出答案而非选项。所以加上这种约束的时候同时还考验模型听话的程度。另外，如果模型输出的分布如下，我们应该算它答对还是答错呢？

<div style="text-align: center">
    <img src="images/lec6/5.png" width=60%/>
</div>

- 认为它对，因为 ABCD 四个选项中，B 的概率最大
- 认为它错，因为所有概率中，3 的概率最大，也就是说模型最希望输出 3

有[研究](https://arxiv.org/abs/2309.03882)发现，即便改一下选项的位置，模型在所有题目上的正答率也会发生变化（比如有些模型可能就喜欢猜 A 选项）。

<div style="text-align: center">
    <img src="images/lec6/6.png" width=70%/>
</div>


## Evaluation by Human

连选择题都很难评估好模型，那么像翻译、摘要等没有单一标准答案的问题模型就更难评估了，因为和标准答案不一样并不代表它就是错的。

<div style="text-align: center">
    <img src="images/lec6/7.png" width=70%/>
</div>

而过去的基准测试集 BLEU，ROUGE 等都是做字面的比对，因此不太可靠。所以或许可以靠我们人类自己来评估模型好坏——有一个叫做 [Chatbot Arena](https://chat.lmsys.org/)（聊天机器人竞技场）的地方为我们提供了一个给模型打分的功能，如下所示：

<div style="text-align: center">
    <img src="images/lec6/8.png" width=70%/>
</div>

并且它还提供了一个排行榜（至少每天都是更新的，不知道是不是实时的）。下面是 2025/08/08 的排行榜：

<div style="text-align: center">
    <img src="images/lec6/9.png" width=100%/>
</div>


## Evaluation by Powerful LM

我们还可将强大的语言模型作为评委来评估其他语言模型的能力：

<div style="text-align: center">
    <img src="images/lec6/10.png" width=70%/>
</div>

相关研究：

- <https://arxiv.org/abs/2305.01937/>
- <https://arxiv.org/abs/2310.05657/>

有一个知名的基准测试集 [MT-Bench](https://arxiv.org/abs/2306.05685) 就是用语言模型（GPT-4）来评估的。之所以它要用语言模型来评估，是因为里面的问题没有一个是有标准答案的，比如：

<div style="text-align: center">
    <img src="images/lec6/11.png" width=70%/>
</div>

那么使用语言模型评估到底准不准呢？如[下图](https://github.com/tatsu-lab/alpaca_eval)所示，MT-Bench 上的分数（右二）和人类打分非常接近（相关程度 0.94）。另外这张图也显示了 MMLU 的相关程度，虽然差了一点但也挺高的，所以在 MMLU 上使用语言模型也许也是可行的做法。

<div style="text-align: center">
    <img src="images/lec6/12.png" width=70%/>
</div>

有人担心大语言模型本身会不会偏袒特定类型的答案。一个比较知名的偏见就是模型偏好较长的答案。所以原来有个叫做 AlpacaEval 的测试集（上图右3）在后来做了个[改版](https://arxiv.org/abs/2404.04475)，考虑了输出长度（经常输出太长的答案要倒扣一些分数），所以与人类的评价更加接近了（上图右一）。

开发 MT-Bench 的团队后来又搞了个测试集 [Arena-Hard](https://lmsys.org/blog/2024-04-19-arena-hard/)，它的相关程度又提高了一些，不过具体细节这里就不再阐述。


## Big-bench

在评估语言模型能力的时候，除了要决定如何根据标准答案判断输出的正确与否外，还有一个议题是我们要向模型问什么。为此我们需要准备各式各样的任务，下面就整理了一些在 NLP 领域中知名的基准测试集：

<div style="text-align: center">
    <img src="images/lec6/13.png" width=70%/>
</div>

接下来重点介绍红框标出来的测试集 [Big-bench](https://github.com/google/BIG-bench)。它里面包含很多奇奇怪怪的任务，比如：

- 根据表情符号猜电影名（上面两组表情符号留给读者来猜）

    <div style="text-align: center">
        <img src="images/lec6/14.png" width=60%/>
    </div>

- 国际象棋下一步该怎么走

    <div style="text-align: center">
        <img src="images/lec6/15.png" width=60%/>
    </div>

    这个例子中没有一个模型给出正确答案（黄线）。不过值得注意的是，绿色实线表示大模型给出的答案，绿色虚线表示小模型给出的答案，可以看到大模型的答案更加接近正确答案，而小模型可能连国际象棋的规则都无法好好遵守。

- ASCII 词汇识别（字符画识别）

    <div style="text-align: center">
        <img src="images/lec6/16.png" width=60%/>
    </div>


## Reading in Long Context

接下来看如何评估模型阅读长文的能力。具体来说，我们会把关键信息放在上下文的不同位置上，看模型是否能利用这段信息给出正确答案。之所以这么做，是因为不同模型关注上下文的位置可能是不一样的，比如只记得开头结尾而忘记中间部分。

<div style="text-align: center">
    <img src="images/lec6/17.png" width=70%/>
</div>

有人构造了[“大海捞针”(needle in a haystack)测试](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)，就是用这种思路来评估模型阅读长文的能力。下图展示了部分模型的测试结果，其中横轴表示上下文长度，纵轴表示关键信息所在位置：

- GPT-4：在上下文较长时，位于 10%-50% 左右的信息很容易被遗忘

    <div style="text-align: center">
        <img src="images/lec6/18.png" width=80%/>
    </div>

- Claude 2.1：号称拥有很强的上下文阅读能力，但表现相当糟糕

    <div style="text-align: center">
        <img src="images/lec6/19.jpg" width=80%/>
    </div>

对这样的实验结果，Anthropic 表示不满，给出了自己的[解释](https://www.anthropic.com/news/claude-2-1-prompting)：如果像下图那样，在提示词的最后加上这么一句话，模型阅读上下文就特别棒了：

<div style="text-align: center">
    <img src="images/lec6/20.png" width=70%/>
</div>


## MACHIAVELLI Benchmark

有一个叫做[马基维利基准测试](https://arxiv.org/abs/2304.03279)(MACHIAVELLI Benchmark)的测试集，让语言模型玩文字冒险游戏，用于评估模型是否为达成目的不择手段。下图展示了它的大致原理：模型根据当前目标和观察，可以采取不同的行动。每个行动包含了不同的奖励分数，但同时也会评估它的道德水平（有些行为是违反道德的）。

<div style="text-align: center">
    <img src="images/lec6/21.png" width=70%/>
</div>

下面给出实验结果。可以看到，作者他们自己训练的 DRRN 模型专门用于得高分，但是道德水平是负的；而 GPT-4 虽然得分不高，但它有起码的道德底线。另外，如果给 GPT-4 一些和道德相关的提示词，并且不要提前告知其目标，它的道德水平还可以得到进一步的提升。

<div style="text-align: center">
    <img src="images/lec6/22.png" width=40%/>
</div>


## Theory of Mind

心理学上有一个术语叫做**心智理论**(theory of mind)，简单来说就是揣测他人想法的能力。有人研究了机器是否具备心智理论能力，其中比较知名的测试是 Sally-Anne 测试，题目如下（下面是 GPT-3.5 的回答，答案是正确的）：

<div style="text-align: center">
    <img src="images/lec6/23.png" width=50%/>
</div>

有人想，也许是因为这个题目在训练数据中出现过，所以它已经知道该如何回答了。因此下面将 Sally 和 Anne 换成其他人，但题目逻辑没有发生变化，来看模型回答如何。

<div style="text-align: center">
    <img src="images/lec6/24.png" width=50%/>
</div>

GPT-3.5 出错了，说明它实际上还没有理解这个问题。让 TAIDE 模型回答这个问题，它也给出了模棱两可的答案：

<div style="text-align: center">
    <img src="images/lec6/25.png" width=70%/>
</div>

---
另外一个更复杂的实验是：先由两人对话，然后第三人加入后续对话，最后问模型相关问题。其中一个涉及到心智理论的问题是第三人关于 ta 加入对话前两人对话内容的看法，正确回答应该是 ta 不知道。

<div style="text-align: center">
    <img src="images/lec6/27.png" width=80%/>
</div>

下面展示了各模型的表现，可以看到所有模型的表现均不理想，说明它们对心智的理解远没我们想象的那么强：

<div style="text-align: center">
    <img src="images/lec6/26.png" width=50%/>
</div>

---
相关研究：

- <https://arxiv.org/abs/2303.12712/>
- <https://arxiv.org/abs/2302.02083/>


## Problem

不要轻易相信基准测试的结果，因为：

- 语言模型在训练的时候可能已经偷偷收集了和基准测试类似的题目
    - <https://arxiv.org/abs/2311.04850/>
    - <https://lmsys.org/blog/2023-11-14-llm-decontaminator//>

    <div style="text-align: center">
        <img src="images/lec6/28.png" width=70%/>
    </div>

- 甚至可能已经看过基准测试中的数据
    - [相关文章](https://arxiv.org/abs/2312.16337)

    <div style="text-align: center">
        <img src="images/lec6/29.png" width=70%/>
    </div>

有[研究](https://arxiv.org/abs/2312.16337)证实了部分模型确实偷看过基准测试的数据。下面列出了专门用于问模型是否看过基准测试数据的提示词，以及实验结果：

<div style="text-align: center">
    <img src="images/lec6/30.png" width=80%/>
</div>


## Other Aspects

评估模型能力的其他方面包括价格、速度等等，这里不会详细介绍，感兴趣的读者可自行研究。

<div style="text-align: center">
    <img src="images/lec6/31.png" width=80%/>
</div>

>[图片来源](https://artificialanalysis.ai/)