---
counter: true
---

# Lec 11: Approximation

## Introduction

回顾上一讲的NP完全问题的话题——在实际情况下，我们该怎么解决这类问题：

- 如果数据规模$N$很小，即使时间复杂度为指数级也是可以接受的
- 如果某些特殊情况能够在多项式时间内完成，那就先处理这些情况
- 设计一种算法，保证能够在多项式时间内得到接近最优解。这一类算法便是接下来要介绍的**近似算法**(approximation algorithm)。

我们一般通过**近似比**(approximation ratio)$\rho(n)$来衡量近似算法的好坏，称这样的近似算法为**$\rho(n)$-近似算法**。它的意义是：对于任意规模为$n$的输入，算法得到一个解所花的成本$C$不会超过得到最优解所花成本$C^*$的$\rho(n)$倍，即：

$$
\max(\dfrac{C}{C^*}, \dfrac{C^*}{C}) \le \rho(n)
$$

最优解的**近似方案**(approximation scheme)指的是一类近似算法，它不仅能够将输入作为问题的实例，而且对于任意给定值$\varepsilon > 0$，它是一个$(1 + \varepsilon)$近似算法。

- **多项式时间近似方案**(polynomial-time approximation scheme, PTAS)：对于任意给定值$\varepsilon > 0$，当输入实例规模为$n$时，该方案能在多项式时间内完成计算，时间复杂度可记为$O(n^{f(\frac{1}{\varepsilon})})$
    - **满多项式时间近似方案**(fully polynomial-time approximation scheme, FPTAS)：在PTAS的基础上，要求该方案的运行时间关于$n$和$\varepsilon$都是多项式级的，时间复杂度可记为$O(n^{O(1)} (\dfrac{1}{\varepsilon})^{O(1)})$

设计算法的时候需要考虑以下几方面的问题：

- **最优性**(optimality)：解的质量
- **效率**(efficiency)：计算的成本
- **全部实例**(all instances)

我们对这几个方面进行组合，可以得到不同的算法类型：

- A+C：针对全部实例的精确算法
- A+B：对于特殊情况的精确且快速的算法
- B+C：近似算法

即使$P = NP$，我们仍然无法保证存在A+B+C的算法解决此类问题。

## Bin Packing

!!! question "问题描述"

    给定$N$个大小分别为$S_1, S_2, \dots, S_N$的物品，满足$\forall\ 1 \le i \le N, 0 < S_i \le 1$，并假设有若干个容量为1的桶。现在请你求出能够装下所有物品的最小桶数。

    ???+ example "例子"

        === "题目"

            $N = 0.7; S_i = 0.2, 0.5, 0.4, 0.7, 0.1, 0.3, 0.8$

        === "答案"

            最优解为：

            <div style="text-align: center">
                <img src="images/lec11/1.png" width="40%">
            </div>

然而，这个看似简单的问题实际上是一个**NP困难问题**。因此，下面给出几种解决该问题的近似算法。

>该问题的变种（决策问题）：给定$K$个桶，我们能否装下$N$个物品。这是个NP完全问题。

!!! note "近似算法"

    === "Next Fit"

        伪代码如下：

        ```c
        void NextFit() {
            read item1;
            while (read item2) {
                if (item2 can be packed in the same bin as item1)
                    place item2 in the bin;
                else
                    create a new bin for item2;
                item1 = item2;
            } // end-while
        }
        ```
        过程很简单，就是按输入顺序一个个放物品，如果当前物品能够放在与上个物品相同的桶内，那就放进去，否则放在新的桶内。

        时间复杂度：$O(N)$

        定理：令$M$为装下这些物品的最优解桶数，那么该方法所得桶数不超过$2M - 1$，并且存在某种输入使得桶数正好为$2M - 1$。因此该算法是一个2-近似算法。

        ??? proof "证明"

            原定理有以下等价命题：如果该算法能得到$2M$或$2M + 1$个桶，那么最优解至少是$M + 1$个桶。只要证明这个命题是正确的，就能说明定理的正确性，下面继续证明。

            令$S(B_i)$为第$i$个桶所装物品的大小，那么可以得到：

            $$
            \begin{align}
            S(B_1) + S(B_2) & > 1 \notag \\
            S(B_3) + S(B_4) & > 1 \notag \\
            \dots \notag \\
            S(B_{2M - 1}) + S(B_{2M}) & > 1 \notag \\
            \end{align}
            $$

            累加得：$\sum\limits_{i = 1}^{2M} S(B_i) > M$

            因为最优解能得到至少$\lceil \dfrac{\text{total size of all the items}}{1}\rceil$个桶，即$\lceil \sum\limits_{i = 1}^{2M} S(B_i) \rceil$，因此最终得到：$\lceil \sum\limits_{i = 1}^{2M} S(B_i) \rceil \ge M + 1$，即该命题是正确的，因此定理是正确的，得证。

        ??? info "补充"

            假设最大物体的大小$< \alpha < 1$，令$NF(L)$和$OPT(L)$分别表示通过Next Fit得到的桶数和最优解的桶数。那么对于任意的物品组$L$，我们有结论：$NF(L) < \rho \cdot OPT(L) + 1$，并且：

            - $\rho = \dfrac{1}{1 - \alpha}(0 \le \alpha \le \dfrac{1}{2})$
                - 简单解释一下：由于$\alpha < \dfrac{1}{2}$，所以除最后一个桶外，每个桶至少会盛放$1 - \alpha$大小的物品（否则的话还可以桶内还可以再放入其他物品），那么可以得到$OPT(L) > (1 - \alpha) (NF(L) - 1)$，稍微计算以下便可以得到上面的结论
            - $\rho = 2(\alpha > \dfrac{1}{2})$
                - 这个实际上已在上面给出了证明

    === "First Fit"

        伪代码如下：

        ```c
        void FirstFit() {
            while (read item) {
                scan for the first bin that is large enough for item;
                if (found)
                    place item in that bin;
                else
                    create a new bin for item;
            } // end-while
        }
        ```

        该策略是：对于当前物品，找到第一个现存的能够容得下它的桶，如果不存在这样的桶再添加新的桶。

        时间复杂度：$O(N\log N)$（循环内扫描桶的时间复杂度优化至$O(\log N)$）

        定理：令$M$为装下这些物品的最优解桶数，那么该方法所得桶数不超过$\dfrac{17 M}{10}$，并且存在某种输入使得桶数正好为$\dfrac{17(M - 1)}{10}$。因此该算法是一个1.7-近似算法。

    === "Best Fit"

        该策略是：将当前物品放入现存的、容得下它的、且放入该物品后剩余空间是最小的桶内，如果不存在这样的桶再添加新的桶。

        时间复杂度：$O(N \log N)$

        与First Fit类似，它也是一个1.7-近似算法。

??? example "例子"

    === "例1"

        === "题目"

            同“问题描述”部分给出的题目，现在要求你用上面讲到过的三种近似算法给出近似解。

        === "答案"

            <div style="text-align: center">
                <img src="images/lec11/2.png" width="80%">
            </div>

    === "例2"

        === "题目"

            假设有以下物品：

            $$
            \begin{align}
            S_i = & \dfrac{1}{7} + \varepsilon, \dfrac{1}{7} + \varepsilon, \dfrac{1}{7} + \varepsilon, \dfrac{1}{7} + \varepsilon, \dfrac{1}{7} + \varepsilon, \dfrac{1}{7} + \varepsilon, \notag \\ & \dfrac{1}{3} + \varepsilon, \dfrac{1}{3} + \varepsilon, \dfrac{1}{3} + \varepsilon, \dfrac{1}{3} + \varepsilon, \dfrac{1}{3} + \varepsilon, \dfrac{1}{3} + \varepsilon, \notag \\ & \dfrac{1}{2} + \varepsilon, \dfrac{1}{2} + \varepsilon, \dfrac{1}{2} + \varepsilon, \dfrac{1}{2} + \varepsilon, \dfrac{1}{2} + \varepsilon, \dfrac{1}{2} + \varepsilon \notag
            \end{align}
            $$

            其中$\varepsilon = 0.001$，请比较它们的最优解和近似解（用上面给出的3种算法计算）。

        === "答案"

            - 最优解：6个桶（每列物品放入一个桶，共6列）
            - 近似解：10个桶（三种近似算法得到相同的近似解）

            因此对于本题，近似算法的效果并不是很理想。

上述的三种近似算法实际上是一种**在线算法**(on-line algorithm)。对于本题，它的意思是：在处理下一个物品前就要放好当前物品，且**不能改变**当前的决策。

由于在线算法无法得知输入何时结束，因此始终无法得到最优解。具体来说，有以下定理：对于本题的所有近似算法，得到的近似解桶数至少是最优解桶数$\dfrac{5}{3}$倍。

因此，我们需要采用**离线算法**(off-line algorithm)来提升近似的准确度，这种算法的思路是：在得到答案前需要检查所有的物品。

- 先找到问题的trouble maker：规模较大的物品
- 解决方案——first/best fit decreasing：先将物品按大小的非递增顺序排序，然后应用first fit或best fit近似算法（两者近乎等价）求解。不难发现，该方法用到了**贪心算法**的思想。

??? example "例子"

    === "题目"

        同“问题描述”部分给出的题目。

    === "答案"

        !!! play "动画演示"

            <div style="text-align: center">
                <img src="images/lec11/3.gif" width="60%">
            </div>

定理：令$M$为装下这些物品的最优解桶数，那么该方法（first fit decreasing）所得桶数不超过$\dfrac{11 M}{9} + \dfrac{6}{9}$，并且存在某种输入使得桶数正好为$\dfrac{11 M}{9} + \dfrac{6}{9}$。

还有以下结论（其中$FFD(L)$表示的是对于物品组$L$，用first fit decreasing算法得到的桶数）：

- $FFD(L) \le \dfrac{3}{2} OPT(L)$
- $\dfrac{3}{2}$是最小的因数，除非可以证明P=NP

??? proof "证明"

    === "证明结论1"

        - 根据前面的定理，我们知道$\forall\ L, FFD(L) \le \dfrac{11}{9}OPT(L) + 1$（这里用的是更宽松的条件）。可以先计算$\dfrac{11}{9}OPT(L) + 1 \le \dfrac{3}{2} OPT(L)$，因为如果这个不等式成立的话，结论1一定成立。解得$OPT \ge 4$，所以接下来只需考虑$OPT = 1, 2, 3$的情况即可
        - $OPT = 1$：$FFD = 1$，显然成立
        - $OPT = 2$：使用定理的不等式，得到$FFD \le \dfrac{11}{9} \times 2 + 1 = \dfrac{31}{9}$，由于$FFD$是整数，因此$FFD \le 3$，因此成立
        - $OPT = 3$：使用定理的不等式，得到$FFD \le \dfrac{11}{9} \times 3 + 1 = \dfrac{42}{9}$，由于$FFD$是整数，因此$FFD \le 4$，因此成立
        - 综上所示，结论1成立

    === "证明结论2"

        >不太想整了，逃x


## Knapsack Problems

### Fractional Version

!!! question "问题描述"

    !!! info "注"

        这里的“背包问题(knapsack problem)”是小数版本的多重背包问题，不是在DP那一讲中提到的那几种简单类型。如果对这类问题有些遗忘的话，建议[回顾](8.md#knapsack-problem)一下。

    令背包容量为$M$，给定$N$类物品，每类物品$i$的重量为$w_i$，价值为$p_i$，被放进背包的比例为$x_i \in [0, 1]$（因此该类物品的总价值为$p_i x_i$）。

    很明显，该问题的最优解即为背包所装物品的最大价值。也就是说，在满足$\sum\limits_{i=1}^n w_i x_i \le M$的限制条件下，令$\sum\limits_{i=1}^np_ix_i$最大。

- 在每个阶段中，我们需要将一类物品放入背包内。
- 对于本题，我们尝试用贪心法来解决，所采取的贪心策略是：按价值密度(profit density)大小$\dfrac{p_i}{w_i}$的降序挑选物品。


??? example "例子"

    === "题目"

        已知$n = 3, M = 20, (p_1, p_2, p_3) = (25, 24, 15), (w_1, w_2, w_3) = (18, 15, 10)$，计算背包的最大价值。

    === "答案"

        $(x_1, x_2, x_3) = (0, 1, \dfrac{1}{2})$，此时最大价值$P = 31.5$


### 0-1 Version

现在回到[0-1背包问题](8.md#0-1KnapsackProblem) ——它在上面一类背包问题的基础上多了一条限制：$x_i = 0 \text{ or } 1$。事实上，这个看似较为简单的问题，它竟是一个NP困难问题。下面我们对这类问题进行更进一步的分析。


??? example "例子"
            
    === "题目"

        已知$n = 5, M = 11, (p_1, p_2, p_3, p_4, p_5) = (1, 6, 18, 22, 28), (w_1, w_2, w_3, w_4, w_5) = (1, 2, 5, 6, 7)$，计算背包的最大价值。

    === "答案"

        - 最优解：$(0, 0, 1, 1, 0), P = 40$
        - 贪心解：$(1, 1, 0, 0, 1), P = 35$
            - 对于本例，无论是采取选最大价值密度的策略，还是采取选最大价值的策略，结果都是一样的
    
从上面的例子中，我们可以发现贪心法在0-1背包问题中并不能成功找到最优解，因而是一种近似算法。下面来证明它实际上是一个2-近似算法：

??? proof "证明"

    通过已知条件，可以得到下列不等式：

    $$
    \begin{align}
    p_{\text{max}} & \le P_{\text{opt}} \le P_{\text{frac}} \notag \\
    p_{\text{max}} & \le P_{\text{greedy}} \notag \\
    P_{\text{opt}} & \le P_{\text{greedy}} + p_{\text{max}} \notag
    \end{align}
    $$

    其中，$p_{\text{max}} = \max\limits_{1 \le i \le n}\{p_i\}，$P_{\text{opt}}$表示本题的最优解，$P_{\text{frac}}$表示上一类背包问题的解，$P_{\text{greedy}}$表示本题的贪心解。

    - 第一个不等式：左边的不等号显然成立，右边的不等号是因为分数版本的背包问题可以取部分物品，那么它一定能够在0-1背包的基础上，通过塞入部分物品将背包塞满，所以分数背包的解一定不小于0-1背包的最优解
    - 第二个不等式也是显然成立的
    - 第三个不等式：不等号两边同时减去$P_{\text{greedy}}$，即最优解与贪心解之差一定不超过最大价值
    >注：目前我还没想清楚该怎么解释第三个不等式

    根据这三个不等式，可以推出：

    $$
    \dfrac{P_{\text{opt}}}{P_{\text{greedy}}} \le 1 + \dfrac{p_{\text{max}}}{P_{\text{greedy}}} \le 2
    $$

    再根据近似比的定义，便可得到近似比为2。

??? info "补充：更厉害的近似算法"

    >来自zgc先生的[paper](https://www.sciencedirect.com/science/article/pii/S0304397512007694?ref=pdf_download&fr=RR-2&rr=8e8891961ab4050c)

所以，用贪心法解0-1背包问题的效果不是很理想。通常的解法便是前面介绍过的[动态规划](8.md)，这里再介绍一种通过分析最小重量求解的dp算法：

- 令$W_{i, p}$为物品1到物品$i$之间的最小质量，总价值为$p$
- 分类讨论：
    - 取物品$i$：$W_{i, p} = w_i + W_{i - 1, p - p_i}$
    - 不取物品$i$：$W_{i, p} = W_{i - 1, p}$
    - 不可能得到价值$p$：$W_{i, p} = \infty$
- 状态转移方程为：

$$
W_{i, p} = \begin{cases}\infty & i = 0 \\ W_{i - 1, p} & p_i > p \\ \min\{W_{i - 1, p}, w_i + W_{i - 1, p - p_i}\} & \text{otherwise}\end{cases}
$$

- 其中，$i = 1, \dots, n, p = 1, \dots, np_{\text{max}}$，因此时间复杂度为$O(n^2 p_{\text{max}})$

!!! question "思考"

    === "问题1"

        === "问题"

            诶，看起来用dp能够在多项式时间内求解0-1背包问题，为什么说0-1背包问题是一个NP困难问题呢？

        === "解答"

            所谓的“多项式时间内”，指的是关于输入数据规模$n$的多项式。而上面给出的时间复杂度中还有一项$p_{max}$，它与数据规模$n$独立，因此这个数可以很大很大，超出$n$的指数级倍。因此，我们无法保证dp解法能够在多项式时间内产生解。

    === "问题2"

        === "问题"

            那如果我们假定$w_i \le N^2$，那么0-1背包问题是否还是NP困难问题呢？

        === "解答"

            No！根据我们在[动态规划](8.md#0-1-knapsack-problem)一节中提到的算法，它的时间复杂度为$O(N \cdot cap)$。如果有了问题中的假定，那么不难得到$M \le N^3$，那么总的时间复杂度就是$O(N^4)$，因此此时0-1背包问题就不是NP困难问题了。

??? example "例子"

    === "题目"

        现在就来分析上述的极端情况：以下面给出的数据为例，当$n$不大，$p_{\text{max}}$非常大（~~其实$p_i$都蛮大的~~）时，你会怎么做呢？

        <div style="text-align: center">
            <img src="images/lec11/4.png" width="40%">
        </div>

    === "答案"

        一种（也许）可行的做法是只保留$p_i$的高位，但保证能够区分这些$p_i$的大小：

        <div style="text-align: center">
            <img src="images/lec11/5.png" width="40%">
        </div>       

        - 但这种做法会损失价值的精度，且如果所有价值的位数一致时，这种做法就不太有效了
        - 对于上述方法，有$(1 + \varepsilon) P_{\text{alg}} \le P$，其中$\varepsilon$为精度参数
        - 我想到一种改进方法：将这些价值先排个序（升序），然后将这些价值与对应的索引建立一个双射关系，在状态转移方程的计算中用索引替代价值，最后再还原为真实的价值作为最大价值


## K-center Problems

!!! question "问题描述"

    给定$n$个地址$s_1, \dots, s_n$，在地图上选择$K$个中心点$c$，使任意地址到离它距离最近的中心点之间的距离中的最大值最小化。

    <div style="text-align: center">
        <img src="images/lec11/6.png" width="60%">
    </div>  

    ---
    这么说可能不太好理解，下面给出符号化的定义：

    本问题提到的距离(distance)不同于图论中边的权重，实际上它就是数学上本来的意思：

    - 同一性(identity)：$dist(x, x) = 0$
    - 对称性(symmetry)：$dist(x, y) = dist(y, x)$
    - 三角不等式(triangle inequality)：$dist(x, y) \le dist(x, z) + dist(z, y)$

    令：
    
    - $dist(s_i, C) = \min\limits_{c \in C} \{dist(s_i, c)\}$，即$s_i$到最近中心点间的距离
    - $r(C) = \max\limits_{i} \{dist(s_i, C)\}$，即所有中心点中最大的最小覆盖半径

    目标：找到一组中心点集$C$，使得$r(C)$最小化，且保证$|C| = K$（$K$为常数）

由于平面是无限大的，穷举法显然是不可能的。下面介绍一种贪心策略：

- 让第一个中心点作为所有地址的中点（就好像只能放一个中心点）
- 随后加入的中心点满足能够减少$r(C)$值的条件

<div style="text-align: center">
    <img src="images/lec11/7.png" width="40%">
</div>  

???+ bug "问题"

    如图所示，假设整个点集包括两个相距很远的子集，且$K = 2$。此时第一个中心点就会被放在两个子集的中间，但最优解应该是中心点位于子集的中间位置的时候，所以这种贪心策略就失效了。

    <div style="text-align: center">
        <img src="images/lec11/8.png" width="60%">
    </div>  

---
因此贪心法需要进一步的改进，先给出伪代码：

```c
Centers Greedy-2r(Sites S[], int n, int K, double r) {
    Sites S`[] = S[];  // S` is the set of the remaining sites
    Centers C[] = NULL;
    while (S`[] != NULL) {
        Select any s form S` and add it to C;
        Delete all s` from S` that are at dist(s`, s) <= 2r;
    } // end-while
    if (|C| <= K) 
        return C;
    else
        ERROR("No set of K centers with covering radius at most r");
}
```

用文字解释该算法的流程：

1. 预备知识：在改进的贪心算法中，我们直接挑选某个地址作为中心点。这种做法之所以可行，是因为某个中心点覆盖半径为$r$的区域，可以近似为以（接近）区域边界上一点$s$为新的中心点，$2r$为半径的区域。虽然这个区域明显比原区域大，但它保证能够覆盖原区域所能覆盖的点。这样的话我们就不必通过繁琐的计算算出中心点，而是从原有的地址中选择中心点，这样就方便了很多。下图很好的说明了这一点：

    <figure style=" width: 70%" markdown="span">
        ![](images/lec11/10_dark.png#only-dark)
        ![](images/lec11/10_light.png#only-light)
        <figcaption></figcaption>
    </figure>

2. 关于参数$r$（$C^*$为最优中心点集，令$r(C^*) \le r$）：由于它的范围是已知的（$0 < r \le r_{\text{max}}$）我们可以使用**二分查找**来找到$r$的值，具体来说：
    - 先令$r = \dfrac{0 + r_{\max}}{2}$
    - 如果能够在这个$r$下面找到满足要求的$K$个中心点，说明这个界还是比较宽松的，需要减小$r$；否则的话增加$r$
    - 时间复杂度：$O(\log r_{\max})$

3. 回到贪心算法上：
    - 从输入点集中随机选取第一个点作为第一个中心，然后删除该点为中心，$2r$为半径的所有点
    - 然后在剩余点中随机选择第二个中心，以此类推
    - 如果该$r$值确实是最优解，那么这一算法在$K$步之内必然停止，且得到的解是最优解的2倍，即该算法是一个**2-近似算法**
    - 定理：假设该算法选择的中心点数超过$K$，那么对于任意规模至多为$K$的中心点集$C^*$，覆盖半径为$r(C*) > r$

---
这个贪心法还可以再改进！还是先给出伪代码：

```c
Centers Greedy-Kcenter(Sites S[], int n, int K) {
    Centers C[] = NULL;
    Select any s from S and add it to C;
    while (|C| < K) {
        Select s from S with maximum dist(s, C);
        Add s to C;
    }  // end-while
    return C;
}
```

- 这里的贪心法与上面的贪心法区别在于：后者是任意选取输入点集中的点作为中心点，而前者的策略是：
    - 第一个点还是任意取的
    - 之后选择离中心点集中的点尽可能远的点作为新的中心点，这样的选择方法更加聪明些
    - 循环$K$遍就结束循环了

- 定理：该算法返回包含规模为$K$的中心点集$C$，使得$r(C) \le 2r(C^*)$，其中$C^*$表示最优中心点集
- 很可惜的是，这种做法属于“换汤不换药”，本质上依旧是一个2-近似算法。

很遗憾的是，对于该问题（中间选择问题），不存在$\rho < 2$的近似算法，下面给出证明：

??? proof "~~证明~~"

    >这部分直接翻译cy的PPT，~~因为我也没搞懂~~

    用归谬法证明：

    - 假如存在多项式时间的$2 - \varepsilon$的近似算法，那么我们也能在多项式时间内解决[支配集(dominating-set)问题](https://en.wikipedia.org/wiki/Dominating_set)（它是一个NPC问题，这一结论来自NPC问题的[性质](10.md#definition)）
    - 当前仅当存在中心选择问题的最优中心点集半径$r(C^*)=1$时，规模为$K$的支配集存在解
    
    >实在看不下去了，之后再补吧:dizzy_face:  